{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit Download Notebook\n",
    "\n",
    "This notebook contains code to download subreddits from http://files.pushshift.io/reddit/subreddits/\n",
    "\n",
    "After dowloading the files in order to decrompress the zst file:  \n",
    "\n",
    "git clone https://github.com/facebook/zstd.git  \n",
    "make  \n",
    "zstd -xvf Reddit_Subreddits.ndjson.zst  \n",
    "\n",
    "\n",
    "more info = https://github.com/pushshift/api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "import ndjson\n",
    "import json\n",
    "import bz2\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'clpsych'\n",
    "project_path = Path(os.getcwd()).parent\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    data_path = 'D:\\Dataset\\{0}\\dataset'.format(project_name)\n",
    "elif sys.platform == 'darwin':\n",
    "    data_path = '/Volumes/Dataset/{0}/dataset'.format(project_name)\n",
    "else:\n",
    "    data_path = Path(project_path, 'dataset')\n",
    "\n",
    "utils_path = str(Path(project_path, 'utils'))\n",
    "# including the project folder and the utils folder\n",
    "if utils_path not in ''.join(sys.path):\n",
    "    sys.path.extend([str(project_path), utils_path])\n",
    "\n",
    "print('project path = {0}'.format(project_path))\n",
    "print('data path = {0}'.format(data_path))\n",
    "print('')\n",
    "print('sys.path = {0}'.format(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base10 id, reddit base36 id, creation epoch, subreddit name, number of subscribers\n",
    "basic = pd.read_csv(Path(data_path, 'subreddits_basic.csv')) \n",
    "basic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file_object = Path(data_path, 'Reddit_Subreddits.ndjson').open().read()\n",
    "data = ndjson.loads(file_object)\n",
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(Path(data_path, '69M_reddit_accounts.csv')) \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_path = Path(data_path, 'comments')\n",
    "data = pd.read_json(Path(n_data_path, 'RC_2005-12'), lines=True) \n",
    "print(data.keys())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_path = Path(data_path, 'submissions')\n",
    "data = pd.read_json(Path(n_data_path, 'RC_2005-12'), lines=True) \n",
    "print(data.keys())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_path = Path(data_path, 'subreddits')\n",
    "data = pd.read_json(Path(n_data_path, 'subreddits.json'), lines=True) \n",
    "print(data.keys())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_files = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = list()\n",
    "file_processed_comments = list()\n",
    "\n",
    "# obtain the body within those comments files\n",
    "comments_path = Path(data_path, 'comments')\n",
    "\n",
    "total_elements = 0\n",
    "\n",
    "for file in comments_path.iterdir():\n",
    "    if file.suffix == '.json' and file.stem not in file_processed_comments:\n",
    "        # open file and get data\n",
    "        data = pd.read_json(file, lines=True)\n",
    "        comments.extend(data['body'].values)\n",
    "        total_elements += data['body'].shape[0]\n",
    "        print('processed {0}'.format(file))\n",
    "        file_processed_comments.append(file.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decompreses the bz2 files\n",
    "comments_path = Path(data_path, 'submissions')\n",
    "# get all the json files and their stem\n",
    "processed_files = [element.replace('.csv', '') for element in os.listdir(comments_path) if '.csv' in element]\n",
    "\n",
    "for file in comments_path.iterdir():\n",
    "#     if (file.suffix == '.bz2' or file.suffix == '') and file.stem not in processed_files \n",
    "    if (file.suffix == '') and file.stem not in processed_files and not file.is_dir():\n",
    "        try:\n",
    "            print('processing {0}'.format(file))\n",
    "            # open file\n",
    "            if file.suffix == '.bz2':\n",
    "                zipfile = bz2.BZ2File(file)\n",
    "                # get the decompressed data\n",
    "                data = zipfile.read()\n",
    "                # convert to string\n",
    "                s = str(data,'utf-8')\n",
    "                ndata = StringIO(s)\n",
    "            elif file.suffix == '':\n",
    "                ndata = file\n",
    "            # convert json to dataframe\n",
    "            df = pd.read_json(ndata, lines=True)\n",
    "            # keep relevant columns\n",
    "            df = df[['subreddit', 'subreddit_id', 'selftext', 'author', 'title', 'created_utc']].copy()\n",
    "            filename = file.stem + '.csv'\n",
    "            new_file = Path(comments_path, filename)\n",
    "            # store in file\n",
    "            df.to_csv(new_file, index=False)\n",
    "            processed_files.append(file.stem)\n",
    "            print('stored {0}'.format(new_file))\n",
    "        except:\n",
    "            print('error')\n",
    "            error_files.append(file.stem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
