{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'clpsych'\n",
    "project_path = Path(os.getcwd()).parent\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    data_path = 'D:\\Dataset\\{0}\\dataset'.format(project_name)\n",
    "elif sys.platform == 'darwin':\n",
    "    data_path = '/Volumes/Dataset/{0}/dataset'.format(project_name)\n",
    "else:\n",
    "    data_path = Path(project_path, 'dataset')\n",
    "\n",
    "utils_path = str(Path(project_path, 'utils'))\n",
    "# including the project folder and the utils folder\n",
    "if utils_path not in ''.join(sys.path):\n",
    "    sys.path.extend([str(project_path), utils_path])\n",
    "\n",
    "print('project path = {0}'.format(project_path))\n",
    "print('data path = {0}'.format(data_path))\n",
    "print('')\n",
    "print('sys.path = {0}'.format(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(data_path, 'clpsych19_test_data')\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in shared_task_posts csv file\n",
    "# filename = 'clpsych19_public_shared_task_posts.csv'\n",
    "filename = 'shared_task_posts_test.csv'\n",
    "# shared_post = pd.read_csv(Path(data_path, filename), header=None, \n",
    "#                                 names=['post_id', 'user_id', 'timestamp', \n",
    "#                                        'subreddits', 'post_title', 'post_body'])\n",
    "shared_post = pd.read_csv(Path(data_path, filename))\n",
    "print(shared_post.shape)\n",
    "shared_post.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'post_user_risk.csv'\n",
    "users = pd.read_csv(Path(data_path, filename))\n",
    "print(users.shape)\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in task_B_train csv file\n",
    "filename = 'risk_tbs.csv'\n",
    "sentiments = pd.read_csv(Path(data_path, filename), header=None, \n",
    "                         names=['post_id', 'sent_1', 'sent_2', 'sent_3', 'sent_4', 'sent_5'])\n",
    "# sentiments = sentiments.set_index('post_id').copy()\n",
    "sentiments.post_id = sentiments.post_id.astype(int)\n",
    "print(sentiments.shape)\n",
    "sentiments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in task_B_train csv file\n",
    "# filename = 'risk_tbs_num.csv'\n",
    "filename = 'clpsych19_test_title_body_sentiments_sentence.csv'\n",
    "# sentence_number = pd.read_csv(Path(data_path, filename),header=None, \n",
    "#                          names=['post_id', 'num_sentences'])\n",
    "sentence_number = pd.read_csv(Path(data_path, filename),header=None, \n",
    "                         names=['user_id', 'post_id', 'num_sentences'])\n",
    "print(sentence_number.shape)\n",
    "sentence_number.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_dataset = sentiments.copy()\n",
    "# n_dataset['user_id'] = 0\n",
    "# n_dataset['risk_label'] = 0\n",
    "# n_dataset['post_id'] = 0\n",
    "# for sent_ele_ix, sent_ele in sentiments.iterrows():\n",
    "#     current_user = users.loc[int(sent_ele['post_id'])]\n",
    "#     user_id = current_user['user_id']\n",
    "#     n_dataset.loc[sent_ele_ix, 'user_id'] = user_id\n",
    "#     n_dataset.loc[sent_ele_ix, 'risk_label'] = current_user['risk_label']\n",
    "#     n_dataset.loc[sent_ele_ix, 'post_id'] = current_user['post_id']\n",
    "# n_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sntiments_num = pd.merge(sentiments, sentence_number, on='post_id', how='inner')\n",
    "sntiments_num.head()\n",
    "dataset = pd.merge(users, sntiments_num, left_index=True, right_on='post_id', how='inner')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[['post_id_x','sent_1','sent_2','sent_3','sent_4','sent_5','user_id','risk_label', 'num_sentences']]\n",
    "dataset.columns = ['post_id','sent_1','sent_2','sent_3','sent_4','sent_5','user_id','risk_label', 'num_sentences']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(dataset, shared_post, on=['post_id', 'user_id'], how='inner')\n",
    "dataset = dataset[['post_id', 'sent_1', 'sent_2', 'sent_3', 'sent_4', 'sent_5', 'user_id',\n",
    "       'risk_label', 'num_sentences', 'timestamp']].copy()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.shape)\n",
    "dataset = dataset[~pd.isnull(dataset['risk_label'])]  \n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(Path(data_path, 'dataset_sentiments.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'clpsych19_test_title_body_sentiments.csv'\n",
    "dataset = pd.read_csv(Path(data_path, filename))\n",
    "dataset.drop('correct', axis=1, inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns = ['user_id', 'post_id', 'sent_1', 'sent_2', 'sent_3', 'sent_4',\n",
    "       'sent_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dataset = pd.merge(dataset, shared_post, on=['post_id', 'user_id'], how='inner')\n",
    "n_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dataset.drop(['subreddit','post_title','post_body'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dataset = pd.merge(n_dataset, sentence_number, on=['post_id', 'user_id'], how='inner')\n",
    "n_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dataset.to_csv(Path(data_path, 'dataset_sentiments_test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(Path(data_path, 'dataset_sentiments_test.csv'))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual post-level sentiment vector: sum of all sentiment vectors for sentences in a post / total number of the sentence in the post [you already computed it for each post]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_sentiments_list_per_post = list()\n",
    "\n",
    "#iterate over user ids\n",
    "for row_ix, row in dataset.iterrows():\n",
    "    new_row = list()\n",
    "    # individual post-level sentiment vector\n",
    "    relevant_dist = row[['sent_1','sent_2','sent_3','sent_4','sent_5']] / row['num_sentences']\n",
    "    # normalization\n",
    "    sentiment_dist = relevant_dist / relevant_dist.sum()\n",
    "    # store\n",
    "#     new_row.extend(row[['post_id', 'user_id', 'risk_label','timestamp']])\n",
    "    new_row.extend(row[['post_id', 'user_id','timestamp']])\n",
    "    new_row.extend(list(sentiment_dist.values))\n",
    "    averaged_sentiments_list_per_post.append(np.array(new_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_obj = Path(data_path, 'sentiment_per_post_test.csv')\n",
    "with file_obj.open('w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "#     writer.writerow(['post_id', 'user_id', 'risk_label', 'timestamp', 'sent_1','sent_2',\n",
    "#                      'sent_3','sent_4','sent_5'])\n",
    "    writer.writerow(['post_id', 'user_id', 'timestamp', 'sent_1','sent_2',\n",
    "                     'sent_3','sent_4','sent_5'])\n",
    "    for row in averaged_sentiments_list_per_post:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_post_sent = pd.read_csv(file_obj)\n",
    "individual_post_sent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All sentence-level average (micro ave.): sum of all sentiment vectors (1x5) across all the posts of a user / total number of sentences across all the posts of the user (Ni,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_user_sentiments_list_micro = list()\n",
    "\n",
    "#iterate over user ids\n",
    "for user in dataset.user_id.unique():\n",
    "    user_posts = dataset[dataset['user_id'] == user]\n",
    "    user_sentiments_list_micro = list()\n",
    "    # get posts for current user\n",
    "    total_sentences = user_posts['num_sentences'].sum()\n",
    "    # All sentence-level average (micro ave.)\n",
    "    relevant_dist_micro = user_posts[['sent_1','sent_2','sent_3','sent_4','sent_5']].sum() / total_sentences\n",
    "    # normalization\n",
    "    sentiment_dist_micro = relevant_dist_micro / relevant_dist_micro.sum()\n",
    "    # store\n",
    "    user_sentiments_list_micro.append(user)\n",
    "    user_sentiments_list_micro.extend(list(sentiment_dist_micro.values))\n",
    "#     user_sentiments_list_micro.append(user_posts['risk_label'].unique()[0])\n",
    "    averaged_user_sentiments_list_micro.append(np.array(user_sentiments_list_micro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_obj = Path(data_path, 'sentiment_per_user_micro_test.csv')\n",
    "with file_obj.open('w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "#     writer.writerow(['user_id', 'sent_1_micro','sent_2_micro','sent_3_micro','sent_4_micro','sent_5_micro', \n",
    "#                      'risk_label'])\n",
    "    writer.writerow(['user_id', 'sent_1_micro','sent_2_micro','sent_3_micro','sent_4_micro','sent_5_micro'])\n",
    "    for row in averaged_user_sentiments_list_micro:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_obj = Path(data_path, 'sentiment_per_user_micro_test.csv')\n",
    "micro = pd.read_csv(file_obj).head()\n",
    "micro.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All post-level average (marco ave.): sum of Individual post-level sentiment vector for all the posts of a user / total number of posts of a user (Ni,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_user_sentiments_list_macro = list()\n",
    "\n",
    "#iterate over user ids\n",
    "for user in dataset.user_id.unique():\n",
    "    user_sentiments_list_macro = list()\n",
    "    user_posts = dataset[dataset['user_id'] == user]\n",
    "    #get posts for current user\n",
    "    total_post = user_posts.shape[0]\n",
    "    # All sentence-level average (macro ave.)\n",
    "    relevant_dist_macro = user_posts[['sent_1','sent_2','sent_3','sent_4','sent_5']].head() / total_post\n",
    "    # normalization\n",
    "    dist_sum_macro = relevant_dist_macro.sum()\n",
    "    sentiment_dist_macro = dist_sum_macro / dist_sum_macro.sum()\n",
    "    # store\n",
    "    user_sentiments_list_macro.append(user)\n",
    "    user_sentiments_list_macro.extend(list(sentiment_dist_macro.values))\n",
    "#     user_sentiments_list_macro.append(user_posts['risk_label'].unique()[0])\n",
    "    averaged_user_sentiments_list_macro.append(np.array(user_sentiments_list_macro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_obj = Path(data_path, 'sentiment_per_user_macro_test.csv')\n",
    "with file_obj.open('w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',')\n",
    "#     writer.writerow(['user_id', 'sent_1_macro','sent_2_macro','sent_3_macro','sent_4_macro','sent_5_macro', \n",
    "#                      'risk_label'])\n",
    "    writer.writerow(['user_id', 'sent_1_macro','sent_2_macro','sent_3_macro','sent_4_macro','sent_5_macro'])\n",
    "    for row in averaged_user_sentiments_list_macro:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro = pd.read_csv(Path(data_path, 'sentiment_per_user_macro_test.csv'))\n",
    "macro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
