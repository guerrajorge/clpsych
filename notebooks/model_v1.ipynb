{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project path = /home/guerramarj/github/clpsych\n",
      "data path = /home/guerramarj/github/clpsych/dataset\n",
      "model path = /home/guerramarj/github/clpsych/models\n",
      "sys.path = ['/cm/local/apps/cuda/libs/current/pynvml', '/home/guerramarj/python/Python-3.6.3', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python36.zip', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/lib-dynload', '', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/Mako-1.0.7-py3.6.egg', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/IPython/extensions', '/home/guerramarj/.ipython', '/home/guerramarj/github/clpsych', '/home/guerramarj/github/clpsych/utils', '/home/guerramarj/github/clpsych/src']\n"
     ]
    }
   ],
   "source": [
    "project_name = 'clpsych'\n",
    "project_path = Path(os.getcwd()).parent\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    data_path = 'D:\\Dataset\\{0}\\dataset'.format(project_name)\n",
    "    model_path = 'D:\\Dataset\\{0}\\models'.format(project_name)\n",
    "    src_path = '/Volumes/Dataset/{0}/src'.format(project_name)\n",
    "    \n",
    "elif sys.platform == 'darwin':\n",
    "    data_path = '/Volumes/Dataset/{0}/dataset'.format(project_name)\n",
    "    model_path = '/Volumes/Dataset/{0}/models'.format(project_name)\n",
    "    src_path = '/Volumes/Dataset/{0}/src'.format(project_name)\n",
    "    \n",
    "else:\n",
    "    data_path = Path(project_path, 'dataset')\n",
    "    model_path = Path(project_path, 'models')\n",
    "    src_path = Path(project_path, 'src')\n",
    "\n",
    "utils_path = str(Path(project_path, 'utils'))\n",
    "# including the project folder and the utils folder\n",
    "if utils_path not in ''.join(sys.path):\n",
    "    sys.path.extend([str(project_path), utils_path, str(src_path)])\n",
    "\n",
    "print('project path = {0}'.format(project_path))\n",
    "print('data path = {0}'.format(data_path))\n",
    "print('model path = {0}'.format(model_path))\n",
    "print('sys.path = {0}'.format(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from utils.datapath import data_path_scripts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "from numpy import interp \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam, Adam\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend\n",
    "from keras import metrics\n",
    "import tensorflow as tf\n",
    "import _pickle as pickle \n",
    "import stanfordnlp\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.datasets import imdb \n",
    "from utils.data_helpers import load_data_and_labels\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 13]\n",
    "\n",
    "# seed for numpy and sklearn\n",
    "random_state = 7\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow recognizes GPUs\n",
      "number of available GPUs = 2\n",
      "list of GPUs = ['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "#confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "print('Tensorflow recognizes GPUs')\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "available_gpu = backend.tensorflow_backend._get_available_gpus()\n",
    "assert len(available_gpu) > 0\n",
    "print('number of available GPUs = {0}'.format(len(available_gpu)))\n",
    "print('list of GPUs = {0}\\n'.format(available_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resulting_confusion_matrix(y, y_pred, file_name, train_test, fig_number):\n",
    "    \n",
    "    print()\n",
    "    print(\"\\n{0} results\\n\".format(train_test))\n",
    "    \n",
    "    n_true = len(y[y == 0])\n",
    "    n_false = len(y[y == 1])\n",
    "    \n",
    "    print('total data points = {0}'.format(y.shape[0]))\n",
    "    print('true data points = {0} (% {1:.3})'.format(n_true, n_true / np.shape(y)[0]))\n",
    "    print('false data points = {0} (% {1:.3})'.format(n_false, n_false / np.shape(y)[0]))\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y, y_pred)\n",
    "    roc_score = auc(fpr, tpr)\n",
    "    r_auc_score = roc_auc_score(y, y_pred)\n",
    "    \n",
    "    print('roc score = {0}'.format(roc_score))\n",
    "    print('roc auc score = {0}'.format(r_auc_score))\n",
    "    \n",
    "    print()    \n",
    "    print(\"confusion matrix\")\n",
    "\n",
    "    target_names = ['Show', 'No-Show']\n",
    "    print(classification_report(y, y_pred, target_names=target_names))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    class_names = ['Show', 'No-Show']\n",
    "    \n",
    "    plt.figure(fig_number)\n",
    "    plt.subplot(221)\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix, without normalization')\n",
    "    print()\n",
    "    \n",
    "    plt.figure(fig_number)\n",
    "    plt.subplot(222)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "    print()\n",
    "    \n",
    "    plt.figure(fig_number)\n",
    "    plt.subplot(223)\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_score)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_metric(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    backend.get_session().run(tf.local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mpost_risklabel.csv\u001b[0m*  preprocessed_corpus.pkl\r\n"
     ]
    }
   ],
   "source": [
    "%ls ../dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = 'wei'\n",
    "if person == 'victor':\n",
    "    data = pickle.load(Path(data_path, 'preprocessed_corpus.pkl').open('rb'))\n",
    "elif person == 'wei':\n",
    "    data = load_data_and_labels(file_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, vocabulary_inv = build_vocab(X_train)\n",
    "vocabulary_size = len(vocabulary_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_padded, sequence_length = pad_sentences(x_text)\n",
    "X_train, y_train = build_input_data(X_train, y_train, vocabulary)\n",
    "# print(X_train.shape)\n",
    "X_test, y_test = build_input_data(X_test, y_test, vocabulary)\n",
    "# all x and y for predicting\n",
    "x, y_class = build_input_data(sentences_padded, y_class, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data['post_title'].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text += data['post_body'].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in data['post_title'] + data['post_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5000 \n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse lookup\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v+0) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in x_train[0] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode your documents\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "docs = ['Gut gemacht',\n",
    "        'Gute arbeit',\n",
    "        'Super idee',\n",
    "        'Perfekt erledigt',\n",
    "        'exzellent',\n",
    "        'naja',\n",
    "        'Schwache arbeit.',\n",
    "        'Nicht gut',\n",
    "        'Miese arbeit.',\n",
    "        'Hätte es besser machen können.']\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_x_test = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate and pad the review sequences \n",
    "from keras.preprocessing import sequence \n",
    "max_review_length = 500 \n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_review_length) \n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build the model \n",
    "embedding_vector_length = 32 \n",
    "model = Sequential() \n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length)) \n",
    "model.add(LSTM(100)) \n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy']) \n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 16001     \n",
      "=================================================================\n",
      "Total params: 176,001\n",
      "Trainable params: 176,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build the model \n",
    "embedding_vector_length = 32 \n",
    "model = Sequential() \n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length)) \n",
    "model.add(Flatten()) \n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy']) \n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 1s 54us/step - loss: 0.0314 - acc: 0.9974\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 1s 54us/step - loss: 0.0198 - acc: 0.9990\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 1s 54us/step - loss: 0.0128 - acc: 0.9996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ab4960dad68>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(x_train, y_train, validation_data=(x_test, y_test), nb_epoch=3, batch_size=64) \n",
    "model.fit(x_train, y_train, epochs=3, batch_size=64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir=/home/guerramarj/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model\n",
    "scores = model.evaluate(_test, y_test, verbose=0) \n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict sentiment from reviews\n",
    "bad = \"this movie was terrible and bad\"\n",
    "good = \"i really liked the movie and had fun\"\n",
    "for review in [good,bad]:\n",
    "    tmp = []\n",
    "    for word in review.split(\" \"):\n",
    "        tmp.append(word_to_id[word])\n",
    "    tmp_padded = sequence.pad_sequences([tmp], maxlen=max_review_length) \n",
    "    print(\"%s. Sentiment: %s\" % (review,model.predict(array([tmp_padded][0]))[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "first, second = data[data['subreddit']  == 'SuicideWatch']['post_body'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "first, second = x_test[0], x_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]], dtype=int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
