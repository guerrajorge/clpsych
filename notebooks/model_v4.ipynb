{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'clpsych'\n",
    "project_path = Path(os.getcwd()).parent\n",
    "src_path = Path(project_path, 'src')\n",
    "model_path = Path(project_path, 'model')\n",
    "utils_path = Path(project_path, 'utils')\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    data_path = 'D:\\Dataset\\{0}\\dataset'.format(project_name)\n",
    "    embedding_path = 'D:\\Dataset\\{0}\\embedding'.format(embedding)\n",
    "    \n",
    "elif sys.platform == 'darwin':\n",
    "    data_path = '/Volumes/Dataset/{0}/dataset'.format(project_name)\n",
    "    embedding_path = '/Volumes/Dataset/{0}/embedding'.format(project_name)\n",
    "    \n",
    "else:\n",
    "    data_path = Path(project_path, 'dataset')\n",
    "    model_path = Path(project_path, 'models')\n",
    "    embedding_path = Path(project_path, 'embedding')\n",
    "\n",
    "# including the project folder and the utils folder\n",
    "if str(utils_path) not in ''.join(sys.path):\n",
    "    sys.path.extend([str(project_path), str(utils_path), str(src_path)])\n",
    "\n",
    "print('project path = {0}'.format(project_path))\n",
    "print('data path = {0}'.format(data_path))\n",
    "print('model path = {0}'.format(model_path))\n",
    "print('embedding path = {0}'.format(embedding_path))\n",
    "print('sys.path = {0}'.format(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from utils.datapath import data_path_scripts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "from numpy import interp \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam, Adam\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.utils import multi_gpu_model\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 13]\n",
    "\n",
    "# seed for numpy and sklearn\n",
    "random_state = 7\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "print('Tensorflow recognizes GPUs')\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "available_gpu = backend.tensorflow_backend._get_available_gpus()\n",
    "assert len(available_gpu) > 0\n",
    "available_gpus = len(available_gpu)\n",
    "print('number of available GPUs = {0}'.format(available_gpus))\n",
    "print('list of GPUs = {0}\\n'.format(available_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_report(y_true, y_pred, y_score=None, average='macro'):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        print(\"Error! y_true {0} is not the same shape as y_pred {1}\".format(\n",
    "              y_true.shape,\n",
    "              y_pred.shape)\n",
    "        )\n",
    "        return\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "\n",
    "    if len(y_true.shape) == 1:\n",
    "        lb.fit(y_true)\n",
    "\n",
    "    #Value counts of predictions\n",
    "    labels, cnt = np.unique(\n",
    "        y_pred,\n",
    "        return_counts=True)\n",
    "    n_classes = len(labels)\n",
    "    pred_cnt = pd.Series(cnt, index=labels)\n",
    "\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            labels=labels)\n",
    "\n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average=average))\n",
    "\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list(metrics_summary),\n",
    "        index=metrics_sum_index,\n",
    "        columns=labels)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    class_report_df['avg / total'] = avg[:-1] + [total]\n",
    "\n",
    "    class_report_df = class_report_df.T\n",
    "    class_report_df['pred-cnt'] = pred_cnt\n",
    "    class_report_df['pred-cnt'].iloc[-1] = total\n",
    "\n",
    "    if not (y_score is None):\n",
    "        # false positive rate\n",
    "        fpr = dict()\n",
    "        # true positive rate\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for label_ix, label in enumerate(labels):\n",
    "            fpr[label], tpr[label], _ = roc_curve(\n",
    "                (y_true == label).astype(int), \n",
    "                y_score[:, label_ix])\n",
    "\n",
    "            roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        if average == 'micro':\n",
    "            if n_classes <= 2:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                    lb.transform(y_true).ravel(), \n",
    "                    y_score[:, 1].ravel())\n",
    "            else:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                        lb.transform(y_true).ravel(), \n",
    "                        y_score.ravel())\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(\n",
    "                fpr[\"avg / total\"], \n",
    "                tpr[\"avg / total\"])\n",
    "\n",
    "        elif average == 'macro':\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([\n",
    "                fpr[i] for i in labels]\n",
    "            ))\n",
    "\n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in labels:\n",
    "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "            # Finally average it and compute AUC\n",
    "            mean_tpr /= n_classes\n",
    "\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        class_report_df['AUC'] = pd.Series(roc_auc)\n",
    "\n",
    "    return class_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sentiment_per_post.csv'\n",
    "dataset = pd.read_csv(Path(data_path, filename))\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(dataset['risk_label'])\n",
    "dataset.loc[: ,'risk_label'] = le.transform(dataset['risk_label']) \n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path(data_path, 'static_features_pandas_v2.pkl')\n",
    "dataset = pd.read_pickle(filename)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Feature Set   Description  \n",
    "A             static_derieved_features  \n",
    "B             post_coount_by_subreddit\n",
    "C             lexicon_count  \n",
    "D             sentiments_macro  \n",
    "E             sentiments_micro  \n",
    "F             empathy\n",
    "G             readability\n",
    "H             social_context\n",
    "I             srl\n",
    "J             ctakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_interest = ['SETAAA']\n",
    "# features_interest = ['SETBBB'] # contains NaN\n",
    "# features_interest = ['SETCCC'] # contains NaN\n",
    "# features_interest = ['SETDDD']\n",
    "# features_interest = ['SETEEE']\n",
    "# features_interest = ['SETFFF'] # contains NaN\n",
    "# features_interest = ['SETGGG'] # contains NaN\n",
    "# features_interest = ['SETHHH'] # contains NaN\n",
    "# features_interest = ['SETIII'] # contains NaN\n",
    "# features_interest = ['SETJJJ'] # contains NaN\n",
    "# features_interest = ['SETAAA', 'SETDDD', 'SETEEE', 'SETFFF', 'SETGGG']\n",
    "features_interest = ['SETAAA', 'SETBBB']\n",
    "\n",
    "features = [x for x in dataset.columns if x.split('__')[0] in features_interest]\n",
    "# including label\n",
    "features.append('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = 'logit'\n",
    "\n",
    "# define 5-fold cross validation test harness\n",
    "k_fold = pd.read_csv(Path(data_path, 'clpsych19_public_crossvalidation_splits.csv'), header=None,\n",
    "                    names=['fold', 'train_text', 'user_id'])\n",
    "\n",
    "# keep non conrol user ids\n",
    "k_fold = k_fold[k_fold['user_id'] > 0]\n",
    "\n",
    "print('5 fold CV starting')\n",
    "for fold_ix in range(1,6):\n",
    "    \n",
    "    print('\\nFold = {0}'.format(fold_ix))\n",
    "    \n",
    "    train_ix = k_fold[(k_fold['fold'] == fold_ix) & (k_fold['train_text'] == 'training')]['user_id']\n",
    "    test_ix = k_fold[(k_fold['fold'] == fold_ix) & (k_fold['train_text'] == 'test')]['user_id']\n",
    "    \n",
    "    x_train = data[data.index.isin(train_ix)].copy()\n",
    "    x_test = data[data.index.isin(test_ix)].copy()\n",
    "    \n",
    "    y_train = x_train['target']\n",
    "    x_train.drop(['target'], axis=1, inplace=True)\n",
    "    \n",
    "    y_test = x_test['target']\n",
    "    x_test.drop(['target'], axis=1, inplace=True)\n",
    "    \n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp_mean.fit(x_train)\n",
    "    x_train = imp_mean.transform(x_train)\n",
    "    \n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp_mean.fit(x_test)\n",
    "    x_test = imp_mean.transform(x_test)\n",
    "\n",
    "#     x_train = x_train.fillna(x_train.mean())\n",
    "#     x_test = x_test.fillna(x_test.mean())\n",
    "    \n",
    "    x_train = (x_train - x_train.mean()) / (x_train.max() - x_train.min())\n",
    "    x_test = (x_test - x_test.mean()) / (x_test.max() - x_test.min())\n",
    "    \n",
    "    if classifier == 'nn':\n",
    "        # basic neural network\n",
    "        model = Sequential()\n",
    "        model.add(Dense(5000, input_dim=np.shape(x_train)[1], activation='relu'))\n",
    "        model.add(Dense(2500, activation='relu'))\n",
    "        model.add(Dense(2000, activation='relu'))\n",
    "        model.add(Dense(1000, activation='relu'))\n",
    "        model.add(Dense(500, activation='relu'))\n",
    "        model.add(Dense(250, activation='relu'))\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(len(y_train.unique()), activation='sigmoid'))\n",
    "\n",
    "        model = multi_gpu_model(model, gpus=available_gpus)\n",
    "\n",
    "        # treating every instance of class 1 as 50 instances of class 0\n",
    "        class_weight = {1: 1, 2: 10, 3:10, 4: 1}\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(optimizer='adam', loss=f1_loss, metrics=[f1])\n",
    "        model.fit(x_train, y_train, epochs=1, batch_size=100, verbose=1, \n",
    "                  class_weight=class_weight)\n",
    "\n",
    "        y_pred_train = np.argmax(model.predict(x_train), axis=1)\n",
    "        y_pred_test = np.argmax(model.predict(x_test), axis=1)\n",
    "        y_score = model.predict(x_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif classifier == 'logit':\n",
    "#         model = LogisticRegression(class_weight='balanced', n_jobs=-1,\n",
    "#                                   multi_class='auto', solver='lbfgs',\n",
    "#                                   tol=0.00001, C=10, max_iter=1000, verbose=True,\n",
    "#                                   random_state=random_state)\n",
    "        model = LogisticRegression(class_weight='balanced', n_jobs=-1,\n",
    "                          multi_class='auto', solver='lbfgs',\n",
    "                          tol=0.00001, C=10, max_iter=1000, verbose=True,\n",
    "                          random_state=random_state)  \n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred_test = model.predict(x_test)\n",
    "        y_score=model.predict_proba(x_test)\n",
    "        \n",
    "    report_with_auc = class_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=y_pred_test, \n",
    "    y_score=y_score,\n",
    "    average='macro')\n",
    "\n",
    "    \n",
    "\n",
    "    cv_column = [fold_ix]\n",
    "    cv_column.extend( [''] * (report_with_auc.index.shape[0] - 1))\n",
    "    report_with_auc['Fold'] = cv_column\n",
    "    report_with_auc['Risk-Factor'] = report_with_auc.index\n",
    "    report_with_auc = report_with_auc.set_index(['Fold', 'Risk-Factor'])\n",
    "\n",
    "    if fold_ix == 1:\n",
    "        report_with_auc_df = report_with_auc.copy()\n",
    "    else:\n",
    "        report_with_auc_df = report_with_auc_df.append(report_with_auc.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_with_auc_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
