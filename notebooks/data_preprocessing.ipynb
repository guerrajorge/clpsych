{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./dataset/post_risklabel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset) # 496, 496 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for datasets.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "#     string = re.sub(r\"\\[.*\\]\", \"Name\", string)\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels():\n",
    "    \"\"\"\n",
    "    Loads polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    dataset = pd.read_csv('./dataset/post_risklabel.csv')\n",
    "    post = dataset['post']\n",
    "    post = [str(s).strip() for s in post]\n",
    "    user_risk_label = dataset['risk_label']\n",
    "    # Split by words\n",
    "    x_text = [clean_str(sent) for sent in post]\n",
    "    x_text = [s.split(\" \") for s in x_text]\n",
    "    # Generate labels\n",
    "    risk_labels = list()\n",
    "    for label in user_risk_label:\n",
    "        if label == 'a':\n",
    "            risk_labels.append([1, 0, 0, 0])\n",
    "        elif label == 'b':\n",
    "            risk_labels.append([0, 1, 0, 0])\n",
    "        elif label == 'c':\n",
    "            risk_labels.append([0, 0, 1, 0])\n",
    "        elif label == 'd':\n",
    "            risk_labels.append([0, 0, 0, 1])\n",
    "    y = np.asarray(risk_labels)\n",
    "    return [x_text, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text, y = load_data_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_padded, sequence_length = pad_sentences(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "# from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "# splitter = SentenceSplitter(language='en')\n",
    "# from util import TextCleaner, InputReader\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for datasets.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "#     string = re.sub(r\"\\[.*\\]\", \"Name\", string)\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels():\n",
    "    \"\"\"\n",
    "    Loads polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    dataset = pd.read_csv('./dataset/post_risklabel.csv')\n",
    "    post = dataset['post']\n",
    "    post = [str(s).strip() for s in post]\n",
    "    user_risk_label = dataset['risk_label']\n",
    "    # Split by words\n",
    "    x_text = [clean_str(sent) for sent in post]\n",
    "    x_text = [s.split(\" \") for s in x_text]\n",
    "    # Generate labels\n",
    "    risk_labels = list()\n",
    "    for label in user_risk_label:\n",
    "        if label == 'a':\n",
    "            risk_labels.append([1, 0, 0, 0])\n",
    "        elif label == 'b':\n",
    "            risk_labels.append([0, 1, 0, 0])\n",
    "        elif label == 'c':\n",
    "            risk_labels.append([0, 0, 1, 0])\n",
    "        elif label == 'd':\n",
    "            risk_labels.append([0, 0, 0, 1])\n",
    "    y = np.asarray(risk_labels)\n",
    "    return [x_text, y]\n",
    "\n",
    "\n",
    "\n",
    "def pad_sentences(sentences, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "    return padded_sentences, sequence_length\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return [vocabulary, vocabulary_inv]\n",
    "\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    sentences_padded_list = list()\n",
    "    for sentence in sentences:\n",
    "        sentence_list = list()\n",
    "        for word in sentence:\n",
    "            if word in vocabulary:\n",
    "                sentence_list.append(vocabulary[word])\n",
    "            else:\n",
    "                sentence_list.append(np.random.uniform(-0.01, 0.01))\n",
    "        sentences_padded_list.append(sentence_list)\n",
    "    x = np.array(sentences_padded_list)\n",
    "    y = np.array(labels)\n",
    "    return [x, y]\n",
    "\n",
    "\n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def get_labels():\n",
    "    \"\"\"\n",
    "    Loads polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    annotated_data = pd.read_csv(\"./dataset/post_risklabel.csv\")\n",
    "    annotation_class = annotated_data['risk_label']\n",
    "    # Generate labels\n",
    "    class_labels = list()\n",
    "    for label in annotation_class:\n",
    "        if label == 'a':\n",
    "            class_labels.append(0)\n",
    "        elif label == 'b':\n",
    "            class_labels.append(1)\n",
    "        elif label == 'c':\n",
    "            class_labels.append(2)\n",
    "        elif label == 'd':\n",
    "            class_labels.append(3)\n",
    "    y = np.asarray(class_labels)\n",
    "    return y\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    # plt.autoscale()\n",
    "    # plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotated social text data\n"
     ]
    }
   ],
   "source": [
    "print('Loading annotated social text data')\n",
    "x_text, y_class = load_data_and_labels()\n",
    "y = get_labels()\n",
    "\n",
    "sentences_padded, sequence_length = pad_sentences(x_text)\n",
    "\n",
    "# global variebles\n",
    "embedding_dim = 200\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "epochs = 1\n",
    "batch_size = 100\n",
    "\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "cvscores = []\n",
    "auc_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 fold CV starting\n",
      "spliting train and test set\n",
      "building vocabulary on train set\n",
      "Mapping sentences to vectors based on vocabulary\n",
      "building embedding matrix using GloVe word embeddings\n",
      "Creating Model...\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Training Model...\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "print('10 fold CV starting')\n",
    "for train, test in kfold.split(sentences_padded, y_class):\n",
    "    # split train & test set\n",
    "    print('spliting train and test set')\n",
    "    X_train = list()\n",
    "    X_test = list()\n",
    "    for index in train:\n",
    "        X_train.append(sentences_padded[index])\n",
    "    for index in test:\n",
    "        X_test.append(sentences_padded[index])\n",
    "    y_train = y_class[train]\n",
    "    y_test = y_class[test]\n",
    "\n",
    "    # building vocabulary on train set\n",
    "    print('building vocabulary on train set')\n",
    "    vocabulary, vocabulary_inv = build_vocab(X_train)\n",
    "\n",
    "    # Maps sentences to vectors based on vocabulary\n",
    "    print('Mapping sentences to vectors based on vocabulary')\n",
    "    X_train, y_train = build_input_data(X_train, y_train, vocabulary)\n",
    "    # print(X_train.shape)\n",
    "    X_test, y_test = build_input_data(X_test, y_test, vocabulary)\n",
    "    # all x and y for predicting\n",
    "    x, y_class = build_input_data(sentences_padded, y_class, vocabulary)\n",
    "    # print(X_test.shape)\n",
    "    vocabulary_size = len(vocabulary_inv)\n",
    "\n",
    "    # building embedding matrix using GloVe word embeddings\n",
    "    print('building embedding matrix using GloVe word embeddings')\n",
    "    embedding_matrix = create_embedding_matrix('./dataset/myGloVe200d.txt', vocabulary, embedding_dim)\n",
    "\n",
    "    # this returns a tensor\n",
    "    print(\"Creating Model...\")\n",
    "    inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "    embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=sequence_length)(inputs)\n",
    "\n",
    "    lstm = LSTM(num_filters, kernel_initializer='normal', activation='relu')(embedding)\n",
    "\n",
    "    dropout = Dropout(drop)(lstm)\n",
    "    output = Dense(units=4, activation='softmax')(dropout)\n",
    "\n",
    "    # this creates a model that includes\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# checkpoint = ModelCheckpoint('./model/weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    print(\"Training Model...\")\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1)  # starts training\n",
    "\n",
    "    # evaluate the model\n",
    "    print(\"Evaluate Model...\")\n",
    "    scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "\n",
    "    print('Saving Model...')\n",
    "    model_name = 'LSTM_GloVe_' + str(len(cvscores))\n",
    "    model.save('./model/'+ model_name + '.hdf5')  # creates a HDF5 file 'my_model.h5'\n",
    "\n",
    "    print('Saving vocabulary to .json file')\n",
    "    with open('./vocabulary/' + model_name + '.json', 'w') as fp:\n",
    "        json.dump(vocabulary, fp)\n",
    "\n",
    "    print('Predicting categories...')\n",
    "    y_pred = model.predict(x)\n",
    "    y_classes = y_pred.argmax(axis=-1)\n",
    "    auc_score = multiclass_roc_auc_score(y, y_classes, average=\"weighted\")\n",
    "    print(\"%s: %.2f%%\" % ('Average AUC', auc_score * 100))\n",
    "    auc_scores.append(auc_score * 100)\n",
    "    df_y_classes = pd.DataFrame(y_classes)\n",
    "    df_y = pd.DataFrame(y)\n",
    "    result = pd.concat([df_y, df_y_classes], axis=1)\n",
    "    result.columns = ['true_class', 'predict_class']\n",
    "    result.to_csv('./results/' + model_name + 'result.csv', encoding='utf-8', index = False)\n",
    "\n",
    "    print('Generating confusion matrix...')\n",
    "    conf_mat = confusion_matrix(y, y_classes)\n",
    "\n",
    "    print('Plotting results...')\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    labels = ['Economic', 'Education', 'Health Care', 'Housing', 'Interaction with the legal system',\n",
    "              'Occupational', 'Other', 'social environment', 'Spiritural Life',\n",
    "              'Support circumstances and networks', 'Transportaion']\n",
    "    # sns.heatmap(conf_mat, annot=True, fmt='d',CNN_social11_model.pyCNN_social11_model.py\n",
    "    #             xticklabels=labels, yticklabels=labels)\n",
    "    # plt.ylabel('Actual')\n",
    "    # plt.xlabel('Predicted')\n",
    "    # fig.savefig('./figure/' + model_name + 'result.png')\n",
    "    # plt.figure()\n",
    "    plot_confusion_matrix(conf_mat, classes=labels, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "    plt.gcf().subplots_adjust(bottom=0.15)\n",
    "\n",
    "    print('Saving plots...')\n",
    "    fig.savefig('./figure/' + model_name + 'result.png')\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "print(\"auc\" + \"%.2f%% (+/- %.2f%%)\" % (np.mean(auc_scores), np.std(auc_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
