{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'clpsych'\n",
    "project_path = Path(os.getcwd()).parent\n",
    "src_path = Path(project_path, 'src')\n",
    "model_path = Path(project_path, 'model')\n",
    "utils_path = Path(project_path, 'utils')\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    data_path = 'D:\\Dataset\\{0}\\dataset'.format(project_name)\n",
    "    embedding_path = 'D:\\Dataset\\{0}\\embedding'.format(embedding)\n",
    "    \n",
    "elif sys.platform == 'darwin':\n",
    "    data_path = '/Volumes/Dataset/{0}/dataset'.format(project_name)\n",
    "    embedding_path = '/Volumes/Dataset/{0}/embedding'.format(project_name)\n",
    "    \n",
    "else:\n",
    "    data_path = Path(project_path, 'dataset')\n",
    "    model_path = Path(project_path, 'models')\n",
    "    embedding_path = Path(project_path, 'embedding')\n",
    "\n",
    "# including the project folder and the utils folder\n",
    "if str(utils_path) not in ''.join(sys.path):\n",
    "    sys.path.extend([str(project_path), str(utils_path), str(src_path)])\n",
    "\n",
    "print('project path = {0}'.format(project_path))\n",
    "print('data path = {0}'.format(data_path))\n",
    "print('model path = {0}'.format(model_path))\n",
    "print('embedding path = {0}'.format(embedding_path))\n",
    "print('sys.path = {0}'.format(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from utils.datapath import data_path_scripts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "from numpy import interp \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from collections import Counter\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam, Adam\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.utils import multi_gpu_model\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 13]\n",
    "\n",
    "# seed for numpy and sklearn\n",
    "random_state = 7\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "print('Tensorflow recognizes GPUs')\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "available_gpu = backend.tensorflow_backend._get_available_gpus()\n",
    "assert len(available_gpu) > 0\n",
    "available_gpus = len(available_gpu)\n",
    "print('number of available GPUs = {0}'.format(available_gpus))\n",
    "print('list of GPUs = {0}\\n'.format(available_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def class_report(y_true, y_pred, y_score=None, average='macro'):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        print(\"Error! y_true {0} is not the same shape as y_pred {1}\".format(\n",
    "              y_true.shape,\n",
    "              y_pred.shape)\n",
    "        )\n",
    "        return\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "    \n",
    "    # check if multi label ?\n",
    "    if len(y_true.shape) == 1:\n",
    "        lb.fit(y_true)\n",
    "\n",
    "    #Value counts of predictions\n",
    "    labels, cnt = np.unique(y_pred, return_counts=True)\n",
    "    n_classes = len(labels)\n",
    "    pred_cnt = pd.Series(cnt, index=labels)\n",
    "    \n",
    "    # precision\n",
    "    #     ratio tp / (tp + fp)\n",
    "    #     ability of the classifier not to label as positive a sample that is negative\n",
    "    # recall\n",
    "    #     tp / (tp + fn)\n",
    "    #     ability of the classifier to find all the positive samples\n",
    "    # f1-score\n",
    "    #     2 * (precision * recall)/(precision + recall)\n",
    "    #     weighted harmonic mean of the precision and recall\n",
    "    #     best value at 1 and worst score at 0\n",
    "    # support\n",
    "    #     number of occurrences of each class in y_true\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            labels=labels)\n",
    "\n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average=average))\n",
    "\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list(metrics_summary),\n",
    "        index=metrics_sum_index,\n",
    "        columns=labels)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    class_report_df['avg / total'] = avg[:-1] + [total]\n",
    "\n",
    "    class_report_df = class_report_df.T\n",
    "    class_report_df['pred-cnt'] = pred_cnt\n",
    "    class_report_df['pred-cnt'].iloc[-1] = total\n",
    "\n",
    "    if not (y_score is None):\n",
    "        # false positive rate\n",
    "        fpr = dict()\n",
    "        # true positive rate\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for label_ix, label in enumerate(labels):\n",
    "            fpr[label], tpr[label], _ = roc_curve(\n",
    "                (y_true == label).astype(int), \n",
    "                y_score[:, label_ix])\n",
    "\n",
    "            roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        if average == 'micro':\n",
    "            if n_classes <= 2:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                    lb.transform(y_true).ravel(), \n",
    "                    y_score[:, 1].ravel())\n",
    "            else:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                        lb.transform(y_true).ravel(), \n",
    "                        y_score.ravel())\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(\n",
    "                fpr[\"avg / total\"], \n",
    "                tpr[\"avg / total\"])\n",
    "\n",
    "        elif average == 'macro':\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([\n",
    "                fpr[i] for i in labels]\n",
    "            ))\n",
    "\n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in labels:\n",
    "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "            # Finally average it and compute AUC\n",
    "            mean_tpr /= n_classes\n",
    "\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        class_report_df['AUC'] = pd.Series(roc_auc)\n",
    "\n",
    "    return class_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sentiment_per_post.csv'\n",
    "dataset = pd.read_csv(Path(data_path, filename))\n",
    "\n",
    "label = 'risk_label'\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(dataset['risk_label'])\n",
    "dataset.loc[: ,'risk_label'] = le.transform(dataset['risk_label']) \n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path(data_path, 'static_features_pandas_v2.pkl')\n",
    "dataset = pd.read_pickle(filename)\n",
    "label = 'target'\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Feature Set   Description  \n",
    "A             static_derieved_features  \n",
    "B             post_coount_by_subreddit\n",
    "C             lexicon_count  \n",
    "D             sentiments_macro  \n",
    "E             sentiments_micro  \n",
    "F             empathy\n",
    "G             readability\n",
    "H             social_context\n",
    "I             srl\n",
    "J             ctakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_interest = ['SETAAA']\n",
    "# features_interest = ['SETBBB'] # contains NaN\n",
    "# features_interest = ['SETCCC'] # contains NaN\n",
    "# features_interest = ['SETDDD']\n",
    "# features_interest = ['SETEEE']\n",
    "# features_interest = ['SETFFF'] # contains NaN\n",
    "# features_interest = ['SETGGG'] # contains NaN\n",
    "# features_interest = ['SETHHH'] # contains NaN\n",
    "# features_interest = ['SETIII'] # contains NaN\n",
    "# features_interest = ['SETJJJ'] # contains NaN\n",
    "features_interest = ['SETAAA', 'SETDDD', 'SETEEE', 'SETFFF']\n",
    "\n",
    "features = [x for x in dataset.columns if x.split('__')[0] in features_interest]\n",
    "# including label\n",
    "features.append('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 5-fold cross validation test harness\n",
    "k_fold = pd.read_csv(Path(data_path, 'clpsych19_public_crossvalidation_splits.csv'), header=None,\n",
    "                    names=['fold', 'train_text', 'user_id'])\n",
    "\n",
    "# keep non conrol user ids\n",
    "k_fold = k_fold[k_fold['user_id'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = 'logit'\n",
    "\n",
    "print('5 fold CV starting')\n",
    "for fold_ix in range(1,6):\n",
    "    \n",
    "    print('\\nFold = {0}'.format(fold_ix))\n",
    "    \n",
    "    train_ix = k_fold[(k_fold['fold'] == fold_ix) & (k_fold['train_text'] == 'training')]['user_id']\n",
    "    test_ix = k_fold[(k_fold['fold'] == fold_ix) & (k_fold['train_text'] == 'test')]['user_id']\n",
    "    \n",
    "    x_train_df = data[data.index.isin(train_ix)].copy()\n",
    "    x_test_df = data[data.index.isin(test_ix)].copy()\n",
    "    \n",
    "    \n",
    "    y_train = x_train_df[label]\n",
    "    x_train = x_train_df.drop([label], axis=1).values\n",
    "    \n",
    "    y_test = x_test_df[label]\n",
    "    x_test = x_test_df.drop([label], axis=1).values\n",
    "    \n",
    "#     y_train = x_train_df[label].values\n",
    "#     x_train = x_train_df.drop([label, 'post_id'], axis=1).values\n",
    "    \n",
    "#     y_test = x_test_df[label].values\n",
    "#     x_test = x_test_df.drop([label, 'post_id'], axis=1).values\n",
    "    \n",
    "#     imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "#     imp_mean.fit(x_train)\n",
    "#     x_train = imp_mean.transform(x_train)\n",
    "    \n",
    "#     imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "#     imp_mean.fit(x_test)\n",
    "#     x_test = imp_mean.transform(x_test)\n",
    "\n",
    "#     x_train = x_train.fillna(x_train.mean())\n",
    "#     x_test = x_test.fillna(x_test.mean())\n",
    "    \n",
    "#     x_train = (x_train - x_train.mean()) / (x_train.max() - x_train.min())\n",
    "#     x_test = (x_test - x_test.mean()) / (x_test.max() - x_test.min())\n",
    "    \n",
    "    if classifier == 'nn':\n",
    "        # basic neural network\n",
    "        model = Sequential()\n",
    "        model.add(Dense(5000, input_dim=np.shape(x_train)[1], activation='relu'))\n",
    "        model.add(Dense(2500, activation='relu'))\n",
    "        model.add(Dense(2000, activation='relu'))\n",
    "        model.add(Dense(1000, activation='relu'))\n",
    "        model.add(Dense(500, activation='relu'))\n",
    "        model.add(Dense(250, activation='relu'))\n",
    "        model.add(Dense(100, activation='relu'))\n",
    "        model.add(Dense(50, activation='relu'))\n",
    "        model.add(Dense(len(y_train.unique()), activation='sigmoid'))\n",
    "\n",
    "        model = multi_gpu_model(model, gpus=available_gpus)\n",
    "\n",
    "        # treating every instance of class 1 as 50 instances of class 0\n",
    "        class_weight = {1: 1, 2: 10, 3:10, 4: 1}\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(optimizer='adam', loss=f1_loss, metrics=[f1])\n",
    "        model.fit(x_train, y_train, epochs=1, batch_size=100, verbose=1, \n",
    "                  class_weight=class_weight)\n",
    "\n",
    "        y_pred_train = np.argmax(model.predict(x_train), axis=1)\n",
    "        y_pred_test = np.argmax(model.predict(x_test), axis=1)\n",
    "        y_score = model.predict(x_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif classifier == 'logit':\n",
    "        \n",
    "        ab_indices = y_train < 3\n",
    "        cd_indices = y_train > 2\n",
    "        \n",
    "        model1 = LogisticRegression(class_weight='balanced', n_jobs=-1,\n",
    "                          multi_class='auto', solver='lbfgs',\n",
    "                          tol=0.0001, C=120, max_iter=100, verbose=True,\n",
    "                          random_state=random_state, penalty='l2')  \n",
    "        model1.fit(x_train[ab_indices], y_train[ab_indices])\n",
    "        \n",
    "        model2 = LogisticRegression(class_weight='balanced', n_jobs=-1,\n",
    "                  multi_class='auto', solver='lbfgs',\n",
    "                  tol=0.0001, C=120, max_iter=100, verbose=True,\n",
    "                  random_state=random_state, penalty='l2')  \n",
    "        model2.fit(x_train[cd_indices], y_train[cd_indices])\n",
    "        \n",
    "        break\n",
    "    \n",
    "        y_pred_train1 = model1.predict(x_train[ab_indices])\n",
    "        y_score_train1 =model1.predict_proba(x_train[ab_indices])\n",
    "#         y_pred_test1 = model1.predict(x_test)\n",
    "        \n",
    "        \n",
    "        y_pred_train1 = model2.predict(x_train[cd_indices])\n",
    "        y_score_train1 =model2.predict_proba(x_train[cd_indices])\n",
    "#         y_pred_test1 = model2.predict(x_test)\n",
    "        \n",
    "        \n",
    "        cm = confusion_matrix(y_true=y_train[ab_indices], y_pred=y_pred_train1)\n",
    "        plot_confusion_matrix(cm, target_names=['a','b'])\n",
    "        \n",
    "        cm = confusion_matrix(y_true=y_train[cd_indices], y_pred=y_pred_train2)\n",
    "        plot_confusion_matrix(cm, target_names=['c','d'])\n",
    "        \n",
    "        cm = confusion_matrix(y_true=y_test, y_pred=y_pred_test)\n",
    "        plot_confusion_matrix(cm, target_names=['a','b','c','d'])\n",
    "        \n",
    "        y_score_test1 =model1.predict_proba(x_test)\n",
    "        y_score_test1 =model2.predict_proba(x_test)\n",
    "        \n",
    "        n_dataset = pd.DataFrame(np.concatenate((y_score_test1, y_score_test2), axis=1), columns=[1,2,3,4])\n",
    "        \n",
    "        \n",
    "    \n",
    "    break\n",
    "    report_with_auc = class_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=y_pred_test, \n",
    "    y_score=y_score,\n",
    "    average='macro')\n",
    "\n",
    "    cv_column = [fold_ix]\n",
    "    cv_column.extend( [''] * (report_with_auc.index.shape[0] - 1))\n",
    "    report_with_auc['Fold'] = cv_column\n",
    "    report_with_auc['Risk-Factor'] = report_with_auc.index\n",
    "    report_with_auc = report_with_auc.set_index(['Fold', 'Risk-Factor'])\n",
    "\n",
    "    if fold_ix == 1:\n",
    "        report_with_auc_df = report_with_auc.copy()\n",
    "    else:\n",
    "        report_with_auc_df = report_with_auc_df.append(report_with_auc.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train1 = model1.predict(x_train[ab_indices])\n",
    "y_score_train1 =model1.predict_proba(x_train[ab_indices])\n",
    "y_pred_test1 = model1.predict(x_test)\n",
    "y_score_test1 =model1.predict_proba(x_test)\n",
    "\n",
    "y_pred_train2 = model2.predict(x_train[cd_indices])\n",
    "y_score_train2 =model2.predict_proba(x_train[cd_indices])\n",
    "y_pred_test2 = model2.predict(x_test)\n",
    "y_score_test2 =model2.predict_proba(x_test)\n",
    "\n",
    "cm = confusion_matrix(y_true=y_train[ab_indices], y_pred=y_pred_train1)\n",
    "plot_confusion_matrix(cm, target_names=['a','b'])\n",
    "\n",
    "cm = confusion_matrix(y_true=y_train[cd_indices], y_pred=y_pred_train2)\n",
    "plot_confusion_matrix(cm, target_names=['c','d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in zip(y_test, y_score_test1, y_score_test2):\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_score_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred = pd.DataFrame(np.concatenate((y_score_test1, y_score_test2), axis=1), columns=[1,2,3,4])\n",
    "all_pred = all_pred.div(all_pred.sum(axis=1), axis=0)\n",
    "final_pred = all_pred.idxmax(axis=1)\n",
    "all_pred['true'] = y_test.values\n",
    "all_pred['pred'] = final_pred\n",
    "all_pred[(all_pred['true'] == 4) & (all_pred['pred'] == 1)]\n",
    "all_pred[(all_pred['true'] == 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred = list()\n",
    "for _, ele in all_pred.iterrows():\n",
    "    ele.drop(['true', 'pred', 'n_pred'], inplace=True)\n",
    "    if ele[3] > ele[2] and ele[3] > ele[4] and ele[1] < .5:\n",
    "        new_pred.append(3)\n",
    "    elif ele[2] > ele[4]:\n",
    "        new_pred.append(2)\n",
    "    else:\n",
    "        new_pred.append(ele.idxmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pred.drop('n_pred', axis=1, inplace=True)\n",
    "all_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred['n_pred'] = new_pred\n",
    "all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=y_test, y_pred=all_pred['n_pred'])\n",
    "plot_confusion_matrix(cm, target_names=['a','b','c','d'])\n",
    "class_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=all_pred['n_pred'],\n",
    "    average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_test1 =model1.predict_proba(x_test)\n",
    "y_score_test2 =model2.predict_proba(x_test)\n",
    "all_pred = pd.DataFrame(np.concatenate((y_score_test1, y_score_test2), axis=1), columns=[1,2,3,4])\n",
    "final_pred = all_pred.idxmax(axis=1)\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=final_pred)\n",
    "plot_confusion_matrix(cm, target_names=['a','b','c','d'])\n",
    "class_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=final_pred, \n",
    "    y_score=all_pred.values,\n",
    "    average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report(\n",
    "    y_true=y_train, \n",
    "    y_pred=y_pred_train, \n",
    "    y_score=y_score_train,\n",
    "    average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=y_pred_test, \n",
    "    y_score=y_score_test,\n",
    "    average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=y_test, y_pred=y_pred_test)\n",
    "plot_confusion_matrix(cm, target_names=['a','b','c','d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in zip(y_test,y_pred_test, y_score_test):\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
