{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'clpsych'\n",
    "project_path = Path(os.getcwd()).parent\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    data_path = 'D:\\Dataset\\{0}\\dataset'.format(project_name)\n",
    "    model_path = 'D:\\Dataset\\{0}\\models'.format(project_name)\n",
    "    src_path = '/Volumes/Dataset/{0}/src'.format(project_name)\n",
    "    \n",
    "elif sys.platform == 'darwin':\n",
    "    data_path = '/Volumes/Dataset/{0}/dataset'.format(project_name)\n",
    "    model_path = '/Volumes/Dataset/{0}/models'.format(project_name)\n",
    "    src_path = '/Volumes/Dataset/{0}/src'.format(project_name)\n",
    "    \n",
    "else:\n",
    "    data_path = Path(project_path, 'dataset')\n",
    "    model_path = Path(project_path, 'models')\n",
    "    src_path = Path(project_path, 'src')\n",
    "\n",
    "utils_path = str(Path(project_path, 'utils'))\n",
    "# including the project folder and the utils folder\n",
    "if utils_path not in ''.join(sys.path):\n",
    "    sys.path.extend([str(project_path), utils_path, str(src_path)])\n",
    "\n",
    "print('project path = {0}'.format(project_path))\n",
    "print('data path = {0}'.format(data_path))\n",
    "print('model path = {0}'.format(model_path))\n",
    "print('sys.path = {0}'.format(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from gensim import corpora\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim import corpora, utils, models, similarities\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary()\n",
    "lemma = WordNetLemmatizer()\n",
    "punctuation = set(string.punctuation)\n",
    "stoplist = set(stopwords.words('english'))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return ''.join([char for char in text if char not in punctuation])\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return ''.join([char for char in text if not char.isdigit()])\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stoplist])\n",
    "\n",
    "def remove_single_chars(text):\n",
    "    return ' '.join([word for word in text.split() if len(word) > 3])\n",
    "\n",
    "def lemmatize(text):\n",
    "    return ' '.join([lemma.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def lower_case(text):\n",
    "    return ' '.join([word.lower() for word in text.split()])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace('\\n', '')\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_single_chars(text)\n",
    "    text = lower_case(text)\n",
    "    text = lemmatize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(Path(data_path, 'risk_title_body.csv'))\n",
    "print(data.shape)\n",
    "data.columns = ['index', 'risk_label', 'title_body']\n",
    "data.drop(['index'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [clean_text(post) for post in data['title_body']]\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = [tokenizer.tokenize(post) for post in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic passes\n",
    "num_passes = 10\n",
    "num_topics = 20\n",
    "random_state = 7\n",
    "\n",
    "# Execution\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "# Remove words that appear less than 5 times and that are in more than in 80% documents\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.7)\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Model\n",
    "# lda = models.LdaModel(corpus, id2word=dictionary, random_state=random_state,\n",
    "#                       num_topics=num_topics, passes=num_passes)\n",
    "lda = models.ldamulticore.LdaMulticore(corpus, id2word=dictionary, random_state=random_state, \n",
    "                          num_topics=num_topics, passes=num_passes, workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.show_topics(num_topics=num_topics,formatted=False,num_words=100)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence = lda.top_topics(corpus,dictionary=dictionary,topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic2topkeywords = {}\n",
    "topic2csb = {}\n",
    "topic2keywords = {}\n",
    "topic2csa = {}\n",
    "num_topics =lda.num_topics\n",
    "cnt =1\n",
    "for ws in coherence:\n",
    "    wset = set(w[1] for w in ws[0])\n",
    "    topic2topkeywords[cnt] = wset # set with top keywords for topic\n",
    "    topic2csb[cnt] = ws[1] #avg coherence scores for each topic\n",
    "    cnt +=1\n",
    "for ws in topics:\n",
    "    # create a unique set of keywords for each topic\n",
    "    wset = set(w[0]for w in ws[1])\n",
    "    topic2keywords[ws[0]+1] = wset\n",
    "    \n",
    "for i in range(1,num_topics+1):\n",
    "    for j in range(1,num_topics+1):  \n",
    "        if topic2keywords[i].intersection(topic2topkeywords[j])==topic2keywords[i]:\n",
    "            topic2csa[i] = topic2csb[j]\n",
    "finalData = pd.DataFrame([],columns=['Topic','words'])\n",
    "finalData['Topic']=topic2keywords.keys()\n",
    "finalData['Topic'] = finalData['Topic'].apply(lambda x: 'Topic'+str(x))\n",
    "finalData['words']=topic2keywords.values()\n",
    "finalData['cs'] = topic2csa.values()\n",
    "finalData.sort_values(by='cs',ascending=False,inplace=True)\n",
    "finalData"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
