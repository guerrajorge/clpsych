{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using imbalanced-learn samplers API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project path = /home/guerramarj/github/clpsych\n",
      "data path = /home/guerramarj/github/clpsych/dataset\n",
      "model path = /home/guerramarj/github/clpsych/models\n",
      "embedding path = /home/guerramarj/github/clpsych/embedding\n",
      "sys.path = ['/cm/local/apps/cuda/libs/current/pynvml', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python36.zip', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/lib-dynload', '', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/Mako-1.0.7-py3.6.egg', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/IPython/extensions', '/home/guerramarj/.ipython', '/home/guerramarj/github/clpsych', '/home/guerramarj/github/clpsych/utils', '/home/guerramarj/github/clpsych/src']\n"
     ]
    }
   ],
   "source": [
    "project_name = 'clpsych'\n",
    "project_path = Path(os.getcwd()).parent\n",
    "src_path = Path(project_path, 'src')\n",
    "utils_path = Path(project_path, 'utils')\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    data_path = 'D:\\Dataset\\{0}\\dataset'.format(project_name)\n",
    "    embedding_path = 'D:\\Dataset\\{0}\\embedding'.format(project_name)\n",
    "    model_path = 'D:\\Dataset\\{0}\\embedding'.format(project_name)\n",
    "    \n",
    "elif sys.platform == 'darwin':\n",
    "    data_path = '/Volumes/Dataset/{0}/dataset'.format(project_name)\n",
    "    embedding_path = '/Volumes/Dataset/{0}/embedding'.format(project_name)\n",
    "    \n",
    "else:\n",
    "    data_path = Path(project_path, 'dataset')\n",
    "    model_path = Path(project_path, 'models')\n",
    "    embedding_path = Path(project_path, 'embedding')\n",
    "\n",
    "# including the project folder and the utils folder\n",
    "if str(utils_path) not in ''.join(sys.path):\n",
    "    sys.path.extend([str(project_path), str(utils_path), str(src_path)])\n",
    "\n",
    "print('project path = {0}'.format(project_path))\n",
    "print('data path = {0}'.format(data_path))\n",
    "print('model path = {0}'.format(model_path))\n",
    "print('embedding path = {0}'.format(embedding_path))\n",
    "print('sys.path = {0}'.format(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import interp \n",
    "from collections import Counter\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# optimizer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from keras.utils import multi_gpu_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# ml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# deeplearning\n",
    "import tensorflow as tf\n",
    "from keras.layers import Reshape, Flatten, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout\n",
    "from keras import metrics\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam, Adam\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "from keras.layers import Input\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 13]\n",
    "\n",
    "# seed for numpy and sklearn\n",
    "random_state = 7\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow recognizes GPUs\n",
      "number of available GPUs = 4\n",
      "list of GPUs = ['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "print('Tensorflow recognizes GPUs')\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "available_gpu = backend.tensorflow_backend._get_available_gpus()\n",
    "assert len(available_gpu) > 0\n",
    "available_gpus = len(available_gpu)\n",
    "print('number of available GPUs = {0}'.format(available_gpus))\n",
    "print('list of GPUs = {0}\\n'.format(available_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_report(y_true, y_pred, y_score=None, average='macro'):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        print(\"Error! y_true {0} is not the same shape as y_pred {1}\".format(\n",
    "              y_true.shape,\n",
    "              y_pred.shape)\n",
    "        )\n",
    "        return\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "\n",
    "    if len(y_true.shape) == 1:\n",
    "        lb.fit(y_true)\n",
    "\n",
    "    #Value counts of predictions\n",
    "    labels, cnt = np.unique(\n",
    "        y_pred,\n",
    "        return_counts=True)\n",
    "    n_classes = len(labels)\n",
    "    pred_cnt = pd.Series(cnt, index=labels)\n",
    "\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            labels=labels)\n",
    "\n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average=average))\n",
    "\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list(metrics_summary),\n",
    "        index=metrics_sum_index,\n",
    "        columns=labels)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    class_report_df['avg / total'] = avg[:-1] + [total]\n",
    "\n",
    "    class_report_df = class_report_df.T\n",
    "    class_report_df['pred-cnt'] = pred_cnt\n",
    "    class_report_df['pred-cnt'].iloc[-1] = total\n",
    "\n",
    "    if not (y_score is None):\n",
    "        # false positive rate\n",
    "        fpr = dict()\n",
    "        # true positive rate\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for label_ix, label in enumerate(labels):\n",
    "            fpr[label], tpr[label], _ = roc_curve(\n",
    "                (y_true == label).astype(int), \n",
    "                y_score[:, label_ix])\n",
    "\n",
    "            roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        if average == 'micro':\n",
    "            if n_classes <= 2:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                    lb.transform(y_true).ravel(), \n",
    "                    y_score[:, 1].ravel())\n",
    "            else:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                        lb.transform(y_true).ravel(), \n",
    "                        y_score.ravel())\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(\n",
    "                fpr[\"avg / total\"], \n",
    "                tpr[\"avg / total\"])\n",
    "\n",
    "        elif average == 'macro':\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([\n",
    "                fpr[i] for i in labels]\n",
    "            ))\n",
    "\n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in labels:\n",
    "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "            # Finally average it and compute AUC\n",
    "            mean_tpr /= n_classes\n",
    "\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        class_report_df['AUC'] = pd.Series(roc_auc)\n",
    "\n",
    "    return class_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(496, 11574)\n",
      "(496, 11614)\n"
     ]
    }
   ],
   "source": [
    "filename = Path(data_path, 'static_features_pandas_v2.pkl')\n",
    "filename = Path(data_path, 'static_features_pandas_aug_v2.2.pkl')\n",
    "dataset = pd.read_pickle(filename)\n",
    "filename = Path(data_path, 'extracted_features_sentiment_per_user_filtered.csv')\n",
    "sentiment_statics = pd.read_csv(filename)\n",
    "sentiment_statics.set_index('user_id', inplace=True)\n",
    "# sentiment_statics.drop('risk_label', axis=1, inplace=True)\n",
    "n_columns = ['SETLLL__{0}'.format(x) for x in sentiment_statics.columns]\n",
    "sentiment_statics.columns = n_columns\n",
    "print(dataset.shape)\n",
    "dataset = dataset.merge(right=sentiment_statics, left_index=True, right_index=True)\n",
    "print(dataset.shape)\n",
    "dataset.to_pickle(Path(data_path, 'static_features_pandas_aug_v2.2.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(496, 11614)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SETAAA__Max_weekly_MH_posts_last2years</th>\n",
       "      <th>SETAAA__Max_weekly_suicide_posts_last2years</th>\n",
       "      <th>SETAAA__N_posts_alltime</th>\n",
       "      <th>SETAAA__N_posts_last2years</th>\n",
       "      <th>SETAAA__N_weeks_with_MH_posts_last2years</th>\n",
       "      <th>SETAAA__N_weeks_with_suicide_posts_last2years</th>\n",
       "      <th>SETAAA__active_days</th>\n",
       "      <th>SETAAA__avg_distance_to_2am_last2years</th>\n",
       "      <th>SETAAA__avg_post_length_last2years</th>\n",
       "      <th>SETAAA__avg_proportion_adjs_last2years</th>\n",
       "      <th>...</th>\n",
       "      <th>SETLLL__sent_4__sample_entropy</th>\n",
       "      <th>SETLLL__sent_4__skewness</th>\n",
       "      <th>SETLLL__sent_5__abs_energy</th>\n",
       "      <th>SETLLL__sent_5__absolute_sum_of_changes</th>\n",
       "      <th>SETLLL__sent_5__linear_trend__attr_\"slope\"</th>\n",
       "      <th>SETLLL__sent_5__maximum</th>\n",
       "      <th>SETLLL__sent_5__mean</th>\n",
       "      <th>SETLLL__sent_5__minimum</th>\n",
       "      <th>SETLLL__sent_5__sample_entropy</th>\n",
       "      <th>SETLLL__sent_5__skewness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>509.961400</td>\n",
       "      <td>11.543417</td>\n",
       "      <td>505.450000</td>\n",
       "      <td>0.148389</td>\n",
       "      <td>...</td>\n",
       "      <td>1.879728</td>\n",
       "      <td>1.285680</td>\n",
       "      <td>0.031081</td>\n",
       "      <td>0.417038</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.066699</td>\n",
       "      <td>0.035918</td>\n",
       "      <td>0.015252</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.538883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>905.823194</td>\n",
       "      <td>6.847304</td>\n",
       "      <td>221.338028</td>\n",
       "      <td>0.160213</td>\n",
       "      <td>...</td>\n",
       "      <td>1.905546</td>\n",
       "      <td>1.897587</td>\n",
       "      <td>0.507650</td>\n",
       "      <td>3.600498</td>\n",
       "      <td>-0.000258</td>\n",
       "      <td>0.586467</td>\n",
       "      <td>0.033911</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.926642</td>\n",
       "      <td>8.118547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>443.223900</td>\n",
       "      <td>13.458903</td>\n",
       "      <td>203.896552</td>\n",
       "      <td>0.184151</td>\n",
       "      <td>...</td>\n",
       "      <td>1.857388</td>\n",
       "      <td>1.515073</td>\n",
       "      <td>0.653651</td>\n",
       "      <td>4.154778</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.536643</td>\n",
       "      <td>0.058049</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>1.089579</td>\n",
       "      <td>3.630981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>458.475648</td>\n",
       "      <td>11.730547</td>\n",
       "      <td>66.303030</td>\n",
       "      <td>0.121091</td>\n",
       "      <td>...</td>\n",
       "      <td>2.141962</td>\n",
       "      <td>1.255933</td>\n",
       "      <td>0.029821</td>\n",
       "      <td>0.575932</td>\n",
       "      <td>-0.000447</td>\n",
       "      <td>0.106684</td>\n",
       "      <td>0.022424</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>1.838279</td>\n",
       "      <td>2.396103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>582.131840</td>\n",
       "      <td>12.403656</td>\n",
       "      <td>518.253333</td>\n",
       "      <td>0.165292</td>\n",
       "      <td>...</td>\n",
       "      <td>1.929469</td>\n",
       "      <td>1.128026</td>\n",
       "      <td>0.172404</td>\n",
       "      <td>2.475222</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.176317</td>\n",
       "      <td>0.037454</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>1.883401</td>\n",
       "      <td>1.869605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11614 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SETAAA__Max_weekly_MH_posts_last2years  \\\n",
       "user_id                                           \n",
       "16                                            0   \n",
       "56                                            0   \n",
       "137                                           0   \n",
       "152                                           0   \n",
       "370                                           1   \n",
       "\n",
       "         SETAAA__Max_weekly_suicide_posts_last2years  SETAAA__N_posts_alltime  \\\n",
       "user_id                                                                         \n",
       "16                                                 1                       20   \n",
       "56                                                 0                      115   \n",
       "137                                                1                       58   \n",
       "152                                                1                       33   \n",
       "370                                                1                       75   \n",
       "\n",
       "         SETAAA__N_posts_last2years  SETAAA__N_weeks_with_MH_posts_last2years  \\\n",
       "user_id                                                                         \n",
       "16                               20                                         0   \n",
       "56                               71                                         0   \n",
       "137                              58                                         0   \n",
       "152                              33                                         0   \n",
       "370                              75                                         5   \n",
       "\n",
       "         SETAAA__N_weeks_with_suicide_posts_last2years  SETAAA__active_days  \\\n",
       "user_id                                                                       \n",
       "16                                                   2           509.961400   \n",
       "56                                                   0           905.823194   \n",
       "137                                                  1           443.223900   \n",
       "152                                                  2           458.475648   \n",
       "370                                                  1           582.131840   \n",
       "\n",
       "         SETAAA__avg_distance_to_2am_last2years  \\\n",
       "user_id                                           \n",
       "16                                    11.543417   \n",
       "56                                     6.847304   \n",
       "137                                   13.458903   \n",
       "152                                   11.730547   \n",
       "370                                   12.403656   \n",
       "\n",
       "         SETAAA__avg_post_length_last2years  \\\n",
       "user_id                                       \n",
       "16                               505.450000   \n",
       "56                               221.338028   \n",
       "137                              203.896552   \n",
       "152                               66.303030   \n",
       "370                              518.253333   \n",
       "\n",
       "         SETAAA__avg_proportion_adjs_last2years            ...             \\\n",
       "user_id                                                    ...              \n",
       "16                                     0.148389            ...              \n",
       "56                                     0.160213            ...              \n",
       "137                                    0.184151            ...              \n",
       "152                                    0.121091            ...              \n",
       "370                                    0.165292            ...              \n",
       "\n",
       "         SETLLL__sent_4__sample_entropy  SETLLL__sent_4__skewness  \\\n",
       "user_id                                                             \n",
       "16                             1.879728                  1.285680   \n",
       "56                             1.905546                  1.897587   \n",
       "137                            1.857388                  1.515073   \n",
       "152                            2.141962                  1.255933   \n",
       "370                            1.929469                  1.128026   \n",
       "\n",
       "         SETLLL__sent_5__abs_energy  SETLLL__sent_5__absolute_sum_of_changes  \\\n",
       "user_id                                                                        \n",
       "16                         0.031081                                 0.417038   \n",
       "56                         0.507650                                 3.600498   \n",
       "137                        0.653651                                 4.154778   \n",
       "152                        0.029821                                 0.575932   \n",
       "370                        0.172404                                 2.475222   \n",
       "\n",
       "         SETLLL__sent_5__linear_trend__attr_\"slope\"  SETLLL__sent_5__maximum  \\\n",
       "user_id                                                                        \n",
       "16                                         0.000112                 0.066699   \n",
       "56                                        -0.000258                 0.586467   \n",
       "137                                        0.000533                 0.536643   \n",
       "152                                       -0.000447                 0.106684   \n",
       "370                                        0.000223                 0.176317   \n",
       "\n",
       "         SETLLL__sent_5__mean  SETLLL__sent_5__minimum  \\\n",
       "user_id                                                  \n",
       "16                   0.035918                 0.015252   \n",
       "56                   0.033911                 0.002740   \n",
       "137                  0.058049                 0.001771   \n",
       "152                  0.022424                 0.002792   \n",
       "370                  0.037454                 0.000622   \n",
       "\n",
       "         SETLLL__sent_5__sample_entropy  SETLLL__sent_5__skewness  \n",
       "user_id                                                            \n",
       "16                             2.302585                  0.538883  \n",
       "56                             0.926642                  8.118547  \n",
       "137                            1.089579                  3.630981  \n",
       "152                            1.838279                  2.396103  \n",
       "370                            1.883401                  1.869605  \n",
       "\n",
       "[5 rows x 11614 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'static_features_pandas_v2.pkl'\n",
    "filename = 'static_features_pandas_aug_v2.pkl'\n",
    "filename = 'static_features_pandas_aug_v2.2.pkl'\n",
    "dataset = pd.read_pickle(Path(data_path, filename))\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 5-fold cross validation test harness\n",
    "k_fold = pd.read_csv(Path(data_path, 'clpsych19_public_crossvalidation_splits.csv'), header=None,\n",
    "                    names=['fold', 'train_text', 'user_id'])\n",
    "\n",
    "# keep non conrol user ids\n",
    "k_fold = k_fold[k_fold['user_id'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Feature Set   Description  \n",
    "A             static_derieved_features  \n",
    "B             post_coount_by_subreddit\n",
    "C             lexicon_count  \n",
    "D             sentiments_macro  \n",
    "E             sentiments_micro  \n",
    "F             empathy\n",
    "G             readability\n",
    "H             social_context\n",
    "I             srl\n",
    "J             ctakes\n",
    "K             sentiment statics\n",
    "L             sentiment statics filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 11574)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset.copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features_interest = ['SETAAA'] \n",
    "# features_interest = ['SETBBB'] # contains NaN\n",
    "# features_interest = ['SETCCC'] # contains NaN\n",
    "# features_interest = ['SETDDD']\n",
    "# features_interest = ['SETEEE']\n",
    "# features_interest = ['SETFFF'] # contains NaN\n",
    "# features_interest = ['SETGGG'] # contains NaN\n",
    "# features_interest = ['SETHHH'] # contains NaN\n",
    "# features_interest = ['SETIII'] # contains NaN\n",
    "# features_interest = ['SETJJJ'] # contains NaN\n",
    "# features_interest = ['SETKKK'] # contains NaN\n",
    "# features_interest = ['SETLLL'] # contains NaN\n",
    "# features_interest = ['SETAAA', 'SETDDD', 'SETEEE', 'SETFFF', 'SETGGG']\n",
    "features_interest = ['SETAAA', 'SETDDD', 'SETEEE']\n",
    "\n",
    "features = [x for x in dataset.columns if x.split('__')[0] in features_interest]\n",
    "# including label\n",
    "features.append('target')\n",
    "data = dataset[features].copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 fold CV starting\n",
      "\n",
      "Fold = 1\n",
      "(395, 31)\n",
      "[(1, 101), (2, 40), (3, 90), (4, 164)]\n",
      "[(1, 164), (2, 164), (3, 164), (4, 164)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "656/656 [==============================] - 4s 6ms/step - loss: 15.6396 - f1: 0.0000e+00\n",
      "Epoch 2/100\n",
      "656/656 [==============================] - 0s 57us/step - loss: 8.1283 - f1: 0.0000e+00\n",
      "Epoch 3/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 5.8397 - f1: 0.0027\n",
      "Epoch 4/100\n",
      "656/656 [==============================] - 0s 54us/step - loss: 4.6062 - f1: 0.0329\n",
      "Epoch 5/100\n",
      "656/656 [==============================] - 0s 59us/step - loss: 4.0564 - f1: 0.0423\n",
      "Epoch 6/100\n",
      "656/656 [==============================] - 0s 57us/step - loss: 3.7259 - f1: 0.0112\n",
      "Epoch 7/100\n",
      "656/656 [==============================] - 0s 59us/step - loss: 3.5249 - f1: 0.0245\n",
      "Epoch 8/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.5058 - f1: 0.0054\n",
      "Epoch 9/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.4538 - f1: 0.0000e+00\n",
      "Epoch 10/100\n",
      "656/656 [==============================] - 0s 61us/step - loss: 3.4323 - f1: 0.0000e+00\n",
      "Epoch 11/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.3792 - f1: 0.0000e+00\n",
      "Epoch 12/100\n",
      "656/656 [==============================] - 0s 58us/step - loss: 3.3008 - f1: 0.0000e+00\n",
      "Epoch 13/100\n",
      "656/656 [==============================] - 0s 62us/step - loss: 3.3298 - f1: 0.0045\n",
      "Epoch 14/100\n",
      "656/656 [==============================] - 0s 54us/step - loss: 3.3233 - f1: 0.0027\n",
      "Epoch 15/100\n",
      "656/656 [==============================] - 0s 57us/step - loss: 3.3304 - f1: 0.0332\n",
      "Epoch 16/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2699 - f1: 0.0235\n",
      "Epoch 17/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2713 - f1: 0.0036\n",
      "Epoch 18/100\n",
      "656/656 [==============================] - 0s 57us/step - loss: 3.3043 - f1: 0.0349\n",
      "Epoch 19/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2508 - f1: 0.0000e+00\n",
      "Epoch 20/100\n",
      "656/656 [==============================] - 0s 54us/step - loss: 3.2889 - f1: 0.0000e+00\n",
      "Epoch 21/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2754 - f1: 0.0000e+00\n",
      "Epoch 22/100\n",
      "656/656 [==============================] - 0s 54us/step - loss: 3.2615 - f1: 0.0000e+00\n",
      "Epoch 23/100\n",
      "656/656 [==============================] - 0s 57us/step - loss: 3.3018 - f1: 0.0000e+00\n",
      "Epoch 24/100\n",
      "656/656 [==============================] - 0s 59us/step - loss: 3.2971 - f1: 0.0000e+00\n",
      "Epoch 25/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2946 - f1: 0.0000e+00\n",
      "Epoch 26/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2714 - f1: 0.0000e+00\n",
      "Epoch 27/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2555 - f1: 0.0000e+00\n",
      "Epoch 28/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2752 - f1: 0.0000e+00\n",
      "Epoch 29/100\n",
      "656/656 [==============================] - 0s 58us/step - loss: 3.2894 - f1: 0.0000e+00\n",
      "Epoch 30/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2588 - f1: 0.0000e+00\n",
      "Epoch 31/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2397 - f1: 0.0000e+00\n",
      "Epoch 32/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2420 - f1: 0.0000e+00\n",
      "Epoch 33/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2902 - f1: 0.0000e+00\n",
      "Epoch 34/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2381 - f1: 0.0000e+00\n",
      "Epoch 35/100\n",
      "656/656 [==============================] - 0s 57us/step - loss: 3.2684 - f1: 0.0000e+00\n",
      "Epoch 36/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2638 - f1: 0.0000e+00\n",
      "Epoch 37/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2063 - f1: 0.0000e+00\n",
      "Epoch 38/100\n",
      "656/656 [==============================] - 0s 57us/step - loss: 3.2358 - f1: 0.0000e+00\n",
      "Epoch 39/100\n",
      "656/656 [==============================] - 0s 54us/step - loss: 3.2104 - f1: 0.0000e+00\n",
      "Epoch 40/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2902 - f1: 0.0000e+00\n",
      "Epoch 41/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2670 - f1: 0.0311\n",
      "Epoch 42/100\n",
      "656/656 [==============================] - 0s 58us/step - loss: 3.2067 - f1: 0.0000e+00\n",
      "Epoch 43/100\n",
      "656/656 [==============================] - 0s 57us/step - loss: 3.2938 - f1: 0.0000e+00\n",
      "Epoch 44/100\n",
      "656/656 [==============================] - 0s 57us/step - loss: 3.1636 - f1: 0.0000e+00\n",
      "Epoch 45/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2688 - f1: 0.0000e+00\n",
      "Epoch 46/100\n",
      "656/656 [==============================] - 0s 54us/step - loss: 3.2840 - f1: 0.0000e+00\n",
      "Epoch 47/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2196 - f1: 0.0000e+00\n",
      "Epoch 48/100\n",
      "656/656 [==============================] - 0s 57us/step - loss: 3.3001 - f1: 0.0000e+00\n",
      "Epoch 49/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2342 - f1: 0.0000e+00\n",
      "Epoch 50/100\n",
      "656/656 [==============================] - 0s 58us/step - loss: 3.2375 - f1: 0.0000e+00\n",
      "Epoch 51/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2492 - f1: 0.0000e+00\n",
      "Epoch 52/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2106 - f1: 0.0000e+00\n",
      "Epoch 53/100\n",
      "656/656 [==============================] - 0s 58us/step - loss: 3.2984 - f1: 0.0000e+00\n",
      "Epoch 54/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.1992 - f1: 0.0000e+00\n",
      "Epoch 55/100\n",
      "656/656 [==============================] - 0s 53us/step - loss: 3.2634 - f1: 0.0000e+00\n",
      "Epoch 56/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2446 - f1: 0.0000e+00\n",
      "Epoch 57/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.1887 - f1: 0.0000e+00\n",
      "Epoch 58/100\n",
      "656/656 [==============================] - 0s 57us/step - loss: 3.3142 - f1: 0.0000e+00\n",
      "Epoch 59/100\n",
      "656/656 [==============================] - 0s 61us/step - loss: 3.2243 - f1: 0.0000e+00\n",
      "Epoch 60/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2643 - f1: 0.0000e+00\n",
      "Epoch 61/100\n",
      "656/656 [==============================] - 0s 58us/step - loss: 3.2745 - f1: 0.0000e+00\n",
      "Epoch 62/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.1939 - f1: 0.0000e+00\n",
      "Epoch 63/100\n",
      "656/656 [==============================] - 0s 54us/step - loss: 3.2726 - f1: 0.0000e+00\n",
      "Epoch 64/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2342 - f1: 0.0000e+00\n",
      "Epoch 65/100\n",
      "656/656 [==============================] - 0s 54us/step - loss: 3.2788 - f1: 0.0000e+00\n",
      "Epoch 66/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2460 - f1: 0.0000e+00\n",
      "Epoch 67/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2197 - f1: 0.0000e+00\n",
      "Epoch 68/100\n",
      "656/656 [==============================] - 0s 57us/step - loss: 3.2588 - f1: 0.0000e+00\n",
      "Epoch 69/100\n",
      "656/656 [==============================] - 0s 57us/step - loss: 3.2307 - f1: 0.0000e+00\n",
      "Epoch 70/100\n",
      "656/656 [==============================] - 0s 58us/step - loss: 3.2486 - f1: 0.0000e+00\n",
      "Epoch 71/100\n",
      "656/656 [==============================] - 0s 58us/step - loss: 3.2793 - f1: 0.0000e+00\n",
      "Epoch 72/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2428 - f1: 0.0000e+00\n",
      "Epoch 73/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2475 - f1: 0.0000e+00\n",
      "Epoch 74/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2611 - f1: 0.0000e+00\n",
      "Epoch 75/100\n",
      "656/656 [==============================] - 0s 57us/step - loss: 3.2155 - f1: 0.0000e+00\n",
      "Epoch 76/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2651 - f1: 0.0000e+00\n",
      "Epoch 77/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2744 - f1: 0.0000e+00\n",
      "Epoch 78/100\n",
      "656/656 [==============================] - 0s 54us/step - loss: 3.2445 - f1: 0.0000e+00\n",
      "Epoch 79/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2475 - f1: 0.0000e+00\n",
      "Epoch 80/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2222 - f1: 0.0000e+00\n",
      "Epoch 81/100\n",
      "656/656 [==============================] - 0s 58us/step - loss: 3.2814 - f1: 0.0000e+00\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656/656 [==============================] - 0s 55us/step - loss: 3.2326 - f1: 0.0000e+00\n",
      "Epoch 83/100\n",
      "656/656 [==============================] - 0s 54us/step - loss: 3.2560 - f1: 0.0000e+00\n",
      "Epoch 84/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2779 - f1: 0.0000e+00\n",
      "Epoch 85/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2273 - f1: 0.0000e+00\n",
      "Epoch 86/100\n",
      "656/656 [==============================] - 0s 54us/step - loss: 3.2625 - f1: 0.0000e+00\n",
      "Epoch 87/100\n",
      "656/656 [==============================] - 0s 58us/step - loss: 3.2387 - f1: 0.0000e+00\n",
      "Epoch 88/100\n",
      "656/656 [==============================] - 0s 60us/step - loss: 3.2343 - f1: 0.0000e+00\n",
      "Epoch 89/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2891 - f1: 0.0000e+00\n",
      "Epoch 90/100\n",
      "656/656 [==============================] - 0s 57us/step - loss: 3.2517 - f1: 0.0000e+00\n",
      "Epoch 91/100\n",
      "656/656 [==============================] - 0s 57us/step - loss: 3.2513 - f1: 0.0000e+00\n",
      "Epoch 92/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2415 - f1: 0.0000e+00\n",
      "Epoch 93/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2578 - f1: 0.0000e+00\n",
      "Epoch 94/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2667 - f1: 0.0000e+00\n",
      "Epoch 95/100\n",
      "656/656 [==============================] - 0s 61us/step - loss: 3.2449 - f1: 0.0000e+00\n",
      "Epoch 96/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2843 - f1: 0.0000e+00\n",
      "Epoch 97/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2506 - f1: 0.0000e+00\n",
      "Epoch 98/100\n",
      "656/656 [==============================] - 0s 61us/step - loss: 3.2431 - f1: 0.0000e+00\n",
      "Epoch 99/100\n",
      "656/656 [==============================] - 0s 56us/step - loss: 3.2532 - f1: 0.0000e+00\n",
      "Epoch 100/100\n",
      "656/656 [==============================] - 0s 55us/step - loss: 3.2500 - f1: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold = 2\n",
      "(396, 31)\n",
      "[(1, 101), (2, 40), (3, 90), (4, 165)]\n",
      "[(1, 165), (2, 165), (3, 165), (4, 165)]\n",
      "Epoch 1/100\n",
      "660/660 [==============================] - 4s 6ms/step - loss: 15.5016 - f1: 0.0000e+00\n",
      "Epoch 2/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 8.2383 - f1: 0.0000e+00\n",
      "Epoch 3/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 5.9388 - f1: 0.0000e+00\n",
      "Epoch 4/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 4.6155 - f1: 0.0000e+00\n",
      "Epoch 5/100\n",
      "660/660 [==============================] - 0s 61us/step - loss: 4.0373 - f1: 0.0000e+00\n",
      "Epoch 6/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.7539 - f1: 0.0000e+00\n",
      "Epoch 7/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.5903 - f1: 0.0000e+00\n",
      "Epoch 8/100\n",
      "660/660 [==============================] - 0s 63us/step - loss: 3.4524 - f1: 0.0000e+00\n",
      "Epoch 9/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.4049 - f1: 0.0000e+00\n",
      "Epoch 10/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.4933 - f1: 0.0000e+00\n",
      "Epoch 11/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3848 - f1: 0.0000e+00\n",
      "Epoch 12/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.3268 - f1: 0.0000e+00\n",
      "Epoch 13/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.3916 - f1: 0.0000e+00\n",
      "Epoch 14/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3798 - f1: 0.0000e+00\n",
      "Epoch 15/100\n",
      "660/660 [==============================] - 0s 61us/step - loss: 3.4398 - f1: 0.0000e+00\n",
      "Epoch 16/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.3654 - f1: 0.0000e+00\n",
      "Epoch 17/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.3549 - f1: 0.0000e+00\n",
      "Epoch 18/100\n",
      "660/660 [==============================] - 0s 61us/step - loss: 3.4127 - f1: 0.0000e+00\n",
      "Epoch 19/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.3718 - f1: 0.0000e+00\n",
      "Epoch 20/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.4073 - f1: 0.0000e+00\n",
      "Epoch 21/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.3629 - f1: 0.0000e+00\n",
      "Epoch 22/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.3723 - f1: 0.0000e+00\n",
      "Epoch 23/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.4346 - f1: 0.0000e+00\n",
      "Epoch 24/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3371 - f1: 0.0000e+00\n",
      "Epoch 25/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.3590 - f1: 0.0000e+00\n",
      "Epoch 26/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.3362 - f1: 0.0000e+00\n",
      "Epoch 27/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.3367 - f1: 0.0000e+00\n",
      "Epoch 28/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3964 - f1: 0.0000e+00\n",
      "Epoch 29/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.3401 - f1: 0.0000e+00\n",
      "Epoch 30/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3092 - f1: 0.0000e+00\n",
      "Epoch 31/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.3552 - f1: 0.0000e+00\n",
      "Epoch 32/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2586 - f1: 0.0000e+00\n",
      "Epoch 33/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3527 - f1: 0.0000e+00\n",
      "Epoch 34/100\n",
      "660/660 [==============================] - 0s 62us/step - loss: 3.3105 - f1: 0.0000e+00\n",
      "Epoch 35/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.3180 - f1: 0.0000e+00\n",
      "Epoch 36/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3821 - f1: 0.0000e+00\n",
      "Epoch 37/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.2553 - f1: 0.0000e+00\n",
      "Epoch 38/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.3815 - f1: 0.0000e+00\n",
      "Epoch 39/100\n",
      "660/660 [==============================] - 0s 61us/step - loss: 3.3025 - f1: 0.0000e+00\n",
      "Epoch 40/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3210 - f1: 0.0000e+00\n",
      "Epoch 41/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.4082 - f1: 0.0000e+00\n",
      "Epoch 42/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2655 - f1: 0.0000e+00\n",
      "Epoch 43/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.3657 - f1: 0.0000e+00\n",
      "Epoch 44/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.2930 - f1: 0.0000e+00\n",
      "Epoch 45/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3189 - f1: 0.0000e+00\n",
      "Epoch 46/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.4121 - f1: 0.0000e+00\n",
      "Epoch 47/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2654 - f1: 0.0000e+00\n",
      "Epoch 48/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.3672 - f1: 0.0000e+00\n",
      "Epoch 49/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3130 - f1: 0.0000e+00\n",
      "Epoch 50/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2966 - f1: 0.0000e+00\n",
      "Epoch 51/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3722 - f1: 0.0000e+00\n",
      "Epoch 52/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2604 - f1: 0.0000e+00\n",
      "Epoch 53/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.4015 - f1: 0.0000e+00\n",
      "Epoch 54/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3337 - f1: 0.0000e+00\n",
      "Epoch 55/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2992 - f1: 0.0000e+00\n",
      "Epoch 56/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3726 - f1: 0.0000e+00\n",
      "Epoch 57/100\n",
      "660/660 [==============================] - 0s 62us/step - loss: 3.2933 - f1: 0.0000e+00\n",
      "Epoch 58/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.3790 - f1: 0.0000e+00\n",
      "Epoch 59/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.3511 - f1: 0.0000e+00\n",
      "Epoch 60/100\n",
      "660/660 [==============================] - 0s 61us/step - loss: 3.3168 - f1: 0.0000e+00\n",
      "Epoch 61/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3729 - f1: 0.0000e+00\n",
      "Epoch 62/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.3000 - f1: 0.0000e+00\n",
      "Epoch 63/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3634 - f1: 0.0000e+00\n",
      "Epoch 64/100\n",
      "660/660 [==============================] - 0s 63us/step - loss: 3.3278 - f1: 0.0000e+00\n",
      "Epoch 65/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.3700 - f1: 0.0000e+00\n",
      "Epoch 66/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3600 - f1: 0.0000e+00\n",
      "Epoch 67/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3156 - f1: 0.0000e+00\n",
      "Epoch 68/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3376 - f1: 0.0000e+00\n",
      "Epoch 69/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3428 - f1: 0.0000e+00\n",
      "Epoch 70/100\n",
      "660/660 [==============================] - 0s 63us/step - loss: 3.3621 - f1: 0.0000e+00\n",
      "Epoch 71/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3639 - f1: 0.0000e+00\n",
      "Epoch 72/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.3501 - f1: 0.0000e+00\n",
      "Epoch 73/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.3350 - f1: 0.0000e+00\n",
      "Epoch 74/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.3523 - f1: 0.0000e+00\n",
      "Epoch 75/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.3463 - f1: 0.0000e+00\n",
      "Epoch 76/100\n",
      "660/660 [==============================] - 0s 61us/step - loss: 3.3340 - f1: 0.0000e+00\n",
      "Epoch 77/100\n",
      "660/660 [==============================] - 0s 63us/step - loss: 3.3952 - f1: 0.0000e+00\n",
      "Epoch 78/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3380 - f1: 0.0000e+00\n",
      "Epoch 79/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3464 - f1: 0.0000e+00\n",
      "Epoch 80/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3312 - f1: 0.0000e+00\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660/660 [==============================] - 0s 59us/step - loss: 3.3458 - f1: 0.0000e+00\n",
      "Epoch 82/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.3729 - f1: 0.0000e+00\n",
      "Epoch 83/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3494 - f1: 0.0000e+00\n",
      "Epoch 84/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3426 - f1: 0.0000e+00\n",
      "Epoch 85/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3592 - f1: 0.0000e+00\n",
      "Epoch 86/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3234 - f1: 0.0000e+00\n",
      "Epoch 87/100\n",
      "660/660 [==============================] - 0s 61us/step - loss: 3.3570 - f1: 0.0000e+00\n",
      "Epoch 88/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.3382 - f1: 0.0000e+00\n",
      "Epoch 89/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3696 - f1: 0.0000e+00\n",
      "Epoch 90/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3755 - f1: 0.0000e+00\n",
      "Epoch 91/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3199 - f1: 0.0000e+00\n",
      "Epoch 92/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.3472 - f1: 0.0000e+00\n",
      "Epoch 93/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3448 - f1: 0.0000e+00\n",
      "Epoch 94/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3584 - f1: 0.0000e+00\n",
      "Epoch 95/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.3825 - f1: 0.0000e+00\n",
      "Epoch 96/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3395 - f1: 0.0000e+00\n",
      "Epoch 97/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3490 - f1: 0.0000e+00\n",
      "Epoch 98/100\n",
      "660/660 [==============================] - 0s 61us/step - loss: 3.3452 - f1: 0.0000e+00\n",
      "Epoch 99/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3292 - f1: 0.0000e+00\n",
      "Epoch 100/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3623 - f1: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold = 3\n",
      "(397, 31)\n",
      "[(1, 102), (2, 40), (3, 90), (4, 165)]\n",
      "[(1, 165), (2, 165), (3, 165), (4, 165)]\n",
      "Epoch 1/100\n",
      "660/660 [==============================] - 4s 6ms/step - loss: 15.4207 - f1: 0.0092\n",
      "Epoch 2/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 8.1044 - f1: 0.0000e+00\n",
      "Epoch 3/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 5.6843 - f1: 0.0000e+00\n",
      "Epoch 4/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 4.4553 - f1: 0.0000e+00\n",
      "Epoch 5/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.9726 - f1: 0.0027\n",
      "Epoch 6/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.6681 - f1: 0.0000e+00\n",
      "Epoch 7/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.5425 - f1: 0.0000e+00\n",
      "Epoch 8/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.4725 - f1: 0.0000e+00\n",
      "Epoch 9/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.4013 - f1: 0.0000e+00\n",
      "Epoch 10/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.3931 - f1: 0.0000e+00\n",
      "Epoch 11/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.4054 - f1: 0.0000e+00\n",
      "Epoch 12/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3091 - f1: 0.0000e+00\n",
      "Epoch 13/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.3016 - f1: 0.0000e+00\n",
      "Epoch 14/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.3134 - f1: 0.0000e+00\n",
      "Epoch 15/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.3102 - f1: 0.0000e+00\n",
      "Epoch 16/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.3125 - f1: 0.0000e+00\n",
      "Epoch 17/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.3433 - f1: 0.0000e+00\n",
      "Epoch 18/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3198 - f1: 0.0000e+00\n",
      "Epoch 19/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2926 - f1: 0.0000e+00\n",
      "Epoch 20/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.3227 - f1: 0.0000e+00\n",
      "Epoch 21/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2473 - f1: 0.0000e+00\n",
      "Epoch 22/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.3132 - f1: 0.0000e+00\n",
      "Epoch 23/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3164 - f1: 0.0000e+00\n",
      "Epoch 24/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2968 - f1: 0.0000e+00\n",
      "Epoch 25/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.3043 - f1: 0.0000e+00\n",
      "Epoch 26/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2324 - f1: 0.0000e+00\n",
      "Epoch 27/100\n",
      "660/660 [==============================] - 0s 61us/step - loss: 3.2416 - f1: 0.0000e+00\n",
      "Epoch 28/100\n",
      "660/660 [==============================] - 0s 54us/step - loss: 3.2470 - f1: 0.0000e+00\n",
      "Epoch 29/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2674 - f1: 0.0000e+00\n",
      "Epoch 30/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2411 - f1: 0.0000e+00\n",
      "Epoch 31/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2322 - f1: 0.0000e+00\n",
      "Epoch 32/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2211 - f1: 0.0000e+00\n",
      "Epoch 33/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.2485 - f1: 0.0000e+00\n",
      "Epoch 34/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2177 - f1: 0.0000e+00\n",
      "Epoch 35/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2566 - f1: 0.0000e+00\n",
      "Epoch 36/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2621 - f1: 0.0000e+00\n",
      "Epoch 37/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2219 - f1: 0.0000e+00\n",
      "Epoch 38/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2924 - f1: 0.0000e+00\n",
      "Epoch 39/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2405 - f1: 0.0000e+00\n",
      "Epoch 40/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2552 - f1: 0.0000e+00\n",
      "Epoch 41/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2913 - f1: 0.0000e+00\n",
      "Epoch 42/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2222 - f1: 0.0000e+00\n",
      "Epoch 43/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3099 - f1: 0.0000e+00\n",
      "Epoch 44/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2060 - f1: 0.0000e+00\n",
      "Epoch 45/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2371 - f1: 0.0000e+00\n",
      "Epoch 46/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2914 - f1: 0.0000e+00\n",
      "Epoch 47/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.1982 - f1: 0.0000e+00\n",
      "Epoch 48/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3065 - f1: 0.0000e+00\n",
      "Epoch 49/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2318 - f1: 0.0000e+00\n",
      "Epoch 50/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2557 - f1: 0.0000e+00\n",
      "Epoch 51/100\n",
      "660/660 [==============================] - 0s 54us/step - loss: 3.2743 - f1: 0.0000e+00\n",
      "Epoch 52/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2109 - f1: 0.0000e+00\n",
      "Epoch 53/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.3255 - f1: 0.0000e+00\n",
      "Epoch 54/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2467 - f1: 0.0000e+00\n",
      "Epoch 55/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2648 - f1: 0.0000e+00\n",
      "Epoch 56/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2836 - f1: 0.0000e+00\n",
      "Epoch 57/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2140 - f1: 0.0000e+00\n",
      "Epoch 58/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3496 - f1: 0.0000e+00\n",
      "Epoch 59/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2356 - f1: 0.0000e+00\n",
      "Epoch 60/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2589 - f1: 0.0000e+00\n",
      "Epoch 61/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.3185 - f1: 0.0000e+00\n",
      "Epoch 62/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2026 - f1: 0.0000e+00\n",
      "Epoch 63/100\n",
      "660/660 [==============================] - 0s 54us/step - loss: 3.2977 - f1: 0.0000e+00\n",
      "Epoch 64/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2258 - f1: 0.0000e+00\n",
      "Epoch 65/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2932 - f1: 0.0000e+00\n",
      "Epoch 66/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2700 - f1: 0.0000e+00\n",
      "Epoch 67/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2210 - f1: 0.0000e+00\n",
      "Epoch 68/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.2689 - f1: 0.0000e+00\n",
      "Epoch 69/100\n",
      "660/660 [==============================] - 0s 54us/step - loss: 3.2331 - f1: 0.0000e+00\n",
      "Epoch 70/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2973 - f1: 0.0000e+00\n",
      "Epoch 71/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.3017 - f1: 0.0000e+00\n",
      "Epoch 72/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2731 - f1: 0.0000e+00\n",
      "Epoch 73/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2866 - f1: 0.0000e+00\n",
      "Epoch 74/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2824 - f1: 0.0000e+00\n",
      "Epoch 75/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2480 - f1: 0.0000e+00\n",
      "Epoch 76/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2953 - f1: 0.0000e+00\n",
      "Epoch 77/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.3097 - f1: 0.0000e+00\n",
      "Epoch 78/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2730 - f1: 0.0000e+00\n",
      "Epoch 79/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2918 - f1: 0.0000e+00\n",
      "Epoch 80/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2549 - f1: 0.0000e+00\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660/660 [==============================] - 0s 56us/step - loss: 3.2803 - f1: 0.0000e+00\n",
      "Epoch 82/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2672 - f1: 0.0000e+00\n",
      "Epoch 83/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2878 - f1: 0.0000e+00\n",
      "Epoch 84/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2807 - f1: 0.0000e+00\n",
      "Epoch 85/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2556 - f1: 0.0000e+00\n",
      "Epoch 86/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.2847 - f1: 0.0000e+00\n",
      "Epoch 87/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2692 - f1: 0.0000e+00\n",
      "Epoch 88/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2382 - f1: 0.0000e+00\n",
      "Epoch 89/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.3142 - f1: 0.0000e+00\n",
      "Epoch 90/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2658 - f1: 0.0000e+00\n",
      "Epoch 91/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2670 - f1: 0.0000e+00\n",
      "Epoch 92/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2931 - f1: 0.0000e+00\n",
      "Epoch 93/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2515 - f1: 0.0000e+00\n",
      "Epoch 94/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.3084 - f1: 0.0000e+00\n",
      "Epoch 95/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2550 - f1: 0.0000e+00\n",
      "Epoch 96/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.3013 - f1: 0.0000e+00\n",
      "Epoch 97/100\n",
      "660/660 [==============================] - 0s 54us/step - loss: 3.2794 - f1: 0.0000e+00\n",
      "Epoch 98/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2594 - f1: 0.0000e+00\n",
      "Epoch 99/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2940 - f1: 0.0000e+00\n",
      "Epoch 100/100\n",
      "660/660 [==============================] - 0s 54us/step - loss: 3.2368 - f1: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold = 4\n",
      "(398, 31)\n",
      "[(1, 102), (2, 40), (3, 91), (4, 165)]\n",
      "[(1, 165), (2, 165), (3, 165), (4, 165)]\n",
      "Epoch 1/100\n",
      "660/660 [==============================] - 4s 6ms/step - loss: 15.6304 - f1: 0.0054\n",
      "Epoch 2/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 7.9223 - f1: 0.0000e+00\n",
      "Epoch 3/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 5.7623 - f1: 0.0201\n",
      "Epoch 4/100\n",
      "660/660 [==============================] - 0s 54us/step - loss: 4.5691 - f1: 0.0563\n",
      "Epoch 5/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 4.0326 - f1: 0.0138\n",
      "Epoch 6/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.6490 - f1: 0.0323\n",
      "Epoch 7/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.4418 - f1: 0.0437\n",
      "Epoch 8/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.4333 - f1: 0.0069\n",
      "Epoch 9/100\n",
      "660/660 [==============================] - 0s 61us/step - loss: 3.3313 - f1: 0.0325\n",
      "Epoch 10/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3908 - f1: 0.0355\n",
      "Epoch 11/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.3182 - f1: 0.0512\n",
      "Epoch 12/100\n",
      "660/660 [==============================] - 0s 54us/step - loss: 3.2688 - f1: 0.0768\n",
      "Epoch 13/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3523 - f1: 0.0373\n",
      "Epoch 14/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2767 - f1: 0.0929\n",
      "Epoch 15/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2768 - f1: 0.0842\n",
      "Epoch 16/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2511 - f1: 0.0546\n",
      "Epoch 17/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2413 - f1: 0.0869\n",
      "Epoch 18/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.3604 - f1: 0.0402\n",
      "Epoch 19/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2621 - f1: 0.0627\n",
      "Epoch 20/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2445 - f1: 0.0895\n",
      "Epoch 21/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2816 - f1: 0.0722\n",
      "Epoch 22/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2889 - f1: 0.1024\n",
      "Epoch 23/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2820 - f1: 0.0199\n",
      "Epoch 24/100\n",
      "660/660 [==============================] - 0s 54us/step - loss: 3.2381 - f1: 0.0834\n",
      "Epoch 25/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2533 - f1: 0.0549\n",
      "Epoch 26/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2307 - f1: 0.0543\n",
      "Epoch 27/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.2094 - f1: 0.0664\n",
      "Epoch 28/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.2385 - f1: 0.0669\n",
      "Epoch 29/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2575 - f1: 0.0590\n",
      "Epoch 30/100\n",
      "660/660 [==============================] - 0s 54us/step - loss: 3.2065 - f1: 0.0899\n",
      "Epoch 31/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2268 - f1: 0.0588\n",
      "Epoch 32/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1762 - f1: 0.0605\n",
      "Epoch 33/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2389 - f1: 0.0507\n",
      "Epoch 34/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2516 - f1: 0.0502\n",
      "Epoch 35/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2279 - f1: 0.0704\n",
      "Epoch 36/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2569 - f1: 0.0501\n",
      "Epoch 37/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.1880 - f1: 0.0622\n",
      "Epoch 38/100\n",
      "660/660 [==============================] - 0s 54us/step - loss: 3.2792 - f1: 0.0470\n",
      "Epoch 39/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2238 - f1: 0.0423\n",
      "Epoch 40/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2442 - f1: 0.0486\n",
      "Epoch 41/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2615 - f1: 0.0598\n",
      "Epoch 42/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.1941 - f1: 0.0558\n",
      "Epoch 43/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2865 - f1: 0.0536\n",
      "Epoch 44/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1867 - f1: 0.0616\n",
      "Epoch 45/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2179 - f1: 0.0832\n",
      "Epoch 46/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2857 - f1: 0.0560\n",
      "Epoch 47/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2201 - f1: 0.0621\n",
      "Epoch 48/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2306 - f1: 0.0768\n",
      "Epoch 49/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2418 - f1: 0.0540\n",
      "Epoch 50/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2619 - f1: 0.0566\n",
      "Epoch 51/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2960 - f1: 0.0420\n",
      "Epoch 52/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.1748 - f1: 0.0522\n",
      "Epoch 53/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2989 - f1: 0.0698\n",
      "Epoch 54/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2423 - f1: 0.1027\n",
      "Epoch 55/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2489 - f1: 0.1202\n",
      "Epoch 56/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2731 - f1: 0.0934\n",
      "Epoch 57/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.1991 - f1: 0.1015\n",
      "Epoch 58/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.3251 - f1: 0.1002\n",
      "Epoch 59/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2376 - f1: 0.1028\n",
      "Epoch 60/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2517 - f1: 0.0717\n",
      "Epoch 61/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2691 - f1: 0.0799\n",
      "Epoch 62/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2163 - f1: 0.0979\n",
      "Epoch 63/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2923 - f1: 0.1088\n",
      "Epoch 64/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1852 - f1: 0.0959\n",
      "Epoch 65/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2807 - f1: 0.1232\n",
      "Epoch 66/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2625 - f1: 0.1200\n",
      "Epoch 67/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2196 - f1: 0.1266\n",
      "Epoch 68/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.2726 - f1: 0.0921\n",
      "Epoch 69/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2226 - f1: 0.1207\n",
      "Epoch 70/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2988 - f1: 0.1282\n",
      "Epoch 71/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.2724 - f1: 0.1465\n",
      "Epoch 72/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2490 - f1: 0.1019\n",
      "Epoch 73/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2577 - f1: 0.0988\n",
      "Epoch 74/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2538 - f1: 0.0972\n",
      "Epoch 75/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2560 - f1: 0.0895\n",
      "Epoch 76/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.2447 - f1: 0.0695\n",
      "Epoch 77/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2742 - f1: 0.0748\n",
      "Epoch 78/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2901 - f1: 0.0772\n",
      "Epoch 79/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2634 - f1: 0.0574\n",
      "Epoch 80/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2601 - f1: 0.0482\n",
      "Epoch 81/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.2859 - f1: 0.0328\n",
      "Epoch 82/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2877 - f1: 0.0550\n",
      "Epoch 83/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2978 - f1: 0.0518\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660/660 [==============================] - 0s 61us/step - loss: 3.2514 - f1: 0.0645\n",
      "Epoch 85/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2761 - f1: 0.0546\n",
      "Epoch 86/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2607 - f1: 0.0467\n",
      "Epoch 87/100\n",
      "660/660 [==============================] - 0s 54us/step - loss: 3.2709 - f1: 0.0603\n",
      "Epoch 88/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2365 - f1: 0.0752\n",
      "Epoch 89/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2652 - f1: 0.0633\n",
      "Epoch 90/100\n",
      "660/660 [==============================] - 0s 54us/step - loss: 3.2784 - f1: 0.0452\n",
      "Epoch 91/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2226 - f1: 0.0527\n",
      "Epoch 92/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2206 - f1: 0.0376\n",
      "Epoch 93/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2526 - f1: 0.0315\n",
      "Epoch 94/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2771 - f1: 0.0506\n",
      "Epoch 95/100\n",
      "660/660 [==============================] - 0s 54us/step - loss: 3.2565 - f1: 0.0411\n",
      "Epoch 96/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2406 - f1: 0.0491\n",
      "Epoch 97/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2450 - f1: 0.0577\n",
      "Epoch 98/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2623 - f1: 0.0448\n",
      "Epoch 99/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2644 - f1: 0.0560\n",
      "Epoch 100/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2225 - f1: 0.0509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold = 5\n",
      "(398, 31)\n",
      "[(1, 102), (2, 40), (3, 91), (4, 165)]\n",
      "[(1, 165), (2, 165), (3, 165), (4, 165)]\n",
      "Epoch 1/100\n",
      "660/660 [==============================] - 4s 6ms/step - loss: 15.6499 - f1: 0.0000e+00\n",
      "Epoch 2/100\n",
      "660/660 [==============================] - 0s 63us/step - loss: 8.3042 - f1: 0.0000e+00\n",
      "Epoch 3/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 5.8666 - f1: 0.0000e+00\n",
      "Epoch 4/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 4.6119 - f1: 0.0022\n",
      "Epoch 5/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 4.0227 - f1: 0.0000e+00\n",
      "Epoch 6/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.7012 - f1: 0.0000e+00\n",
      "Epoch 7/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.4679 - f1: 0.0000e+00\n",
      "Epoch 8/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.3706 - f1: 0.0000e+00\n",
      "Epoch 9/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.3363 - f1: 0.0000e+00\n",
      "Epoch 10/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.3473 - f1: 0.0000e+00\n",
      "Epoch 11/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2782 - f1: 0.0000e+00\n",
      "Epoch 12/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2337 - f1: 0.0000e+00\n",
      "Epoch 13/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2569 - f1: 0.0000e+00\n",
      "Epoch 14/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2065 - f1: 0.0000e+00\n",
      "Epoch 15/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2543 - f1: 0.0000e+00\n",
      "Epoch 16/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2601 - f1: 0.0000e+00\n",
      "Epoch 17/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.2097 - f1: 0.0000e+00\n",
      "Epoch 18/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1944 - f1: 0.0000e+00\n",
      "Epoch 19/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2206 - f1: 0.0000e+00\n",
      "Epoch 20/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2439 - f1: 0.0000e+00\n",
      "Epoch 21/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1813 - f1: 0.0000e+00\n",
      "Epoch 22/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2039 - f1: 0.0000e+00\n",
      "Epoch 23/100\n",
      "660/660 [==============================] - 0s 62us/step - loss: 3.2144 - f1: 0.0000e+00\n",
      "Epoch 24/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.1701 - f1: 0.0000e+00\n",
      "Epoch 25/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.1940 - f1: 0.0000e+00\n",
      "Epoch 26/100\n",
      "660/660 [==============================] - 0s 63us/step - loss: 3.1561 - f1: 0.0000e+00\n",
      "Epoch 27/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2129 - f1: 0.0000e+00\n",
      "Epoch 28/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2164 - f1: 0.0000e+00\n",
      "Epoch 29/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1463 - f1: 0.0000e+00\n",
      "Epoch 30/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.1341 - f1: 0.0000e+00\n",
      "Epoch 31/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.1345 - f1: 0.0000e+00\n",
      "Epoch 32/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.1665 - f1: 0.0000e+00\n",
      "Epoch 33/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.1774 - f1: 0.0000e+00\n",
      "Epoch 34/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1419 - f1: 0.0000e+00\n",
      "Epoch 35/100\n",
      "660/660 [==============================] - 0s 62us/step - loss: 3.1898 - f1: 0.0000e+00\n",
      "Epoch 36/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1666 - f1: 0.0000e+00\n",
      "Epoch 37/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.1235 - f1: 0.0000e+00\n",
      "Epoch 38/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.1932 - f1: 0.0000e+00\n",
      "Epoch 39/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.1484 - f1: 0.0000e+00\n",
      "Epoch 40/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.1972 - f1: 0.0000e+00\n",
      "Epoch 41/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2118 - f1: 0.0000e+00\n",
      "Epoch 42/100\n",
      "660/660 [==============================] - 0s 61us/step - loss: 3.1297 - f1: 0.0000e+00\n",
      "Epoch 43/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.1845 - f1: 0.0000e+00\n",
      "Epoch 44/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.1261 - f1: 0.0000e+00\n",
      "Epoch 45/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1948 - f1: 0.0000e+00\n",
      "Epoch 46/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2173 - f1: 0.0000e+00\n",
      "Epoch 47/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.1229 - f1: 0.0000e+00\n",
      "Epoch 48/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2033 - f1: 0.0000e+00\n",
      "Epoch 49/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.1658 - f1: 0.0000e+00\n",
      "Epoch 50/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.1602 - f1: 0.0000e+00\n",
      "Epoch 51/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2065 - f1: 0.0000e+00\n",
      "Epoch 52/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1389 - f1: 0.0000e+00\n",
      "Epoch 53/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.2451 - f1: 0.0000e+00\n",
      "Epoch 54/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.1671 - f1: 0.0000e+00\n",
      "Epoch 55/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1476 - f1: 0.0000e+00\n",
      "Epoch 56/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2217 - f1: 0.0000e+00\n",
      "Epoch 57/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.1502 - f1: 0.0000e+00\n",
      "Epoch 58/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2239 - f1: 0.0000e+00\n",
      "Epoch 59/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1878 - f1: 0.0000e+00\n",
      "Epoch 60/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1542 - f1: 0.0000e+00\n",
      "Epoch 61/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2346 - f1: 0.0000e+00\n",
      "Epoch 62/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1338 - f1: 0.0000e+00\n",
      "Epoch 63/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2120 - f1: 0.0000e+00\n",
      "Epoch 64/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.1816 - f1: 0.0000e+00\n",
      "Epoch 65/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2039 - f1: 0.0000e+00\n",
      "Epoch 66/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.2171 - f1: 0.0000e+00\n",
      "Epoch 67/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1531 - f1: 0.0000e+00\n",
      "Epoch 68/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1910 - f1: 0.0000e+00\n",
      "Epoch 69/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.1866 - f1: 0.0000e+00\n",
      "Epoch 70/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.1920 - f1: 0.0000e+00\n",
      "Epoch 71/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2174 - f1: 0.0000e+00\n",
      "Epoch 72/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.1706 - f1: 0.0000e+00\n",
      "Epoch 73/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.1984 - f1: 0.0000e+00\n",
      "Epoch 74/100\n",
      "660/660 [==============================] - 0s 61us/step - loss: 3.1885 - f1: 0.0000e+00\n",
      "Epoch 75/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.1761 - f1: 0.0000e+00\n",
      "Epoch 76/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2010 - f1: 0.0000e+00\n",
      "Epoch 77/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2109 - f1: 0.0000e+00\n",
      "Epoch 78/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.1863 - f1: 0.0000e+00\n",
      "Epoch 79/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.1880 - f1: 0.0000e+00\n",
      "Epoch 80/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.1739 - f1: 0.0000e+00\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "660/660 [==============================] - 0s 58us/step - loss: 3.2082 - f1: 0.0000e+00\n",
      "Epoch 82/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.1992 - f1: 0.0000e+00\n",
      "Epoch 83/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.1960 - f1: 0.0000e+00\n",
      "Epoch 84/100\n",
      "660/660 [==============================] - 0s 59us/step - loss: 3.1835 - f1: 0.0000e+00\n",
      "Epoch 85/100\n",
      "660/660 [==============================] - 0s 55us/step - loss: 3.1916 - f1: 0.0000e+00\n",
      "Epoch 86/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1837 - f1: 0.0000e+00\n",
      "Epoch 87/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.1968 - f1: 0.0000e+00\n",
      "Epoch 88/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.1796 - f1: 0.0000e+00\n",
      "Epoch 89/100\n",
      "660/660 [==============================] - 0s 60us/step - loss: 3.2138 - f1: 0.0000e+00\n",
      "Epoch 90/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2058 - f1: 0.0000e+00\n",
      "Epoch 91/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.1650 - f1: 0.0000e+00\n",
      "Epoch 92/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.1961 - f1: 0.0000e+00\n",
      "Epoch 93/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1962 - f1: 0.0000e+00\n",
      "Epoch 94/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.2006 - f1: 0.0000e+00\n",
      "Epoch 95/100\n",
      "660/660 [==============================] - 0s 58us/step - loss: 3.2134 - f1: 0.0000e+00\n",
      "Epoch 96/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.1705 - f1: 0.0000e+00\n",
      "Epoch 97/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2129 - f1: 0.0000e+00\n",
      "Epoch 98/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.1769 - f1: 0.0000e+00\n",
      "Epoch 99/100\n",
      "660/660 [==============================] - 0s 56us/step - loss: 3.1904 - f1: 0.0000e+00\n",
      "Epoch 100/100\n",
      "660/660 [==============================] - 0s 57us/step - loss: 3.2005 - f1: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "classifier = 'nn'\n",
    "\n",
    "ave_f1_scores = list()\n",
    "\n",
    "print('5 fold CV starting')\n",
    "for fold_ix in range(1,6):\n",
    "    \n",
    "    print('\\nFold = {0}'.format(fold_ix))\n",
    "    \n",
    "    train_ix = k_fold[(k_fold['fold'] == fold_ix) & (k_fold['train_text'] == 'training')]['user_id']\n",
    "    test_ix = k_fold[(k_fold['fold'] == fold_ix) & (k_fold['train_text'] == 'test')]['user_id']\n",
    "    \n",
    "    x_train = data[data.index.isin(train_ix)].copy()\n",
    "    x_test = data[data.index.isin(test_ix)].copy()\n",
    "    \n",
    "    x_train = x_train.fillna(0)\n",
    "    x_test = x_test.fillna(0)\n",
    "    \n",
    "    y_train = x_train['target']\n",
    "    x_train.drop(['target'], axis=1, inplace=True)\n",
    "    \n",
    "    y_test = x_test['target']\n",
    "    x_test.drop(['target'], axis=1, inplace=True)\n",
    "    \n",
    "    print(x_train.shape)\n",
    "    print(sorted(Counter(y_train).items()))\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "    print(sorted(Counter(y_train).items()))\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.fit_transform(x_test)\n",
    "    \n",
    "    # one-hot encoding\n",
    "    n_y_train = pd.get_dummies(y_train)\n",
    "    n_y_test = pd.get_dummies(y_test)\n",
    "    \n",
    "    if classifier == 'nn':\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(30, input_dim=np.shape(x_train)[1], activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(4, activation='softmax'))\n",
    "        \n",
    "        adam = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "        model = multi_gpu_model(model, gpus=available_gpus)\n",
    "        model.compile(optimizer=adam, loss=f1_loss,\n",
    "                      metrics=[f1])\n",
    "        model.fit(x_train, n_y_train, epochs=100, batch_size=100, verbose=1)\n",
    "        y_pred_train = np.argmax(model.predict(x_train), axis=1) + 1\n",
    "        y_pred_test = np.argmax(model.predict(x_test), axis=1) + 1\n",
    "        y_score = model.predict(x_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "    elif classifier == 'logit':\n",
    "        model = LogisticRegression(class_weight='balanced', n_jobs=-1,\n",
    "                          multi_class='auto', solver='lbfgs',\n",
    "                          tol=0.00001, C=10, max_iter=1, verbose=True,\n",
    "                          random_state=random_state)  \n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred_test = model.predict(x_test)\n",
    "        y_score=model.predict_proba(x_test)\n",
    "        \n",
    "    report_with_auc = class_report(\n",
    "    y_true=y_test, \n",
    "    y_pred=y_pred_test, \n",
    "    y_score=y_score,\n",
    "    average='macro')\n",
    "\n",
    "    ave_f1_scores.append(report_with_auc['f1-score'].values[-1])\n",
    "\n",
    "    cv_column = [fold_ix]\n",
    "    cv_column.extend( [''] * (report_with_auc.index.shape[0] - 1))\n",
    "    report_with_auc['Fold'] = cv_column\n",
    "    report_with_auc['Risk-Factor'] = report_with_auc.index\n",
    "    report_with_auc = report_with_auc.set_index(['Fold', 'Risk-Factor'])\n",
    "\n",
    "    if fold_ix == 1:\n",
    "        report_with_auc_df = report_with_auc.copy()\n",
    "    else:\n",
    "        report_with_auc_df = report_with_auc_df.append(report_with_auc.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>pred-cnt</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold</th>\n",
       "      <th>Risk-Factor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.257426</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.409449</td>\n",
       "      <td>26.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.064356</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.102362</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <td>0.230000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373984</td>\n",
       "      <td>23.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.057500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.093496</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>1</th>\n",
       "      <td>0.252525</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>25.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.587027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.063131</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.100806</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.615405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>1</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>25.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.423562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\"></th>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.494318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>22.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.644737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.086081</td>\n",
       "      <td>0.201818</td>\n",
       "      <td>0.101787</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.532048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <td>0.418367</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.589928</td>\n",
       "      <td>41.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.104592</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.147482</td>\n",
       "      <td>41.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  precision    recall  f1-score  support  pred-cnt       AUC\n",
       "Fold Risk-Factor                                                            \n",
       "1    1             0.257426  1.000000  0.409449     26.0     101.0  0.500000\n",
       "     avg / total   0.064356  0.250000  0.102362     26.0      26.0  0.500000\n",
       "2    3             0.230000  1.000000  0.373984     23.0     100.0  0.500000\n",
       "     avg / total   0.057500  0.250000  0.093496     23.0      23.0  0.500000\n",
       "3    1             0.252525  1.000000  0.403226     25.0      99.0  0.587027\n",
       "     avg / total   0.063131  0.250000  0.100806     25.0      25.0  0.615405\n",
       "4    1             0.153846  0.080000  0.105263     25.0      13.0  0.423562\n",
       "     2             0.000000  0.000000  0.000000     10.0       1.0  0.494318\n",
       "     3             0.190476  0.727273  0.301887     22.0      84.0  0.644737\n",
       "     avg / total   0.086081  0.201818  0.101787     57.0      57.0  0.532048\n",
       "5    4             0.418367  1.000000  0.589928     41.0      98.0  0.500000\n",
       "     avg / total   0.104592  0.250000  0.147482     41.0      41.0  0.500000"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_with_auc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average f1-score (all folds) = 0.10918681865440867\n"
     ]
    }
   ],
   "source": [
    "print('average f1-score (all folds) = {0}'.format(np.mean(ave_f1_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'all'    \n",
    "'SETAAA' \n",
    "'SETBBB' \n",
    "'SETCCC'\n",
    "'SETDDD'\n",
    "'SETEEE'\n",
    "'SETFFF' \n",
    "'SETGGG' \n",
    "'SETHHH' \n",
    "'SETIII' \n",
    "'SETJJJ' \n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETFFF', 'SETGGG'\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETEEE'\n",
    "'SETAAA', 'SETKKK' 0.20175058667560125 (droptout)\n",
    "'SETAAA', 'SETLLL' 0.12430375888494909 (dropout)\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETLLL', 'SETHHH', 'SETGGG' 0.11088291908006576 (dropout)\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETLLL', 'SETHHH', 'SETGGG' 0.07753291443154947 (reg)\n",
    "'SETAAA', 'SETDDD', 'SETEEE'  0.17349602442078949 (drop)\n",
    "\n",
    "(nn) (dropout)\n",
    "'SETAAA', 'SETDDD', 'SETEEE' 0.19014727458966665\n",
    "\n",
    "(nn) (dropout) (1hl)\n",
    "'SETAAA', 'SETDDD', 'SETEEE' 0.3255926882010475\n",
    "\n",
    "(logit)\n",
    "'SETAAA', 'SETDDD', 'SETEEE'  0.32241407748653905 \n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETLLL' 0.24496511781531444\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETFFF', 'SETGGG' 0.11353938632270907\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETFFF' 0.12148581335816777\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETGGG' 0.25394562653592845"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
