{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook with code use to normalize the filtered subreddit comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project path = C:\\Users\\e_guerramarj\\github\\clpsych\n",
      "data path = D:\\Dataset\\clpsych\\dataset\n",
      "\n",
      "sys.path = ['', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp\\\\python36.zip', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp\\\\DLLs', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp\\\\lib', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\site-packages', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\e_guerramarj\\\\.ipython', 'C:\\\\Users\\\\e_guerramarj\\\\github\\\\clpsych', 'C:\\\\Users\\\\e_guerramarj\\\\github\\\\clpsych\\\\utils']\n"
     ]
    }
   ],
   "source": [
    "project_name = 'clpsych'\n",
    "project_path = Path(os.getcwd()).parent\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    data_path = 'D:\\Dataset\\{0}\\dataset'.format(project_name)\n",
    "elif sys.platform == 'darwin':\n",
    "    data_path = '/Volumes/Dataset/{0}/dataset'.format(project_name)\n",
    "else:\n",
    "    data_path = Path(project_path, 'dataset')\n",
    "\n",
    "utils_path = str(Path(project_path, 'utils'))\n",
    "# including the project folder and the utils folder\n",
    "if utils_path not in ''.join(sys.path):\n",
    "    sys.path.extend([str(project_path), utils_path])\n",
    "\n",
    "print('project path = {0}'.format(project_path))\n",
    "print('data path = {0}'.format(data_path))\n",
    "print('')\n",
    "print('sys.path = {0}'.format(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to normalize contents\n",
    "def clean_str(string):\n",
    "     # float occurs when the string is emtpy\n",
    "    if not pd.isnull(string):\n",
    "        print('\\t\\t len={0}'.format(len(string)))\n",
    "        \"\"\"\n",
    "        Tokenization/string cleaning for datasets.\n",
    "        Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "        \"\"\"\n",
    "        string = string.replace('  ','')\n",
    "        string = re.sub(r'((http|https|ftp|ftps)\\:\\/\\/[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(\\/\\S*)?)', '_URL_', string)\n",
    "        string = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '_EMAIL_', string)\n",
    "        string = re.sub(r'address = ([0-9]{1,3}[\\.]){3}[0-9]{1,3}', '_IP_', string)\n",
    "        string = string.replace('[deleted]', '')\n",
    "        return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_path = Path(data_path, 'submissions')\n",
    "# get all the json files and their stem\n",
    "processed_files = [element.replace('norm.csv', '') \n",
    "                   for element in os.listdir(rs_path) if 'norm.csv' in element]\n",
    "\n",
    "need_to_process = ['RS_2016-02',\n",
    " 'RS_2016-03',\n",
    " 'RS_2016-04',\n",
    " 'RS_2016-05']\n",
    "\n",
    "for file in rs_path.iterdir():\n",
    "    if file.suffix == '.csv' and file.stem not in processed_files and not file.is_dir() and file.stem in need_to_process:\n",
    "        print('file = {0}'.format(file.stem))\n",
    "        print('\\t reading - {0}'.format( datetime.now()))\n",
    "        data = pd.read_csv(file)\n",
    "        total = data.shape[0]\n",
    "        print('\\t processing - {0}'.format(datetime.now()))\n",
    "        final = list()\n",
    "        for row_ix, row in enumerate(data['title'] + data['selftext']):\n",
    "            print('\\t\\t {0}/{1}'.format(row_ix, total))\n",
    "            final.append(clean_str(row))\n",
    "        print('\\t storing - {0}'.format(datetime.now()))\n",
    "        n_data = pd.DataFrame({'title_body': final})\n",
    "        new_filename = file.stem + '_norm.csv'\n",
    "        n_data.to_csv(Path(rs_path, new_filename), index=False)\n",
    "        print('\\t finished {0}'.format(datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
