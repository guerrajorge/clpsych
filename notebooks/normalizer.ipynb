{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook with code use to normalize the filtered subreddit comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'clpsych'\n",
    "project_path = Path(os.getcwd()).parent\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    data_path = 'D:\\Dataset\\{0}\\dataset'.format(project_name)\n",
    "elif sys.platform == 'darwin':\n",
    "    data_path = '/Volumes/Dataset/{0}/dataset'.format(project_name)\n",
    "else:\n",
    "    data_path = Path(project_path, 'dataset')\n",
    "\n",
    "utils_path = str(Path(project_path, 'utils'))\n",
    "# including the project folder and the utils folder\n",
    "if utils_path not in ''.join(sys.path):\n",
    "    sys.path.extend([str(project_path), utils_path])\n",
    "\n",
    "print('project path = {0}'.format(project_path))\n",
    "print('data path = {0}'.format(data_path))\n",
    "print('')\n",
    "print('sys.path = {0}'.format(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to normalize contents\n",
    "def clean_str(string):\n",
    "     # float occurs when the string is emtpy\n",
    "    if not pd.isnull(string):\n",
    "        print('\\t\\t len={0}'.format(len(string)))\n",
    "        \"\"\"\n",
    "        Tokenization/string cleaning for datasets.\n",
    "        Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "        \"\"\"\n",
    "        string = string.replace('  ','')\n",
    "        string = re.sub(r'((http|https|ftp|ftps)\\:\\/\\/[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(\\/\\S*)?)', '_URL_', string)\n",
    "        string = re.sub(r'[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(\\/\\S*)?', '_URL_', string)\n",
    "        string = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '_EMAIL_', string)\n",
    "        string = re.sub(r'address = ([0-9]{1,3}[\\.]){3}[0-9]{1,3}', '_IP_', string)\n",
    "        string = string.replace('[deleted]', '')\n",
    "        return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_path = Path(data_path, 'submissions')\n",
    "# get all the json files and their stem\n",
    "processed_files = list()\n",
    "\n",
    "for element in os.listdir(rs_path):\n",
    "    if 'norm' in element:\n",
    "        name = ''\n",
    "        if '_norm_stats.csv' in element:\n",
    "            name = element.replace('_norm_stats.csv', '')\n",
    "        else:\n",
    "            name = element.replace('_norm.csv', '')\n",
    "        processed_files.append(name)\n",
    "\n",
    "need_to_process = ['']\n",
    "\n",
    "for file in rs_path.iterdir():\n",
    "    if file.suffix == '.csv' and file.stem not in processed_files and not file.is_dir() and file.stem in need_to_process:\n",
    "        print('file = {0}'.format(file.stem))\n",
    "        print('\\t reading - {0}'.format( datetime.now()))\n",
    "        data = pd.read_csv(file)\n",
    "        total = data.shape[0]\n",
    "        print('\\t processing - {0}'.format(datetime.now()))\n",
    "        final = list()\n",
    "        for row in tqdm(data['title'] + data['selftext']):\n",
    "            final.append(clean_str(row))\n",
    "        print('\\t storing - {0}'.format(datetime.now()))\n",
    "        n_data = pd.DataFrame({'title_body': final})\n",
    "        n_data['subreddit'] = data['subreddit']\n",
    "        new_filename = file.stem + '_norm.csv'\n",
    "        n_data.to_csv(Path(rs_path, new_filename), index=False)\n",
    "        print('\\t stats - {0}'.format(datetime.now()))\n",
    "        stats = n_data.groupby('subreddit').count()\n",
    "        stats['total posts'] = ''\n",
    "        stats['total posts'].iloc[0] = data.shape[0]\n",
    "        stats_filename = file.stem + '_norm_stats.csv'\n",
    "        stats_file = Path(rs_path, stats_filename)\n",
    "        stats.to_csv(stats_file)\n",
    "        print('\\t finished {0}'.format(datetime.now()))\n",
    "        processed_files.append(file.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_path = Path(data_path, 'submissions')\n",
    "\n",
    "\n",
    "\n",
    "need_to_process_files = [element.replace('.csv', '') \n",
    "                   for element in os.listdir(rs_path) if ('.csv' in element) and ('norm' not in element) and (element.replace('.csv', '') not in processed_files)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
