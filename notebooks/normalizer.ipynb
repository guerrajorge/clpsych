{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook with code use to normalize the filtered subreddit comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project path = C:\\Users\\e_guerramarj\\github\\clpsych\n",
      "data path = D:\\Dataset\\clpsych\\dataset\n",
      "\n",
      "sys.path = ['', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp\\\\python36.zip', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp\\\\DLLs', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp\\\\lib', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\site-packages', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\e_guerramarj\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\nlp\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\e_guerramarj\\\\.ipython', 'C:\\\\Users\\\\e_guerramarj\\\\github\\\\clpsych', 'C:\\\\Users\\\\e_guerramarj\\\\github\\\\clpsych\\\\utils']\n"
     ]
    }
   ],
   "source": [
    "project_name = 'clpsych'\n",
    "project_path = Path(os.getcwd()).parent\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    data_path = 'D:\\Dataset\\{0}\\dataset'.format(project_name)\n",
    "elif sys.platform == 'darwin':\n",
    "    data_path = '/Volumes/Dataset/{0}/dataset'.format(project_name)\n",
    "else:\n",
    "    data_path = Path(project_path, 'dataset')\n",
    "\n",
    "utils_path = str(Path(project_path, 'utils'))\n",
    "# including the project folder and the utils folder\n",
    "if utils_path not in ''.join(sys.path):\n",
    "    sys.path.extend([str(project_path), utils_path])\n",
    "\n",
    "print('project path = {0}'.format(project_path))\n",
    "print('data path = {0}'.format(data_path))\n",
    "print('')\n",
    "print('sys.path = {0}'.format(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to normalize contents\n",
    "def clean_str(string):\n",
    "    # float occurs when the string is emtpy\n",
    "    if type(string) != float:\n",
    "        \"\"\"\n",
    "        Tokenization/string cleaning for datasets.\n",
    "        Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "        \"\"\"\n",
    "        string = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', '_URL_', string)\n",
    "        string = re.sub(r'[\\W\\.-]+@[\\W\\.-]+', '_EMAIL_', string)\n",
    "        string = re.sub(r'address = \\S+', '_IP_', string)\n",
    "        return string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_path = Path(data_path, 'submissions')\n",
    "# get all the json files and their stem\n",
    "processed_files = [element.replace('norm.csv', '') \n",
    "                   for element in os.listdir(rs_path) if 'norm.csv' in element]\n",
    "\n",
    "need_to_process = ['RS_2016-02',\n",
    " 'RS_2016-03',\n",
    " 'RS_2016-04',\n",
    " 'RS_2016-05']\n",
    "\n",
    "for file in rs_path.iterdir():\n",
    "    if file.suffix == '.csv' and file.stem not in processed_files and not file.is_dir() and file.stem in need_to_process:\n",
    "        print('reading = {0} {1}'.format(file, datetime.now()))\n",
    "        data = pd.read_csv(file)\n",
    "        print('\\t processing titles {1}'.format(file, datetime.now()))\n",
    "        n_data = pd.DataFrame(columns=['title', 'selftext'])\n",
    "        n_data['title'] = data['title'].apply(clean_str)\n",
    "        print('\\t processing text {1}'.format(file, datetime.now()))\n",
    "        n_data['selftext'] = data['selftext'].apply(clean_str)\n",
    "        print('\\t storing {0}'.format(datetime.now()))\n",
    "        new_filename = file.stem + '_norm.csv'\n",
    "        n_data.to_csv(Path(rs_path, new_filename), index=False)\n",
    "        print('\\t finished {0}'.format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(data_path, 'submissions', 'RS_v2_2006-03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize(data, func):\n",
    "    data_split = np.array_split(data, partitions)\n",
    "    print('data split size = {0}'.format(np.shape(data_split)))\n",
    "    pool = Pool(cores)\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count, Pool\n",
    " \n",
    "\n",
    "cores = cpu_count() - 1 #Number of CPU cores on your system\n",
    "partitions = cores\n",
    "print('cores = {0}'.format(cores))\n",
    "\n",
    "print('reading = {0} {1}'.format(file.stem, datetime.now()))\n",
    "data = pd.read_csv(file)\n",
    "print('processing {1}'.format(file, datetime.now()))\n",
    "data = parallelize(data['title'] + data['selftext'], clean_str);\n",
    "print('storing {0}'.format(datetime.now()))\n",
    "new_filename = file.stem + '_norm.csv'\n",
    "data.to_csv(Path(rs_path, new_filename), index=False)\n",
    "print('finished {0}'.format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading = RS_v2_2006-03 2019-03-12 22:58:22.740624\n",
      "processing (RS_v2_2006-03)\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "pool = mp.Pool(mp.cpu_count())\n",
    "print('reading = {0} {1}'.format(file.stem, datetime.now()))\n",
    "data = pd.read_csv(file)\n",
    "print('processing {0}'.format(datetime.now()))\n",
    "results = pool.map(clean_str, [row for row in data['title'] + data['selftext']])\n",
    "pool.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
