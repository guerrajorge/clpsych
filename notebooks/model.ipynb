{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "project_name = 'clpsych'\n",
    "project_path = Path(os.getcwd()).parent\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    data_path = 'D:\\Dataset\\{0}\\dataset'.format(project_name)\n",
    "    model_path = 'D:\\Dataset\\{0}\\models'.format(project_name)\n",
    "    src_path = '/Volumes/Dataset/{0}/src'.format(project_name)\n",
    "    \n",
    "elif sys.platform == 'darwin':\n",
    "    data_path = '/Volumes/Dataset/{0}/dataset'.format(project_name)\n",
    "    model_path = '/Volumes/Dataset/{0}/models'.format(project_name)\n",
    "    src_path = '/Volumes/Dataset/{0}/src'.format(project_name)\n",
    "    \n",
    "else:\n",
    "    data_path = Path(project_path, 'dataset')\n",
    "    model_path = Path(project_path, 'models')\n",
    "    src_path = Path(project_path, 'src')\n",
    "\n",
    "utils_path = str(Path(project_path, 'utils'))\n",
    "# including the project folder and the utils folder\n",
    "if utils_path not in ''.join(sys.path):\n",
    "    sys.path.extend([str(project_path), utils_path, str(src_path)])\n",
    "\n",
    "print('project path = {0}'.format(project_path))\n",
    "print('data path = {0}'.format(data_path))\n",
    "print('model path = {0}'.format(model_path))\n",
    "print('sys.path = {0}'.format(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from utils.datapath import data_path_scripts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "from numpy import interp \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import re\n",
    "# from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "# from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Dropout\n",
    "# from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam, Adam\n",
    "# from keras.regularizers import l1_l2\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras import backend\n",
    "# from keras import metrics\n",
    "# import tensorflow as tf\n",
    "import _pickle as pickle \n",
    "import stanfordnlp\n",
    "from data_helpers import load_data_and_labels\n",
    "\n",
    "# %matplotlib inline\n",
    "# plt.rcParams['figure.figsize'] = [20, 13]\n",
    "\n",
    "# seed for numpy and sklearn\n",
    "random_state = 7\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -c conda-forge gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#stanfordnlp.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "print('Tensorflow recognizes GPUs')\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "available_gpu = backend.tensorflow_backend._get_available_gpus()\n",
    "assert len(available_gpu) > 0\n",
    "print('number of available GPUs = {0}'.format(len(available_gpu)))\n",
    "print('list of GPUs = {0}\\n'.format(available_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def resulting_confusion_matrix(y, y_pred, file_name, train_test, fig_number):\n",
    "    \n",
    "    print()\n",
    "    print(\"\\n{0} results\\n\".format(train_test))\n",
    "    \n",
    "    n_true = len(y[y == 0])\n",
    "    n_false = len(y[y == 1])\n",
    "    \n",
    "    print('total data points = {0}'.format(y.shape[0]))\n",
    "    print('true data points = {0} (% {1:.3})'.format(n_true, n_true / np.shape(y)[0]))\n",
    "    print('false data points = {0} (% {1:.3})'.format(n_false, n_false / np.shape(y)[0]))\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y, y_pred)\n",
    "    roc_score = auc(fpr, tpr)\n",
    "    r_auc_score = roc_auc_score(y, y_pred)\n",
    "    \n",
    "    print('roc score = {0}'.format(roc_score))\n",
    "    print('roc auc score = {0}'.format(r_auc_score))\n",
    "    \n",
    "    print()    \n",
    "    print(\"confusion matrix\")\n",
    "\n",
    "    target_names = ['Show', 'No-Show']\n",
    "    print(classification_report(y, y_pred, target_names=target_names))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    class_names = ['Show', 'No-Show']\n",
    "    \n",
    "    plt.figure(fig_number)\n",
    "    plt.subplot(221)\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix, without normalization')\n",
    "    print()\n",
    "    \n",
    "    plt.figure(fig_number)\n",
    "    plt.subplot(222)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "    print()\n",
    "    \n",
    "    plt.figure(fig_number)\n",
    "    plt.subplot(223)\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_score)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def auc_metric(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    backend.get_session().run(tf.local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%ls ../dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "person = 'victor'\n",
    "if person == 'victor':\n",
    "    data = pickle.load(Path(data_path, 'preprocessed_corpus.pkl').open('rb'))\n",
    "elif person == 'wei':\n",
    "    data = load_data_and_labels(file_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data.head()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stan_nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English\n",
    "doc = stan_nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
    "print(doc.sentences[0].print_dependencies())\n",
    "\n",
    "res = stan_nlp.annotate(\"I love you. I hate him. You are nice. He is dumb\",\n",
    "                   properties={\n",
    "                       'annotators': 'sentiment',\n",
    "                       'outputFormat': 'json',\n",
    "                       'timeout': 1000,\n",
    "                   })\n",
    "for s in res[\"sentences\"]:\n",
    "    print(\"%d: '%s': %s %s\" % (\n",
    "        s[\"index\"],\n",
    "        \" \".join([t[\"word\"] for t in s[\"tokens\"]]),\n",
    "        s[\"sentimentValue\"], s[\"sentiment\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning_py372",
   "language": "python",
   "name": "deeplearning_py372"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
