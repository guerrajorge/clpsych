{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple sentence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'clpsych'\n",
    "project_path = Path(os.getcwd()).parent\n",
    "src_path = Path(project_path, 'src')\n",
    "utils_path = Path(project_path, 'utils')\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    data_path = 'D:\\Dataset\\{0}\\dataset'.format(project_name)\n",
    "    embedding_path = 'D:\\Dataset\\{0}\\embedding'.format(project_name)\n",
    "    model_path = 'D:\\Dataset\\{0}\\embedding'.format(project_name)\n",
    "    \n",
    "elif sys.platform == 'darwin':\n",
    "    data_path = '/Volumes/Dataset/{0}/dataset'.format(project_name)\n",
    "    embedding_path = '/Volumes/Dataset/{0}/embedding'.format(project_name)\n",
    "    \n",
    "else:\n",
    "    data_path = Path(project_path, 'dataset')\n",
    "    model_path = Path(project_path, 'models')\n",
    "    embedding_path = Path(project_path, 'embedding')\n",
    "\n",
    "# including the project folder and the utils folder\n",
    "if str(utils_path) not in ''.join(sys.path):\n",
    "    sys.path.extend([str(project_path), str(utils_path), str(src_path)])\n",
    "\n",
    "print('project path = {0}'.format(project_path))\n",
    "print('data path = {0}'.format(data_path))\n",
    "print('model path = {0}'.format(model_path))\n",
    "print('embedding path = {0}'.format(embedding_path))\n",
    "print('sys.path = {0}'.format(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "# from utils.result_vis import class_report\n",
    "# flatten list\n",
    "from functools import reduce\n",
    "from operator import iconcat\n",
    "# processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "\n",
    "# visualization\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from numpy import interp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing on the given text.\n",
    "\n",
    "    Steps :\n",
    "    - Removing HTML tags\n",
    "    - Removing punctuation\n",
    "    - Lowering text\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)    \n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)    \n",
    "    \n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    # replace punctuation characters with spaces\n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_info = pd.read_csv(Path(data_path, 'post_user_risk_tb.csv'))\n",
    "# user_info = user_info.set_index('user_id')\n",
    "# user_info.drop(['post_id', 'subreddits_x', 'subreddits_y', 'timestamp'], axis=1, inplace=True)\n",
    "# title_body = user_info['post_title'].astype(str) + user_info['post_body'].astype(str)\n",
    "# n_title_body = [x.replace('nan', '') for x in title_body]\n",
    "# user_info['title_body'] = n_title_body\n",
    "# user_info.drop(['post_title', 'post_body'], axis=1, inplace=True)\n",
    "# user_info.to_csv(Path(data_path, 'user_id_risk_label_title_body.csv'))\n",
    "# grouping all the title and body\n",
    "# data = data.groupby('user_id').agg({'title_body': ' '.join, \n",
    "#                                               'risk_label':'first' }).reset_index()\n",
    "# # removign control\n",
    "# print(data.shape)\n",
    "# data = data[~pd.isnull(data['risk_label'])]\n",
    "# print(data.shape)\n",
    "# data = data.set_index('user_id')\n",
    "# data.columns = ['title_body', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_report(y_true, y_pred, y_score=None, average='macro'):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        print(\"Error! y_true {0} is not the same shape as y_pred {1}\".format(\n",
    "              y_true.shape,\n",
    "              y_pred.shape)\n",
    "        )\n",
    "        return\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "\n",
    "    if len(y_true.shape) == 1:\n",
    "        lb.fit(y_true)\n",
    "\n",
    "    #Value counts of predictions\n",
    "    labels, cnt = np.unique(\n",
    "        y_pred,\n",
    "        return_counts=True)\n",
    "    n_classes = len(labels)\n",
    "    pred_cnt = pd.Series(cnt, index=labels)\n",
    "\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            labels=labels)\n",
    "\n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average=average))\n",
    "\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list(metrics_summary),\n",
    "        index=metrics_sum_index,\n",
    "        columns=labels)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    class_report_df['avg / total'] = avg[:-1] + [total]\n",
    "\n",
    "    class_report_df = class_report_df.T\n",
    "    class_report_df['pred-cnt'] = pred_cnt\n",
    "    class_report_df['pred-cnt'].iloc[-1] = total\n",
    "\n",
    "    if not (y_score is None):\n",
    "        # false positive rate\n",
    "        fpr = dict()\n",
    "        # true positive rate\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for label_ix, label in enumerate(labels):\n",
    "            fpr[label], tpr[label], _ = roc_curve(\n",
    "                (y_true == label).astype(int), \n",
    "                y_score[:, label_ix])\n",
    "\n",
    "            roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        if average == 'micro':\n",
    "            if n_classes <= 2:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                    lb.transform(y_true).ravel(), \n",
    "                    y_score[:, 1].ravel())\n",
    "            else:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                        lb.transform(y_true).ravel(), \n",
    "                        y_score.ravel())\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(\n",
    "                fpr[\"avg / total\"], \n",
    "                tpr[\"avg / total\"])\n",
    "\n",
    "        elif average == 'macro':\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([\n",
    "                fpr[i] for i in labels]\n",
    "            ))\n",
    "\n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in labels:\n",
    "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "            # Finally average it and compute AUC\n",
    "            mean_tpr /= n_classes\n",
    "\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        class_report_df['AUC'] = pd.Series(roc_auc)\n",
    "\n",
    "    return class_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(Path(data_path, 'user_id_risk_label_title_body.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 5-fold cross validation test harness\n",
    "k_fold = pd.read_csv(Path(data_path, 'clpsych19_public_crossvalidation_splits.csv'), header=None,\n",
    "                    names=['fold', 'train_text', 'user_id'])\n",
    "\n",
    "# keep non conrol user ids\n",
    "k_fold = k_fold[k_fold['user_id'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.copy()\n",
    "ave_f1_scores = list()\n",
    "\n",
    "print('5 fold CV starting')\n",
    "for fold_ix in range(1,6):\n",
    "    \n",
    "    print('fold = {0}'.format(fold_ix))\n",
    "    \n",
    "    train_ix = k_fold[(k_fold['fold'] == fold_ix) & (k_fold['train_text'] == 'training')]['user_id']\n",
    "    test_ix = k_fold[(k_fold['fold'] == fold_ix) & (k_fold['train_text'] == 'test')]['user_id']\n",
    "    \n",
    "    x_train = data[data.user_id.isin(train_ix)].copy()\n",
    "    x_test = data[data.user_id.isin(test_ix)].copy()\n",
    "    \n",
    "    y_train = x_train['target']\n",
    "    x_train.drop(['target'], axis=1, inplace=True)\n",
    "    \n",
    "    y_test = x_test['target']\n",
    "    x_test.drop(['target'], axis=1, inplace=True)\n",
    "    \n",
    "#     vectorizer = CountVectorizer(stop_words=\"english\",\n",
    "#                          preprocessor=clean_text)\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
    "                             preprocessor=clean_text,\n",
    "                             ngram_range=(1, 2))\n",
    "    \n",
    "    x_train = vectorizer.fit_transform(x_train['title_body'])    \n",
    "    x_test = vectorizer.transform(x_test['title_body'])\n",
    "    \n",
    "    # Training\n",
    "    svm = LinearSVC()\n",
    "    model = CalibratedClassifierCV(svm, cv=None) \n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    y_pred_test = model.predict(x_test)\n",
    "    y_score=model.predict_proba(x_test)\n",
    "    \n",
    "    report_with_auc = class_report( y_true=y_test, y_pred=y_pred_test, y_score=y_score, average='macro')\n",
    "        \n",
    "    ave_f1_scores.append(report_with_auc['f1-score'].values[-1])\n",
    "\n",
    "    cv_column = [fold_ix]\n",
    "    cv_column.extend( [''] * (report_with_auc.index.shape[0] - 1))\n",
    "    report_with_auc['Fold'] = cv_column\n",
    "    report_with_auc['Risk-Factor'] = report_with_auc.index\n",
    "    report_with_auc = report_with_auc.set_index(['Fold', 'Risk-Factor'])\n",
    "\n",
    "    if fold_ix == 1:\n",
    "        report_with_auc_df = report_with_auc.copy()\n",
    "    else:\n",
    "        report_with_auc_df = report_with_auc_df.append(report_with_auc.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_with_auc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('average f1-score (all folds) = {0}'.format(np.mean(ave_f1_scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning_py372",
   "language": "python",
   "name": "deeplearning_py372"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
