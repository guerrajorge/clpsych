{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings for sentiment analysis Suicide Sentiment Analysis\n",
    "\n",
    "StanfordCore NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'clpsych'\n",
    "project_path = Path(os.getcwd()).parent\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    data_path = 'D:\\Dataset\\{0}\\dataset'.format(project_name)\n",
    "    model_path = Path(project_path, 'models')\n",
    "    src_path = src_path = Path(project_path, 'src')\n",
    "    \n",
    "elif sys.platform == 'darwin':\n",
    "    data_path = '/Volumes/Dataset/{0}/dataset'.format(project_name)\n",
    "    model_path = '/Volumes/Dataset/{0}/models'.format(project_name)\n",
    "    src_path = '/Volumes/Dataset/{0}/src'.format(project_name)\n",
    "    \n",
    "else:\n",
    "    data_path = Path(project_path, 'dataset')\n",
    "    model_path = Path(project_path, 'models')\n",
    "    src_path = Path(project_path, 'src')\n",
    "\n",
    "utils_path = str(Path(project_path, 'utils'))\n",
    "# including the project folder and the utils folder\n",
    "if utils_path not in ''.join(sys.path):\n",
    "    sys.path.extend([str(project_path), str(utils_path), str(src_path)])\n",
    "\n",
    "print('project path = {0}'.format(project_path))\n",
    "print('data path = {0}'.format(data_path))\n",
    "print('model path = {0}'.format(model_path))\n",
    "print('utils path = {0}'.format(utils_path))\n",
    "print('sys.path = {0}'.format(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# NLP\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "# wei\n",
    "from data_helpers import build_vocab\n",
    "from data_helpers import load_data_and_labels\n",
    "from data_helpers import pad_sentences\n",
    "\n",
    "from statistics import mean\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_word(word):\n",
    "    # Remove punctuation\n",
    "    word = word.strip('\\'\"?!,.():;')\n",
    "    # Convert more than 2 letter repetitions to 2 letter\n",
    "    # funnnnny --> funny\n",
    "    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "    # Remove - & '\n",
    "    word = re.sub(r'(-|\\')', '', word)\n",
    "    return word\n",
    "\n",
    "\n",
    "def is_valid_word(word):\n",
    "    # Check if word begins with an alphabet\n",
    "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)\n",
    "\n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    processed_tweet = []\n",
    "    # Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    # Replaces URLs with the word URL\n",
    "    tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' URL ', tweet)\n",
    "    # Replace @handle with the word USER_MENTION\n",
    "    tweet = re.sub(r'@[\\S]+', 'USER_MENTION', tweet)\n",
    "    # Replaces #hashtag with hashtag\n",
    "    tweet = re.sub(r'#(\\S+)', r' \\1 ', tweet)\n",
    "    # Remove RT (retweet)\n",
    "    tweet = re.sub(r'\\brt\\b', '', tweet)\n",
    "    # Replace 2+ dots with space\n",
    "    tweet = re.sub(r'\\.{2,}', ' ', tweet)\n",
    "    # remove nans\n",
    "    tweet = tweet.replace('nan', '')\n",
    "    # remove @ mentions\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "    # Strip space, \" and ' from tweet\n",
    "    tweet = tweet.strip(' \"\\'')\n",
    "    # Replace emojis with either EMO_POS or EMO_NEG\n",
    "    tweet = handle_emojis(tweet)\n",
    "    # Replace multiple spaces with a single space\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    words = tweet.split()\n",
    "\n",
    "    for word in words:\n",
    "        word = preprocess_word(word)\n",
    "        if is_valid_word(word):\n",
    "            processed_tweet.append(word)\n",
    "\n",
    "    return ' '.join(processed_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suicide_data = pd.read_csv(Path(data_path, 'risk_title_body.csv'))\n",
    "suicide_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suicide_data.columns = ['index', 'risk_label', 'title_body']\n",
    "suicide_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suicide_data['text'] = suicide_data.title_body.apply(preprocess_tweet)\n",
    "# suicide_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "res = nlp.annotate(\"I love you. I hate him. You are nice. He is dumb\",\n",
    "                   properties={\n",
    "                       'annotators': 'sentiment',\n",
    "                       'outputFormat': 'json'\n",
    "                   })\n",
    "for s in res[\"sentences\"]:\n",
    "    print(\"{0}: {1}: {2} {3}\".format( s[\"index\"], \" \".join([t[\"word\"] for t in s[\"tokens\"]]), s[\"sentimentValue\"], s[\"sentiment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(Path(data_path, 'Tweets.csv'))\n",
    "tweets['prop'] = tweets['text'].apply(remove_mentions)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "sentiment_list = list()\n",
    "averaged_sentiment = pd.DataFrame(columns=['text', 'sentiments', 'sentiment_dist','y'])\n",
    "\n",
    "# islice()\tseq, [start,] stop [, step]\n",
    "for _, tweet in islice(tweets.iterrows(), 0, 12):\n",
    "    res = nlp.annotate(tweet['prop'], properties={ 'annotators': 'sentiment', 'outputFormat': 'json'})\n",
    "    sentimement = list()\n",
    "    sentiment_dist = list()\n",
    "    for s in res[\"sentences\"]:\n",
    "        print(\"{0}: {1}: {2} {3}\".format( s[\"index\"], \" \".join([t[\"word\"] for t in s[\"tokens\"]]), s[\"sentimentValue\"], s[\"sentiment\"]))\n",
    "        sentimement.append(s[\"sentiment\"])\n",
    "        sentiment_dist.append(s['sentimentDistribution'])\n",
    "    averaged_sentiment = averaged_sentiment.append({'text':tweet['prop'], 'sentiments':sentimement, 'sent dist' : sentiment_dist,'y' : tweet['airline_sentiment']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "sentiment_list = list()\n",
    "sentiment_df = pd.DataFrame(columns=['text', 'sentiments', 'sentiment_dist','y'])\n",
    "\n",
    "total_len = suicide_data.shape[0]\n",
    "\n",
    "for ix, post in suicide_data.iterrows():\n",
    "    print('processing {0}/{1}'.format(ix, total_len))\n",
    "    res = nlp.annotate(post['title_body'], properties={ 'timeout': 600000, 'annotators': 'sentiment', 'outputFormat': 'json'})\n",
    "    sentimement = list()\n",
    "    sentiment_dist = list()\n",
    "    for s in res[\"sentences\"]:\n",
    "        print(\"{0}: {1}: {2} {3}\".format( s[\"index\"], \" \".join([t[\"word\"] for t in s[\"tokens\"]]), s[\"sentimentValue\"], s[\"sentiment\"]))\n",
    "        sentimement.append(s[\"sentiment\"])\n",
    "        sentiment_dist.append(s['sentimentDistribution'])\n",
    "    sentiment_df = sentiment_df.append({'text':post['title_body'], 'sentiments':sentimement, 'sent dist' : sentiment_dist,\n",
    "                                        'y' : post['risk_label']}, ignore_index=True)\n",
    "    if ix == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suicide_data[0:11403].to_csv(Path(data_path, 'risk_title_body_1.csv'), index=False)\n",
    "suicide_data[11403:22806].to_csv(Path(data_path, 'risk_title_body_2.csv'), index=False)\n",
    "suicide_data[22806:34209].to_csv(Path(data_path, 'risk_title_body_3.csv'), index=False)\n",
    "suicide_data[34209:45612].to_csv(Path(data_path, 'risk_title_body_4.csv'), index=False)\n",
    "suicide_data[45612:].to_csv(Path(data_path, 'risk_title_body_5.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
