{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using imbalanced-learn samplers API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project path = /home/guerramarj/github/clpsych\n",
      "data path = /home/guerramarj/github/clpsych/dataset\n",
      "model path = /home/guerramarj/github/clpsych/models\n",
      "embedding path = /home/guerramarj/github/clpsych/embedding\n",
      "sys.path = ['/cm/local/apps/cuda/libs/current/pynvml', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python36.zip', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/lib-dynload', '', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/Mako-1.0.7-py3.6.egg', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/IPython/extensions', '/home/guerramarj/.ipython', '/home/guerramarj/github/clpsych', '/home/guerramarj/github/clpsych/utils', '/home/guerramarj/github/clpsych/src']\n"
     ]
    }
   ],
   "source": [
    "project_name = 'clpsych'\n",
    "project_path = Path(os.getcwd()).parent\n",
    "src_path = Path(project_path, 'src')\n",
    "utils_path = Path(project_path, 'utils')\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    data_path = 'D:\\Dataset\\{0}\\dataset'.format(project_name)\n",
    "    embedding_path = 'D:\\Dataset\\{0}\\embedding'.format(project_name)\n",
    "    model_path = 'D:\\Dataset\\{0}\\embedding'.format(project_name)\n",
    "    \n",
    "elif sys.platform == 'darwin':\n",
    "    data_path = '/Volumes/Dataset/{0}/dataset'.format(project_name)\n",
    "    embedding_path = '/Volumes/Dataset/{0}/embedding'.format(project_name)\n",
    "    \n",
    "else:\n",
    "    data_path = Path(project_path, 'dataset')\n",
    "    model_path = Path(project_path, 'models')\n",
    "    embedding_path = Path(project_path, 'embedding')\n",
    "\n",
    "# including the project folder and the utils folder\n",
    "if str(utils_path) not in ''.join(sys.path):\n",
    "    sys.path.extend([str(project_path), str(utils_path), str(src_path)])\n",
    "\n",
    "print('project path = {0}'.format(project_path))\n",
    "print('data path = {0}'.format(data_path))\n",
    "print('model path = {0}'.format(model_path))\n",
    "print('embedding path = {0}'.format(embedding_path))\n",
    "print('sys.path = {0}'.format(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import interp \n",
    "from collections import Counter\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# optimizer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from keras.utils import multi_gpu_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# ml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# deeplearning\n",
    "import tensorflow as tf\n",
    "from keras.layers import Reshape, Flatten, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout\n",
    "from keras import metrics\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam, Adam\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "from keras.layers import Input\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 13]\n",
    "\n",
    "# seed for numpy and sklearn\n",
    "random_state = 7\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow recognizes GPUs\n",
      "number of available GPUs = 4\n",
      "list of GPUs = ['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "print('Tensorflow recognizes GPUs')\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "available_gpu = backend.tensorflow_backend._get_available_gpus()\n",
    "assert len(available_gpu) > 0\n",
    "available_gpus = len(available_gpu)\n",
    "print('number of available GPUs = {0}'.format(available_gpus))\n",
    "print('list of GPUs = {0}\\n'.format(available_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_report(y_true, y_pred, y_score=None, average='macro'):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        print(\"Error! y_true {0} is not the same shape as y_pred {1}\".format(\n",
    "              y_true.shape,\n",
    "              y_pred.shape)\n",
    "        )\n",
    "        return\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "\n",
    "    if len(y_true.shape) == 1:\n",
    "        lb.fit(y_true)\n",
    "\n",
    "    #Value counts of predictions\n",
    "    labels, cnt = np.unique(\n",
    "        y_pred,\n",
    "        return_counts=True)\n",
    "    n_classes = len(labels)\n",
    "    pred_cnt = pd.Series(cnt, index=labels)\n",
    "\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            labels=labels)\n",
    "\n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average=average))\n",
    "\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list(metrics_summary),\n",
    "        index=metrics_sum_index,\n",
    "        columns=labels)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    class_report_df['avg / total'] = avg[:-1] + [total]\n",
    "\n",
    "    class_report_df = class_report_df.T\n",
    "    class_report_df['pred-cnt'] = pred_cnt\n",
    "    class_report_df['pred-cnt'].iloc[-1] = total\n",
    "\n",
    "    if not (y_score is None):\n",
    "        # false positive rate\n",
    "        fpr = dict()\n",
    "        # true positive rate\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for label_ix, label in enumerate(labels):\n",
    "            fpr[label], tpr[label], _ = roc_curve(\n",
    "                (y_true == label).astype(int), \n",
    "                y_score[:, label_ix])\n",
    "\n",
    "            roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        if average == 'micro':\n",
    "            if n_classes <= 2:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                    lb.transform(y_true).ravel(), \n",
    "                    y_score[:, 1].ravel())\n",
    "            else:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                        lb.transform(y_true).ravel(), \n",
    "                        y_score.ravel())\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(\n",
    "                fpr[\"avg / total\"], \n",
    "                tpr[\"avg / total\"])\n",
    "\n",
    "        elif average == 'macro':\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([\n",
    "                fpr[i] for i in labels]\n",
    "            ))\n",
    "\n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in labels:\n",
    "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "            # Finally average it and compute AUC\n",
    "            mean_tpr /= n_classes\n",
    "\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        class_report_df['AUC'] = pd.Series(roc_auc)\n",
    "\n",
    "    return class_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path(data_path, 'static_features_pandas_v2.pkl')\n",
    "filename = Path(data_path, 'static_features_pandas_aug_v2.2.pkl')\n",
    "dataset = pd.read_pickle(filename)\n",
    "filename = Path(data_path, 'extracted_features_sentiment_per_user_filtered.csv')\n",
    "sentiment_statics = pd.read_csv(filename)\n",
    "sentiment_statics.set_index('user_id', inplace=True)\n",
    "# sentiment_statics.drop('risk_label', axis=1, inplace=True)\n",
    "n_columns = ['SETLLL__{0}'.format(x) for x in sentiment_statics.columns]\n",
    "sentiment_statics.columns = n_columns\n",
    "print(dataset.shape)\n",
    "dataset = dataset.merge(right=sentiment_statics, left_index=True, right_index=True)\n",
    "print(dataset.shape)\n",
    "dataset.to_pickle(Path(data_path, 'static_features_pandas_aug_v2.2.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(496, 11614)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SETAAA__Max_weekly_MH_posts_last2years</th>\n",
       "      <th>SETAAA__Max_weekly_suicide_posts_last2years</th>\n",
       "      <th>SETAAA__N_posts_alltime</th>\n",
       "      <th>SETAAA__N_posts_last2years</th>\n",
       "      <th>SETAAA__N_weeks_with_MH_posts_last2years</th>\n",
       "      <th>SETAAA__N_weeks_with_suicide_posts_last2years</th>\n",
       "      <th>SETAAA__active_days</th>\n",
       "      <th>SETAAA__avg_distance_to_2am_last2years</th>\n",
       "      <th>SETAAA__avg_post_length_last2years</th>\n",
       "      <th>SETAAA__avg_proportion_adjs_last2years</th>\n",
       "      <th>...</th>\n",
       "      <th>SETLLL__sent_4__sample_entropy</th>\n",
       "      <th>SETLLL__sent_4__skewness</th>\n",
       "      <th>SETLLL__sent_5__abs_energy</th>\n",
       "      <th>SETLLL__sent_5__absolute_sum_of_changes</th>\n",
       "      <th>SETLLL__sent_5__linear_trend__attr_\"slope\"</th>\n",
       "      <th>SETLLL__sent_5__maximum</th>\n",
       "      <th>SETLLL__sent_5__mean</th>\n",
       "      <th>SETLLL__sent_5__minimum</th>\n",
       "      <th>SETLLL__sent_5__sample_entropy</th>\n",
       "      <th>SETLLL__sent_5__skewness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>509.961400</td>\n",
       "      <td>11.543417</td>\n",
       "      <td>505.450000</td>\n",
       "      <td>0.148389</td>\n",
       "      <td>...</td>\n",
       "      <td>1.879728</td>\n",
       "      <td>1.285680</td>\n",
       "      <td>0.031081</td>\n",
       "      <td>0.417038</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.066699</td>\n",
       "      <td>0.035918</td>\n",
       "      <td>0.015252</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.538883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>905.823194</td>\n",
       "      <td>6.847304</td>\n",
       "      <td>221.338028</td>\n",
       "      <td>0.160213</td>\n",
       "      <td>...</td>\n",
       "      <td>1.905546</td>\n",
       "      <td>1.897587</td>\n",
       "      <td>0.507650</td>\n",
       "      <td>3.600498</td>\n",
       "      <td>-0.000258</td>\n",
       "      <td>0.586467</td>\n",
       "      <td>0.033911</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.926642</td>\n",
       "      <td>8.118547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>443.223900</td>\n",
       "      <td>13.458903</td>\n",
       "      <td>203.896552</td>\n",
       "      <td>0.184151</td>\n",
       "      <td>...</td>\n",
       "      <td>1.857388</td>\n",
       "      <td>1.515073</td>\n",
       "      <td>0.653651</td>\n",
       "      <td>4.154778</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.536643</td>\n",
       "      <td>0.058049</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>1.089579</td>\n",
       "      <td>3.630981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>458.475648</td>\n",
       "      <td>11.730547</td>\n",
       "      <td>66.303030</td>\n",
       "      <td>0.121091</td>\n",
       "      <td>...</td>\n",
       "      <td>2.141962</td>\n",
       "      <td>1.255933</td>\n",
       "      <td>0.029821</td>\n",
       "      <td>0.575932</td>\n",
       "      <td>-0.000447</td>\n",
       "      <td>0.106684</td>\n",
       "      <td>0.022424</td>\n",
       "      <td>0.002792</td>\n",
       "      <td>1.838279</td>\n",
       "      <td>2.396103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>582.131840</td>\n",
       "      <td>12.403656</td>\n",
       "      <td>518.253333</td>\n",
       "      <td>0.165292</td>\n",
       "      <td>...</td>\n",
       "      <td>1.929469</td>\n",
       "      <td>1.128026</td>\n",
       "      <td>0.172404</td>\n",
       "      <td>2.475222</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.176317</td>\n",
       "      <td>0.037454</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>1.883401</td>\n",
       "      <td>1.869605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11614 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SETAAA__Max_weekly_MH_posts_last2years  \\\n",
       "user_id                                           \n",
       "16                                            0   \n",
       "56                                            0   \n",
       "137                                           0   \n",
       "152                                           0   \n",
       "370                                           1   \n",
       "\n",
       "         SETAAA__Max_weekly_suicide_posts_last2years  SETAAA__N_posts_alltime  \\\n",
       "user_id                                                                         \n",
       "16                                                 1                       20   \n",
       "56                                                 0                      115   \n",
       "137                                                1                       58   \n",
       "152                                                1                       33   \n",
       "370                                                1                       75   \n",
       "\n",
       "         SETAAA__N_posts_last2years  SETAAA__N_weeks_with_MH_posts_last2years  \\\n",
       "user_id                                                                         \n",
       "16                               20                                         0   \n",
       "56                               71                                         0   \n",
       "137                              58                                         0   \n",
       "152                              33                                         0   \n",
       "370                              75                                         5   \n",
       "\n",
       "         SETAAA__N_weeks_with_suicide_posts_last2years  SETAAA__active_days  \\\n",
       "user_id                                                                       \n",
       "16                                                   2           509.961400   \n",
       "56                                                   0           905.823194   \n",
       "137                                                  1           443.223900   \n",
       "152                                                  2           458.475648   \n",
       "370                                                  1           582.131840   \n",
       "\n",
       "         SETAAA__avg_distance_to_2am_last2years  \\\n",
       "user_id                                           \n",
       "16                                    11.543417   \n",
       "56                                     6.847304   \n",
       "137                                   13.458903   \n",
       "152                                   11.730547   \n",
       "370                                   12.403656   \n",
       "\n",
       "         SETAAA__avg_post_length_last2years  \\\n",
       "user_id                                       \n",
       "16                               505.450000   \n",
       "56                               221.338028   \n",
       "137                              203.896552   \n",
       "152                               66.303030   \n",
       "370                              518.253333   \n",
       "\n",
       "         SETAAA__avg_proportion_adjs_last2years            ...             \\\n",
       "user_id                                                    ...              \n",
       "16                                     0.148389            ...              \n",
       "56                                     0.160213            ...              \n",
       "137                                    0.184151            ...              \n",
       "152                                    0.121091            ...              \n",
       "370                                    0.165292            ...              \n",
       "\n",
       "         SETLLL__sent_4__sample_entropy  SETLLL__sent_4__skewness  \\\n",
       "user_id                                                             \n",
       "16                             1.879728                  1.285680   \n",
       "56                             1.905546                  1.897587   \n",
       "137                            1.857388                  1.515073   \n",
       "152                            2.141962                  1.255933   \n",
       "370                            1.929469                  1.128026   \n",
       "\n",
       "         SETLLL__sent_5__abs_energy  SETLLL__sent_5__absolute_sum_of_changes  \\\n",
       "user_id                                                                        \n",
       "16                         0.031081                                 0.417038   \n",
       "56                         0.507650                                 3.600498   \n",
       "137                        0.653651                                 4.154778   \n",
       "152                        0.029821                                 0.575932   \n",
       "370                        0.172404                                 2.475222   \n",
       "\n",
       "         SETLLL__sent_5__linear_trend__attr_\"slope\"  SETLLL__sent_5__maximum  \\\n",
       "user_id                                                                        \n",
       "16                                         0.000112                 0.066699   \n",
       "56                                        -0.000258                 0.586467   \n",
       "137                                        0.000533                 0.536643   \n",
       "152                                       -0.000447                 0.106684   \n",
       "370                                        0.000223                 0.176317   \n",
       "\n",
       "         SETLLL__sent_5__mean  SETLLL__sent_5__minimum  \\\n",
       "user_id                                                  \n",
       "16                   0.035918                 0.015252   \n",
       "56                   0.033911                 0.002740   \n",
       "137                  0.058049                 0.001771   \n",
       "152                  0.022424                 0.002792   \n",
       "370                  0.037454                 0.000622   \n",
       "\n",
       "         SETLLL__sent_5__sample_entropy  SETLLL__sent_5__skewness  \n",
       "user_id                                                            \n",
       "16                             2.302585                  0.538883  \n",
       "56                             0.926642                  8.118547  \n",
       "137                            1.089579                  3.630981  \n",
       "152                            1.838279                  2.396103  \n",
       "370                            1.883401                  1.869605  \n",
       "\n",
       "[5 rows x 11614 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'static_features_pandas_v2.pkl'\n",
    "filename = 'static_features_pandas_aug_v2.pkl'\n",
    "filename = 'static_features_pandas_aug_v2.2.pkl'\n",
    "dataset = pd.read_pickle(Path(data_path, filename))\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 5-fold cross validation test harness\n",
    "k_fold = pd.read_csv(Path(data_path, 'clpsych19_public_crossvalidation_splits.csv'), header=None,\n",
    "                    names=['fold', 'train_text', 'user_id'])\n",
    "\n",
    "# keep non conrol user ids\n",
    "k_fold = k_fold[k_fold['user_id'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Feature Set   Description  \n",
    "A             static_derieved_features  \n",
    "B             post_coount_by_subreddit\n",
    "C             lexicon_count  \n",
    "D             sentiments_macro  \n",
    "E             sentiments_micro  \n",
    "F             empathy\n",
    "G             readability\n",
    "H             social_context\n",
    "I             srl\n",
    "J             ctakes\n",
    "K             sentiment statics\n",
    "L             sentiment statics filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 11614)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset.copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features_interest = ['SETAAA'] \n",
    "# features_interest = ['SETBBB'] # contains NaN\n",
    "# features_interest = ['SETCCC'] # contains NaN\n",
    "# features_interest = ['SETDDD']\n",
    "# features_interest = ['SETEEE']\n",
    "# features_interest = ['SETFFF'] # contains NaN\n",
    "# features_interest = ['SETGGG'] # contains NaN\n",
    "# features_interest = ['SETHHH'] # contains NaN\n",
    "# features_interest = ['SETIII'] # contains NaN\n",
    "# features_interest = ['SETJJJ'] # contains NaN\n",
    "# features_interest = ['SETKKK'] # contains NaN\n",
    "# features_interest = ['SETLLL'] # contains NaN\n",
    "# features_interest = ['SETAAA', 'SETDDD', 'SETEEE', 'SETFFF', 'SETGGG']\n",
    "features_interest = ['SETAAA', 'SETDDD', 'SETEEE']\n",
    "\n",
    "features = [x for x in dataset.columns if x.split('__')[0] in features_interest]\n",
    "# including label\n",
    "features.append('target')\n",
    "data = dataset[features].copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 fold CV starting\n",
      "\n",
      "Fold = 1\n",
      "(395, 11613)\n",
      "[(1, 101), (2, 40), (3, 90), (4, 164)]\n",
      "[(1, 164), (2, 164), (3, 164), (4, 164)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 4ms/step - loss: 0.6134 - f1: 0.3457\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 126us/step - loss: 0.6639 - f1: 0.3361\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 0.6600 - f1: 0.3400\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 124us/step - loss: 0.6606 - f1: 0.3394\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6601 - f1: 0.3399\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6541 - f1: 0.3459\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 124us/step - loss: 0.6435 - f1: 0.3565\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6311 - f1: 0.3689\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.5646 - f1: 0.4354\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.5582 - f1: 0.4417\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6696 - f1: 0.3304\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 124us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 0.6691 - f1: 0.3309\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 125us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 125us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6687 - f1: 0.3313\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 125us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 126us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 126us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 128us/step - loss: 0.6693 - f1: 0.3307\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 125us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 127us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 129us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 126us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6687 - f1: 0.3313\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 128us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 124us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 129us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 67/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 132us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 125us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 133us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 134us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 127us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 124us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328/328 [==============================] - 0s 124us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 118us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6685 - f1: 0.3315\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 118us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 124us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 126us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 1/100\n",
      "328/328 [==============================] - 1s 4ms/step - loss: 0.6183 - f1: 0.3773\n",
      "Epoch 2/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 3/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 4/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 5/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 6/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 7/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 8/100\n",
      "328/328 [==============================] - 0s 124us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 9/100\n",
      "328/328 [==============================] - 0s 127us/step - loss: 0.6684 - f1: 0.3316\n",
      "Epoch 10/100\n",
      "328/328 [==============================] - 0s 126us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 11/100\n",
      "328/328 [==============================] - 0s 127us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 12/100\n",
      "328/328 [==============================] - 0s 124us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 13/100\n",
      "328/328 [==============================] - 0s 124us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 14/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 15/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 16/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 17/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 18/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 19/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 20/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 21/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 22/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 23/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 24/100\n",
      "328/328 [==============================] - 0s 125us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 25/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 26/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 27/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 28/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 29/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 30/100\n",
      "328/328 [==============================] - 0s 126us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 31/100\n",
      "328/328 [==============================] - 0s 124us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 32/100\n",
      "328/328 [==============================] - 0s 126us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 33/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 34/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 35/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 36/100\n",
      "328/328 [==============================] - 0s 124us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 37/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 38/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6683 - f1: 0.3317\n",
      "Epoch 39/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 40/100\n",
      "328/328 [==============================] - 0s 124us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 41/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 42/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 43/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 44/100\n",
      "328/328 [==============================] - 0s 124us/step - loss: 0.6686 - f1: 0.3314\n",
      "Epoch 45/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 46/100\n",
      "328/328 [==============================] - 0s 124us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 47/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 48/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6693 - f1: 0.3307\n",
      "Epoch 49/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 50/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 51/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 52/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 53/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 54/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 55/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 56/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 57/100\n",
      "328/328 [==============================] - 0s 118us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 58/100\n",
      "328/328 [==============================] - 0s 118us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 59/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 60/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 61/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 62/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6688 - f1: 0.3312\n",
      "Epoch 63/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 64/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 65/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 66/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328/328 [==============================] - 0s 125us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 68/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 69/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 70/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 71/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 72/100\n",
      "328/328 [==============================] - 0s 127us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 73/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 74/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 75/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 76/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 77/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 78/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 79/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 80/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 81/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 82/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 83/100\n",
      "328/328 [==============================] - 0s 119us/step - loss: 0.6684 - f1: 0.3316\n",
      "Epoch 84/100\n",
      "328/328 [==============================] - 0s 129us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 85/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 86/100\n",
      "328/328 [==============================] - 0s 118us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 87/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6683 - f1: 0.3317\n",
      "Epoch 88/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 89/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 90/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 91/100\n",
      "328/328 [==============================] - 0s 122us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 92/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 93/100\n",
      "328/328 [==============================] - 0s 128us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 94/100\n",
      "328/328 [==============================] - 0s 125us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 95/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 96/100\n",
      "328/328 [==============================] - 0s 130us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 97/100\n",
      "328/328 [==============================] - 0s 121us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 98/100\n",
      "328/328 [==============================] - 0s 132us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 99/100\n",
      "328/328 [==============================] - 0s 120us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 100/100\n",
      "328/328 [==============================] - 0s 123us/step - loss: 0.6670 - f1: 0.3330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold = 2\n",
      "(396, 11613)\n",
      "[(1, 101), (2, 40), (3, 90), (4, 165)]\n",
      "[(1, 165), (2, 165), (3, 165), (4, 165)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "330/330 [==============================] - 1s 4ms/step - loss: 0.6234 - f1: 0.3439\n",
      "Epoch 2/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 3/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 4/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 5/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 6/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 7/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6700 - f1: 0.3300\n",
      "Epoch 8/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 9/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 10/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 11/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 12/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 13/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 14/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 15/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6684 - f1: 0.3316\n",
      "Epoch 16/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 17/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 18/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 19/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 20/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 21/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6687 - f1: 0.3313\n",
      "Epoch 22/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 23/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6683 - f1: 0.3317\n",
      "Epoch 24/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 25/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 26/100\n",
      "330/330 [==============================] - 0s 130us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 27/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 28/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 29/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 30/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 31/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 32/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 33/100\n",
      "330/330 [==============================] - 0s 131us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 34/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 35/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 36/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 37/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 38/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 39/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 40/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 41/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 42/100\n",
      "330/330 [==============================] - 0s 118us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 43/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 44/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 45/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 46/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 47/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 48/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 49/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6684 - f1: 0.3316\n",
      "Epoch 50/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 51/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6684 - f1: 0.3316\n",
      "Epoch 52/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 53/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 54/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 55/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 56/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 57/100\n",
      "330/330 [==============================] - 0s 118us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 58/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 59/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 60/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 61/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 62/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 63/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 64/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 65/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 66/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 67/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 68/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 69/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 70/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 71/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 72/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 73/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 74/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 75/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 76/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 77/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 78/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 79/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 80/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 81/100\n",
      "330/330 [==============================] - 0s 129us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 82/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 83/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 0s 121us/step - loss: 0.6689 - f1: 0.3311\n",
      "Epoch 85/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 86/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 87/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 88/100\n",
      "330/330 [==============================] - 0s 135us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 89/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 90/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 91/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 92/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 93/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 94/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 95/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6685 - f1: 0.3315\n",
      "Epoch 96/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 97/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 98/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 99/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 100/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6684 - f1: 0.3316\n",
      "Epoch 1/100\n",
      "330/330 [==============================] - 1s 4ms/step - loss: 0.6380 - f1: 0.3311\n",
      "Epoch 2/100\n",
      "330/330 [==============================] - 0s 143us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 3/100\n",
      "330/330 [==============================] - 0s 142us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 4/100\n",
      "330/330 [==============================] - 0s 138us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 5/100\n",
      "330/330 [==============================] - 0s 131us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 6/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 7/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 8/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 9/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 10/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 11/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 12/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 13/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 14/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 15/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6686 - f1: 0.3314\n",
      "Epoch 16/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 17/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 18/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 19/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 20/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 21/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 22/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 23/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 24/100\n",
      "330/330 [==============================] - 0s 129us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 25/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 26/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 27/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 28/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 29/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 30/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 31/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 32/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 33/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 34/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 35/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 36/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 37/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 38/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 39/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 40/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 41/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 42/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 43/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 44/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 45/100\n",
      "330/330 [==============================] - 0s 129us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 46/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 47/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 48/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 49/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 50/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 51/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 52/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 53/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 54/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 55/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 56/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 57/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 58/100\n",
      "330/330 [==============================] - 0s 130us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 59/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 60/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 61/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 62/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 63/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 64/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 65/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 66/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 0s 124us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 68/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 69/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 70/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 71/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 72/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 73/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 74/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 75/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 76/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 77/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 78/100\n",
      "330/330 [==============================] - 0s 138us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 79/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6684 - f1: 0.3316\n",
      "Epoch 80/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 81/100\n",
      "330/330 [==============================] - 0s 129us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 82/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 83/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 84/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 85/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 86/100\n",
      "330/330 [==============================] - 0s 130us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 87/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 88/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 89/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 90/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 91/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 92/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 93/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 94/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 95/100\n",
      "330/330 [==============================] - 0s 129us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 96/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 97/100\n",
      "330/330 [==============================] - 0s 133us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 98/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 99/100\n",
      "330/330 [==============================] - 0s 130us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 100/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6683 - f1: 0.3317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold = 3\n",
      "(397, 11613)\n",
      "[(1, 102), (2, 40), (3, 90), (4, 165)]\n",
      "[(1, 165), (2, 165), (3, 165), (4, 165)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "330/330 [==============================] - 2s 5ms/step - loss: 0.6126 - f1: 0.3397\n",
      "Epoch 2/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6683 - f1: 0.3317\n",
      "Epoch 3/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 4/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 5/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 6/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6699 - f1: 0.3301\n",
      "Epoch 7/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6685 - f1: 0.3315\n",
      "Epoch 8/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 9/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 10/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 11/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 12/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 13/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 14/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 15/100\n",
      "330/330 [==============================] - 0s 131us/step - loss: 0.6694 - f1: 0.3306\n",
      "Epoch 16/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 17/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 18/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 19/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 20/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 21/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 22/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 23/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 24/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 25/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 26/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 27/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 28/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 29/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 30/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 31/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 32/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 33/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 34/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 35/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 36/100\n",
      "330/330 [==============================] - 0s 130us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 37/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 38/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 39/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 40/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 41/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 42/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 43/100\n",
      "330/330 [==============================] - 0s 131us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 44/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 45/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 46/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 47/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 48/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 49/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 50/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 51/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 52/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 53/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6684 - f1: 0.3316\n",
      "Epoch 54/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 55/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 56/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 57/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 58/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 59/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 60/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 61/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 62/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 63/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 64/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 65/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6683 - f1: 0.3317\n",
      "Epoch 66/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 67/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 68/100\n",
      "330/330 [==============================] - 0s 133us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 69/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 70/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6683 - f1: 0.3317\n",
      "Epoch 71/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 72/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 73/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 74/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 75/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 76/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 77/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 78/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 79/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 80/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 81/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 82/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 83/100\n",
      "330/330 [==============================] - 0s 130us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 0s 125us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 85/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 86/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 87/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 88/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 89/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 90/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 91/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 92/100\n",
      "330/330 [==============================] - 0s 132us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 93/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 94/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 95/100\n",
      "330/330 [==============================] - 0s 131us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 96/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 97/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6683 - f1: 0.3317\n",
      "Epoch 98/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 99/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 100/100\n",
      "330/330 [==============================] - 0s 129us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 1/100\n",
      "330/330 [==============================] - 2s 5ms/step - loss: 0.6169 - f1: 0.4014\n",
      "Epoch 2/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 3/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 4/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 5/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 6/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 7/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 8/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 9/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 10/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 11/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 12/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 13/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 14/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 15/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 16/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 17/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 18/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 19/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 20/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 21/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 22/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6692 - f1: 0.3308\n",
      "Epoch 23/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 24/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 25/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 26/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 27/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 28/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 29/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 30/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6685 - f1: 0.3315\n",
      "Epoch 31/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 32/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 33/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 34/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 35/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6685 - f1: 0.3315\n",
      "Epoch 36/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 37/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 38/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 39/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 40/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 41/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 42/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 43/100\n",
      "330/330 [==============================] - 0s 130us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 44/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 45/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 46/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6684 - f1: 0.3316\n",
      "Epoch 47/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 48/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 49/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 50/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 51/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 52/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 53/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 54/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 55/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 56/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 57/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 58/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 59/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 60/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 61/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6685 - f1: 0.3315\n",
      "Epoch 62/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 63/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 64/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 65/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 66/100\n",
      "330/330 [==============================] - 0s 129us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 0s 120us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 68/100\n",
      "330/330 [==============================] - 0s 131us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 69/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 70/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6608 - f1: 0.3392\n",
      "Epoch 71/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6713 - f1: 0.3287\n",
      "Epoch 72/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6038 - f1: 0.3962\n",
      "Epoch 73/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6191 - f1: 0.3809\n",
      "Epoch 74/100\n",
      "330/330 [==============================] - 0s 118us/step - loss: 0.5450 - f1: 0.4550\n",
      "Epoch 75/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.5189 - f1: 0.4800\n",
      "Epoch 76/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.5789 - f1: 0.4211\n",
      "Epoch 77/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6193 - f1: 0.3807\n",
      "Epoch 78/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6418 - f1: 0.3582\n",
      "Epoch 79/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6439 - f1: 0.3561\n",
      "Epoch 80/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6334 - f1: 0.3666\n",
      "Epoch 81/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6256 - f1: 0.3744\n",
      "Epoch 82/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6497 - f1: 0.3503\n",
      "Epoch 83/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6540 - f1: 0.3460\n",
      "Epoch 84/100\n",
      "330/330 [==============================] - 0s 133us/step - loss: 0.6323 - f1: 0.3677\n",
      "Epoch 85/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6497 - f1: 0.3503\n",
      "Epoch 86/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6629 - f1: 0.3371\n",
      "Epoch 87/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6340 - f1: 0.3660\n",
      "Epoch 88/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6357 - f1: 0.3643\n",
      "Epoch 89/100\n",
      "330/330 [==============================] - 0s 118us/step - loss: 0.6310 - f1: 0.3690\n",
      "Epoch 90/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6579 - f1: 0.3421\n",
      "Epoch 91/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6706 - f1: 0.3294\n",
      "Epoch 92/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6531 - f1: 0.3469\n",
      "Epoch 93/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6310 - f1: 0.3690\n",
      "Epoch 94/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6583 - f1: 0.3417\n",
      "Epoch 95/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6574 - f1: 0.3426\n",
      "Epoch 96/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6249 - f1: 0.3751\n",
      "Epoch 97/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6512 - f1: 0.3488\n",
      "Epoch 98/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6528 - f1: 0.3472\n",
      "Epoch 99/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6451 - f1: 0.3549\n",
      "Epoch 100/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6505 - f1: 0.3495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold = 4\n",
      "(398, 11613)\n",
      "[(1, 102), (2, 40), (3, 91), (4, 165)]\n",
      "[(1, 165), (2, 165), (3, 165), (4, 165)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "330/330 [==============================] - 2s 5ms/step - loss: 0.5562 - f1: 0.4495\n",
      "Epoch 2/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 3/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 4/100\n",
      "330/330 [==============================] - 0s 131us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 5/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 6/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 7/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 8/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 9/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 10/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 11/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 12/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 13/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 14/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6695 - f1: 0.3305\n",
      "Epoch 15/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 16/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 17/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 18/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 19/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6690 - f1: 0.3310\n",
      "Epoch 20/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 21/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 22/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 23/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 24/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 25/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 26/100\n",
      "330/330 [==============================] - 0s 129us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 27/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 28/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 29/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 30/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 31/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 32/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 33/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 34/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 35/100\n",
      "330/330 [==============================] - 0s 131us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 36/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 37/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 38/100\n",
      "330/330 [==============================] - 0s 131us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 39/100\n",
      "330/330 [==============================] - 0s 130us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 40/100\n",
      "330/330 [==============================] - 0s 134us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 41/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6688 - f1: 0.3312\n",
      "Epoch 42/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 43/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 44/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 45/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 46/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6693 - f1: 0.3307\n",
      "Epoch 47/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6698 - f1: 0.3302\n",
      "Epoch 48/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 49/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 50/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 51/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 52/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 53/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 54/100\n",
      "330/330 [==============================] - 0s 130us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 55/100\n",
      "330/330 [==============================] - 0s 129us/step - loss: 0.6684 - f1: 0.3316\n",
      "Epoch 56/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 57/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 58/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 59/100\n",
      "330/330 [==============================] - 0s 133us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 60/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 61/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 62/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 63/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 64/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 65/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 66/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 67/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 68/100\n",
      "330/330 [==============================] - 0s 130us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 69/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 70/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 71/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 72/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 73/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 74/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 75/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 76/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 77/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 78/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 79/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 80/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 81/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 82/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 83/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 0s 122us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 85/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6688 - f1: 0.3312\n",
      "Epoch 86/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6684 - f1: 0.3316\n",
      "Epoch 87/100\n",
      "330/330 [==============================] - 0s 133us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 88/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 89/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 90/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 91/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 92/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 93/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 94/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6685 - f1: 0.3315\n",
      "Epoch 95/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 96/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 97/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 98/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 99/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 100/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 1/100\n",
      "330/330 [==============================] - 2s 5ms/step - loss: 0.6265 - f1: 0.3314\n",
      "Epoch 2/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6698 - f1: 0.3302\n",
      "Epoch 3/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 4/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6683 - f1: 0.3317\n",
      "Epoch 5/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 6/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 7/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 8/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 9/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 10/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 11/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 12/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 13/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 14/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 15/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6687 - f1: 0.3313\n",
      "Epoch 16/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 17/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6685 - f1: 0.3315\n",
      "Epoch 18/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6684 - f1: 0.3316\n",
      "Epoch 19/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 20/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 21/100\n",
      "330/330 [==============================] - 0s 129us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 22/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 23/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 24/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 25/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 26/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 27/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 28/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 29/100\n",
      "330/330 [==============================] - 0s 129us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 30/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 31/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6687 - f1: 0.3313\n",
      "Epoch 32/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 33/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 34/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 35/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 36/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 37/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 38/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 39/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 40/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 41/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 42/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 43/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 44/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 45/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 46/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 47/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 48/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 49/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 50/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 51/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 52/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 53/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 54/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 55/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 56/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 57/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 58/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 59/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 60/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 61/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 62/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 63/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 64/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 65/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 66/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 0s 124us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 68/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 69/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 70/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 71/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 72/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 73/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 74/100\n",
      "330/330 [==============================] - 0s 130us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 75/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 76/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 77/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 78/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 79/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 80/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 81/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 82/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 83/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 84/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 85/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 86/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 87/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 88/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 89/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 90/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 91/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 92/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 93/100\n",
      "330/330 [==============================] - 0s 136us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 94/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 95/100\n",
      "330/330 [==============================] - 0s 129us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 96/100\n",
      "330/330 [==============================] - 0s 131us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 97/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 98/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 99/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 100/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6678 - f1: 0.3322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold = 5\n",
      "(398, 11613)\n",
      "[(1, 102), (2, 40), (3, 91), (4, 165)]\n",
      "[(1, 165), (2, 165), (3, 165), (4, 165)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "330/330 [==============================] - 2s 5ms/step - loss: 0.5345 - f1: 0.4404\n",
      "Epoch 2/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 3/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6692 - f1: 0.3308\n",
      "Epoch 4/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 5/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 6/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 7/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 8/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 9/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 10/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 11/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 12/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 13/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 14/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 15/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 16/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 17/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 18/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 19/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 20/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 21/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 22/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 23/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 24/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 25/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6690 - f1: 0.3310\n",
      "Epoch 26/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 27/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 28/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 29/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 30/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6680 - f1: 0.3320\n",
      "Epoch 31/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 32/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 33/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 34/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 35/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 36/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 37/100\n",
      "330/330 [==============================] - 0s 130us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 38/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 39/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 40/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 41/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 42/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 43/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 44/100\n",
      "330/330 [==============================] - 0s 129us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 45/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 46/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 47/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 48/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 49/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 50/100\n",
      "330/330 [==============================] - 0s 129us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 51/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 52/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 53/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 54/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 55/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 56/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 57/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 58/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6696 - f1: 0.3304\n",
      "Epoch 59/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6667 - f1: 0.3333\n",
      "Epoch 60/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 61/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 62/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 63/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 64/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 65/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 66/100\n",
      "330/330 [==============================] - 0s 137us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 67/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 68/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 69/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 70/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 71/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 72/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 73/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 74/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6677 - f1: 0.3323\n",
      "Epoch 75/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 76/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 77/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 78/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6685 - f1: 0.3315\n",
      "Epoch 79/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 80/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 81/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6668 - f1: 0.3332\n",
      "Epoch 82/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 83/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 0s 126us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 85/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 86/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6675 - f1: 0.3325\n",
      "Epoch 87/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 88/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 89/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 90/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6678 - f1: 0.3322\n",
      "Epoch 91/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 92/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6669 - f1: 0.3331\n",
      "Epoch 93/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 94/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6671 - f1: 0.3329\n",
      "Epoch 95/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6670 - f1: 0.3330\n",
      "Epoch 96/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 97/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 98/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6674 - f1: 0.3326\n",
      "Epoch 99/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6679 - f1: 0.3321\n",
      "Epoch 100/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6672 - f1: 0.3328\n",
      "Epoch 1/100\n",
      "330/330 [==============================] - 2s 5ms/step - loss: 0.6104 - f1: 0.3563\n",
      "Epoch 2/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.5946 - f1: 0.4033\n",
      "Epoch 3/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.4846 - f1: 0.5155\n",
      "Epoch 4/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.5763 - f1: 0.4250\n",
      "Epoch 5/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6658 - f1: 0.3342\n",
      "Epoch 6/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6091 - f1: 0.3909\n",
      "Epoch 7/100\n",
      "330/330 [==============================] - 0s 130us/step - loss: 0.6715 - f1: 0.3285\n",
      "Epoch 8/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6612 - f1: 0.3390\n",
      "Epoch 9/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6443 - f1: 0.3557\n",
      "Epoch 10/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6691 - f1: 0.3309\n",
      "Epoch 11/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6670 - f1: 0.3312\n",
      "Epoch 12/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6625 - f1: 0.3375\n",
      "Epoch 13/100\n",
      "330/330 [==============================] - 0s 134us/step - loss: 0.6576 - f1: 0.3424\n",
      "Epoch 14/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6569 - f1: 0.3431\n",
      "Epoch 15/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6638 - f1: 0.3362\n",
      "Epoch 16/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6641 - f1: 0.3359\n",
      "Epoch 17/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6536 - f1: 0.3464\n",
      "Epoch 18/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6631 - f1: 0.3369\n",
      "Epoch 19/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6574 - f1: 0.3426\n",
      "Epoch 20/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6586 - f1: 0.3414\n",
      "Epoch 21/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6545 - f1: 0.3434\n",
      "Epoch 22/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6694 - f1: 0.3306\n",
      "Epoch 23/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6604 - f1: 0.3396\n",
      "Epoch 24/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6478 - f1: 0.3522\n",
      "Epoch 25/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6681 - f1: 0.3319\n",
      "Epoch 26/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6697 - f1: 0.3303\n",
      "Epoch 27/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6694 - f1: 0.3306\n",
      "Epoch 28/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6502 - f1: 0.3498\n",
      "Epoch 29/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6673 - f1: 0.3327\n",
      "Epoch 30/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6633 - f1: 0.3367\n",
      "Epoch 31/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6513 - f1: 0.3487\n",
      "Epoch 32/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6594 - f1: 0.3406\n",
      "Epoch 33/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6625 - f1: 0.3375\n",
      "Epoch 34/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6651 - f1: 0.3349\n",
      "Epoch 35/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6698 - f1: 0.3302\n",
      "Epoch 36/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6551 - f1: 0.3449\n",
      "Epoch 37/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6628 - f1: 0.3372\n",
      "Epoch 38/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6533 - f1: 0.3467\n",
      "Epoch 39/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6627 - f1: 0.3373\n",
      "Epoch 40/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6694 - f1: 0.3306\n",
      "Epoch 41/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6630 - f1: 0.3370\n",
      "Epoch 42/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6333 - f1: 0.3667\n",
      "Epoch 43/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6682 - f1: 0.3318\n",
      "Epoch 44/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6630 - f1: 0.3370\n",
      "Epoch 45/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6615 - f1: 0.3385\n",
      "Epoch 46/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6473 - f1: 0.3527\n",
      "Epoch 47/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6699 - f1: 0.3301\n",
      "Epoch 48/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6575 - f1: 0.3425\n",
      "Epoch 49/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6577 - f1: 0.3423\n",
      "Epoch 50/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6484 - f1: 0.3516\n",
      "Epoch 51/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6640 - f1: 0.3360\n",
      "Epoch 52/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6676 - f1: 0.3324\n",
      "Epoch 53/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6586 - f1: 0.3414\n",
      "Epoch 54/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6721 - f1: 0.3278\n",
      "Epoch 55/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6659 - f1: 0.3341\n",
      "Epoch 56/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6638 - f1: 0.3362\n",
      "Epoch 57/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6677 - f1: 0.3319\n",
      "Epoch 58/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6626 - f1: 0.3374\n",
      "Epoch 59/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6478 - f1: 0.3517\n",
      "Epoch 60/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6599 - f1: 0.3424\n",
      "Epoch 61/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6564 - f1: 0.3436\n",
      "Epoch 62/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6483 - f1: 0.3517\n",
      "Epoch 63/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6628 - f1: 0.3372\n",
      "Epoch 64/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6722 - f1: 0.3278\n",
      "Epoch 65/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6605 - f1: 0.3395\n",
      "Epoch 66/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6540 - f1: 0.3460\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 0s 123us/step - loss: 0.6616 - f1: 0.3384\n",
      "Epoch 68/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6613 - f1: 0.3387\n",
      "Epoch 69/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6626 - f1: 0.3374\n",
      "Epoch 70/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6495 - f1: 0.3505\n",
      "Epoch 71/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6550 - f1: 0.3450\n",
      "Epoch 72/100\n",
      "330/330 [==============================] - 0s 125us/step - loss: 0.6644 - f1: 0.3356\n",
      "Epoch 73/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6537 - f1: 0.3463\n",
      "Epoch 74/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6488 - f1: 0.3512\n",
      "Epoch 75/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6684 - f1: 0.3316\n",
      "Epoch 76/100\n",
      "330/330 [==============================] - 0s 126us/step - loss: 0.6725 - f1: 0.3275\n",
      "Epoch 77/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6542 - f1: 0.3458\n",
      "Epoch 78/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6626 - f1: 0.3374\n",
      "Epoch 79/100\n",
      "330/330 [==============================] - 0s 127us/step - loss: 0.6644 - f1: 0.3356\n",
      "Epoch 80/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6374 - f1: 0.3626\n",
      "Epoch 81/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6659 - f1: 0.3341\n",
      "Epoch 82/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6599 - f1: 0.3401\n",
      "Epoch 83/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6588 - f1: 0.3409\n",
      "Epoch 84/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6546 - f1: 0.3454\n",
      "Epoch 85/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6465 - f1: 0.3535\n",
      "Epoch 86/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6541 - f1: 0.3459\n",
      "Epoch 87/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6606 - f1: 0.3394\n",
      "Epoch 88/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6685 - f1: 0.3315\n",
      "Epoch 89/100\n",
      "330/330 [==============================] - 0s 121us/step - loss: 0.6619 - f1: 0.3398\n",
      "Epoch 90/100\n",
      "330/330 [==============================] - 0s 132us/step - loss: 0.6628 - f1: 0.3385\n",
      "Epoch 91/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6710 - f1: 0.3290\n",
      "Epoch 92/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6456 - f1: 0.3544\n",
      "Epoch 93/100\n",
      "330/330 [==============================] - 0s 128us/step - loss: 0.6691 - f1: 0.3307\n",
      "Epoch 94/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6697 - f1: 0.3303\n",
      "Epoch 95/100\n",
      "330/330 [==============================] - 0s 119us/step - loss: 0.6566 - f1: 0.3434\n",
      "Epoch 96/100\n",
      "330/330 [==============================] - 0s 123us/step - loss: 0.6585 - f1: 0.3415\n",
      "Epoch 97/100\n",
      "330/330 [==============================] - 0s 120us/step - loss: 0.6629 - f1: 0.3371\n",
      "Epoch 98/100\n",
      "330/330 [==============================] - 0s 130us/step - loss: 0.6570 - f1: 0.3430\n",
      "Epoch 99/100\n",
      "330/330 [==============================] - 0s 124us/step - loss: 0.6593 - f1: 0.3407\n",
      "Epoch 100/100\n",
      "330/330 [==============================] - 0s 122us/step - loss: 0.6694 - f1: 0.3306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "classifier = 'nn2'\n",
    "\n",
    "ave_f1_scores = list()\n",
    "\n",
    "print('5 fold CV starting')\n",
    "for fold_ix in range(1,6):\n",
    "    \n",
    "    print('\\nFold = {0}'.format(fold_ix))\n",
    "    \n",
    "    train_ix = k_fold[(k_fold['fold'] == fold_ix) & (k_fold['train_text'] == 'training')]['user_id']\n",
    "    test_ix = k_fold[(k_fold['fold'] == fold_ix) & (k_fold['train_text'] == 'test')]['user_id']\n",
    "    \n",
    "    x_train = data[data.index.isin(train_ix)].copy()\n",
    "    x_test = data[data.index.isin(test_ix)].copy()\n",
    "    \n",
    "    x_train = x_train.fillna(0)\n",
    "    x_test = x_test.fillna(0)\n",
    "    \n",
    "    y_train = x_train['target']\n",
    "    x_train.drop(['target'], axis=1, inplace=True)\n",
    "    \n",
    "    y_test = x_test['target']\n",
    "    x_test.drop(['target'], axis=1, inplace=True)\n",
    "    \n",
    "    print(x_train.shape)\n",
    "    print(sorted(Counter(y_train).items()))\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "    print(sorted(Counter(y_train).items()))\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.fit_transform(x_test)\n",
    "    \n",
    "    # one-hot encoding\n",
    "    n_y_train = pd.get_dummies(y_train)\n",
    "    n_y_test = pd.get_dummies(y_test)\n",
    "    \n",
    "    if classifier == 'nn':\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(30, input_dim=np.shape(x_train)[1], activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(4, activation='softmax'))\n",
    "        \n",
    "        adam = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "        model = multi_gpu_model(model, gpus=available_gpus)\n",
    "        model.compile(optimizer=adam, loss=f1_loss,\n",
    "                      metrics=[f1])\n",
    "        model.fit(x_train, n_y_train, epochs=100, batch_size=100, verbose=1)\n",
    "        y_pred_train = np.argmax(model.predict(x_train), axis=1) + 1\n",
    "        y_pred_test = np.argmax(model.predict(x_test), axis=1) + 1\n",
    "        y_score = model.predict(x_test)\n",
    "        \n",
    "    elif classifier == 'nn2':\n",
    "        \n",
    "        # divide training dataset\n",
    "        ix_2_4 = (y_train == 2) | (y_train == 4)\n",
    "        ix_1_3 = (y_train == 1) | (y_train == 3)\n",
    "\n",
    "        # fetch the right data points\n",
    "        x_train_24 = x_train[ix_2_4]\n",
    "        y_train_24 = y_train[ix_2_4]\n",
    "        x_train_13 = x_train[ix_1_3]\n",
    "        y_train_13 = y_train[ix_1_3]\n",
    "        \n",
    "        # one-hot encoding\n",
    "        n_y_train = pd.get_dummies(y_train_13)\n",
    "        \n",
    "        model13 = Sequential()\n",
    "        model13.add(Dense(30, input_dim=np.shape(x_train)[1], activation='relu'))\n",
    "        model13.add(Dropout(0.2))\n",
    "        model13.add(Dense(2, activation='softmax'))\n",
    "        \n",
    "        adam = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "        model13 = multi_gpu_model(model13, gpus=available_gpus)\n",
    "        model13.compile(optimizer=adam, loss=f1_loss,\n",
    "                      metrics=[f1])\n",
    "        model13.fit(x_train_13, n_y_train, epochs=100, batch_size=100, verbose=1)\n",
    "        \n",
    "        # one-hot encoding\n",
    "        n_y_train = pd.get_dummies(y_train_24)\n",
    "        \n",
    "        model24 = Sequential()\n",
    "        model24.add(Dense(30, input_dim=np.shape(x_train)[1], activation='relu'))\n",
    "        model24.add(Dropout(0.2))\n",
    "        model24.add(Dense(2, activation='softmax'))\n",
    "        \n",
    "        adam = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "        model24 = multi_gpu_model(model24, gpus=available_gpus)\n",
    "        model24.compile(optimizer=adam, loss=f1_loss,\n",
    "                      metrics=[f1])\n",
    "        model24.fit(x_train_24, n_y_train, epochs=100, batch_size=100, verbose=1)\n",
    "        \n",
    "        y_pred_test_13 = model13.predict(x_test)\n",
    "        y_pred_test_24 = model24.predict(x_test)\n",
    "\n",
    "        y_score = np.concatenate((y_pred_test_13, y_pred_test_24), axis=1)\n",
    "        y_pred_test = np.argmax(y_score, axis=1) + 1\n",
    "        \n",
    "    elif classifier == 'logit':\n",
    "        model = LogisticRegression(class_weight='balanced', n_jobs=-1,\n",
    "                          multi_class='auto', solver='lbfgs',\n",
    "                          tol=0.00001, C=10, max_iter=1, verbose=True,\n",
    "                          random_state=random_state)  \n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred_test = model.predict(x_test)\n",
    "        y_score=model.predict_proba(x_test)\n",
    "    \n",
    "    elif classifier == 'logit2':\n",
    "        \n",
    "        # 1-2,3-4 = 0.263955\n",
    "        # 1-3,2-4 = 0.332951\n",
    "        # 1-4,2-3 = 0.193993\n",
    "\n",
    "        # divide training dataset\n",
    "        ix_2_4 = (y_train == 2) | (y_train == 4)\n",
    "        ix_1_3 = (y_train == 1) | (y_train == 3)\n",
    "\n",
    "        # fetch the right data points\n",
    "        x_train_24 = x_train[ix_2_4]\n",
    "        y_train_24 = y_train[ix_2_4]\n",
    "        x_train_13 = x_train[ix_1_3]\n",
    "        y_train_13 = y_train[ix_1_3]\n",
    "\n",
    "        # first model\n",
    "        model24 = LogisticRegression(class_weight='balanced', n_jobs=-1,\n",
    "                          multi_class='auto', solver='lbfgs',\n",
    "                          tol=0.00001, C=10, max_iter=1, verbose=True,\n",
    "                          random_state=random_state)  \n",
    "        model24.fit(x_train_24, y_train_24)\n",
    "\n",
    "        # second model\n",
    "        model13 = LogisticRegression(class_weight='balanced', n_jobs=-1,\n",
    "                          multi_class='auto', solver='lbfgs',\n",
    "                          tol=0.00001, C=10, max_iter=1, verbose=True,\n",
    "                          random_state=random_state)  \n",
    "        model13.fit(x_train_13, y_train_13)\n",
    "\n",
    "        y_pred_test_13 = model13.predict(x_train_13)\n",
    "        y_score_13 = model13.predict_proba(x_train_13)\n",
    "        print(class_report( y_true=y_train_13, y_pred=y_pred_test_13, y_score=y_score_13, average='macro'))\n",
    "\n",
    "        y_pred_test_24 = model24.predict(x_train_24)\n",
    "        y_score_24 = model24.predict_proba(x_train_24)\n",
    "        print(class_report( y_true=y_train_24, y_pred=y_pred_test_24, y_score=y_score_24, average='macro'))\n",
    "\n",
    "        y_pred_test_13 = model13.predict_log_proba(x_test)\n",
    "        y_pred_test_24 = model24.predict_log_proba(x_test)\n",
    "\n",
    "        y_score = np.concatenate((y_pred_test_13, y_pred_test_24), axis=1)\n",
    "        y_pred_test = np.argmax(y_score, axis=1) + 1\n",
    "    \n",
    "    report_with_auc = class_report( y_true=y_test, \n",
    "                                   y_pred=y_pred_test, \n",
    "                                   y_score=y_score,\n",
    "                                   average='macro')\n",
    "        \n",
    "    ave_f1_scores.append(report_with_auc['f1-score'].values[-1])\n",
    "\n",
    "    cv_column = [fold_ix]\n",
    "    cv_column.extend( [''] * (report_with_auc.index.shape[0] - 1))\n",
    "    report_with_auc['Fold'] = cv_column\n",
    "    report_with_auc['Risk-Factor'] = report_with_auc.index\n",
    "    report_with_auc = report_with_auc.set_index(['Fold', 'Risk-Factor'])\n",
    "\n",
    "    if fold_ix == 1:\n",
    "        report_with_auc_df = report_with_auc.copy()\n",
    "    else:\n",
    "        report_with_auc_df = report_with_auc_df.append(report_with_auc.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>pred-cnt</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold</th>\n",
       "      <th>Risk-Factor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <td>0.099010</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.180180</td>\n",
       "      <td>10.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.045045</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>1</th>\n",
       "      <td>0.260000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.412698</td>\n",
       "      <td>26.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.103175</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>2</th>\n",
       "      <td>0.101010</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.183486</td>\n",
       "      <td>10.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.025253</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.045872</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>1</th>\n",
       "      <td>0.255102</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>25.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.063776</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.101626</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>2</th>\n",
       "      <td>0.102041</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>10.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.025510</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.046296</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  precision  recall  f1-score  support  pred-cnt  AUC\n",
       "Fold Risk-Factor                                                     \n",
       "1    2             0.099010    1.00  0.180180     10.0     101.0  0.5\n",
       "     avg / total   0.024752    0.25  0.045045     10.0      10.0  0.5\n",
       "2    1             0.260000    1.00  0.412698     26.0     100.0  0.5\n",
       "     avg / total   0.065000    0.25  0.103175     26.0      26.0  0.5\n",
       "3    2             0.101010    1.00  0.183486     10.0      99.0  0.5\n",
       "     avg / total   0.025253    0.25  0.045872     10.0      10.0  0.5\n",
       "4    1             0.255102    1.00  0.406504     25.0      98.0  0.5\n",
       "     avg / total   0.063776    0.25  0.101626     25.0      25.0  0.5\n",
       "5    2             0.102041    1.00  0.185185     10.0      98.0  0.5\n",
       "     avg / total   0.025510    0.25  0.046296     10.0      10.0  0.5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_with_auc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average f1-score (all folds) = 0.06840270408182693\n"
     ]
    }
   ],
   "source": [
    "print('average f1-score (all folds) = {0}'.format(np.mean(ave_f1_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "'all'    \n",
    "'SETAAA' \n",
    "'SETBBB' \n",
    "'SETCCC'\n",
    "'SETDDD'\n",
    "'SETEEE'\n",
    "'SETFFF' \n",
    "'SETGGG' \n",
    "'SETHHH' \n",
    "'SETIII' \n",
    "'SETJJJ' \n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETFFF', 'SETGGG'\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETEEE'\n",
    "'SETAAA', 'SETKKK' 0.20175058667560125 (droptout)\n",
    "'SETAAA', 'SETLLL' 0.12430375888494909 (dropout)\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETLLL', 'SETHHH', 'SETGGG' 0.11088291908006576 (dropout)\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETLLL', 'SETHHH', 'SETGGG' 0.07753291443154947 (reg)\n",
    "'SETAAA', 'SETDDD', 'SETEEE'  0.17349602442078949 (drop)\n",
    "\n",
    "(nn) (dropout)\n",
    "'SETAAA', 'SETDDD', 'SETEEE' 0.19014727458966665\n",
    "\n",
    "(nn) (dropout) (1hl)\n",
    "'SETAAA', 'SETDDD', 'SETEEE' 0.3255926882010475\n",
    "\n",
    "(logit)\n",
    "'SETAAA', 'SETDDD', 'SETEEE'  0.32241407748653905 \n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETLLL' 0.24496511781531444\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETFFF', 'SETGGG' 0.11353938632270907\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETFFF' 0.12148581335816777\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETGGG' 0.25394562653592845"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
