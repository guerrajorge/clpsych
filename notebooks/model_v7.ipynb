{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using imbalanced-learn samplers API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'clpsych'\n",
    "project_path = Path(os.getcwd()).parent\n",
    "src_path = Path(project_path, 'src')\n",
    "utils_path = Path(project_path, 'utils')\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    data_path = 'D:\\Dataset\\{0}\\dataset'.format(project_name)\n",
    "    embedding_path = 'D:\\Dataset\\{0}\\embedding'.format(project_name)\n",
    "    model_path = 'D:\\Dataset\\{0}\\embedding'.format(project_name)\n",
    "    \n",
    "elif sys.platform == 'darwin':\n",
    "    data_path = '/Volumes/Dataset/{0}/dataset'.format(project_name)\n",
    "    embedding_path = '/Volumes/Dataset/{0}/embedding'.format(project_name)\n",
    "    \n",
    "else:\n",
    "    data_path = Path(project_path, 'dataset')\n",
    "    model_path = Path(project_path, 'models')\n",
    "    embedding_path = Path(project_path, 'embedding')\n",
    "\n",
    "# including the project folder and the utils folder\n",
    "if str(utils_path) not in ''.join(sys.path):\n",
    "    sys.path.extend([str(project_path), str(utils_path), str(src_path)])\n",
    "\n",
    "print('project path = {0}'.format(project_path))\n",
    "print('data path = {0}'.format(data_path))\n",
    "print('model path = {0}'.format(model_path))\n",
    "print('embedding path = {0}'.format(embedding_path))\n",
    "print('sys.path = {0}'.format(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import interp \n",
    "from collections import Counter\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# optimizer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from keras.utils import multi_gpu_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# ml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# deeplearning\n",
    "import tensorflow as tf\n",
    "from keras.layers import Reshape, Flatten, Concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout\n",
    "from keras import metrics\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam, Adam\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "from keras.layers import Input\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 13]\n",
    "\n",
    "# seed for numpy and sklearn\n",
    "random_state = 7\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "print('Tensorflow recognizes GPUs')\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "available_gpu = backend.tensorflow_backend._get_available_gpus()\n",
    "assert len(available_gpu) > 0\n",
    "available_gpus = len(available_gpu)\n",
    "print('number of available GPUs = {0}'.format(available_gpus))\n",
    "print('list of GPUs = {0}\\n'.format(available_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_report(y_true, y_pred, y_score=None, average='macro'):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        print(\"Error! y_true {0} is not the same shape as y_pred {1}\".format(\n",
    "              y_true.shape,\n",
    "              y_pred.shape)\n",
    "        )\n",
    "        return\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "\n",
    "    if len(y_true.shape) == 1:\n",
    "        lb.fit(y_true)\n",
    "\n",
    "    #Value counts of predictions\n",
    "    labels, cnt = np.unique(\n",
    "        y_pred,\n",
    "        return_counts=True)\n",
    "    n_classes = len(labels)\n",
    "    pred_cnt = pd.Series(cnt, index=labels)\n",
    "\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            labels=labels)\n",
    "\n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average=average))\n",
    "\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list(metrics_summary),\n",
    "        index=metrics_sum_index,\n",
    "        columns=labels)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    class_report_df['avg / total'] = avg[:-1] + [total]\n",
    "\n",
    "    class_report_df = class_report_df.T\n",
    "    class_report_df['pred-cnt'] = pred_cnt\n",
    "    class_report_df['pred-cnt'].iloc[-1] = total\n",
    "\n",
    "    if not (y_score is None):\n",
    "        # false positive rate\n",
    "        fpr = dict()\n",
    "        # true positive rate\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for label_ix, label in enumerate(labels):\n",
    "            fpr[label], tpr[label], _ = roc_curve(\n",
    "                (y_true == label).astype(int), \n",
    "                y_score[:, label_ix])\n",
    "\n",
    "            roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        if average == 'micro':\n",
    "            if n_classes <= 2:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                    lb.transform(y_true).ravel(), \n",
    "                    y_score[:, 1].ravel())\n",
    "            else:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                        lb.transform(y_true).ravel(), \n",
    "                        y_score.ravel())\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(\n",
    "                fpr[\"avg / total\"], \n",
    "                tpr[\"avg / total\"])\n",
    "\n",
    "        elif average == 'macro':\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([\n",
    "                fpr[i] for i in labels]\n",
    "            ))\n",
    "\n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in labels:\n",
    "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "            # Finally average it and compute AUC\n",
    "            mean_tpr /= n_classes\n",
    "\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        class_report_df['AUC'] = pd.Series(roc_auc)\n",
    "\n",
    "    return class_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path(data_path, 'static_features_pandas_v2.pkl')\n",
    "filename = Path(data_path, 'static_features_pandas_aug_v2.2.pkl')\n",
    "dataset = pd.read_pickle(filename)\n",
    "filename = Path(data_path, 'extracted_features_sentiment_per_user_filtered.csv')\n",
    "sentiment_statics = pd.read_csv(filename)\n",
    "sentiment_statics.set_index('user_id', inplace=True)\n",
    "# sentiment_statics.drop('risk_label', axis=1, inplace=True)\n",
    "n_columns = ['SETLLL__{0}'.format(x) for x in sentiment_statics.columns]\n",
    "sentiment_statics.columns = n_columns\n",
    "print(dataset.shape)\n",
    "dataset = dataset.merge(right=sentiment_statics, left_index=True, right_index=True)\n",
    "print(dataset.shape)\n",
    "dataset.to_pickle(Path(data_path, 'static_features_pandas_aug_v2.2.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'static_features_pandas_v2.pkl'\n",
    "filename = 'static_features_pandas_aug_v2.pkl'\n",
    "filename = 'static_features_pandas_aug_v2.2.pkl'\n",
    "dataset = pd.read_pickle(Path(data_path, filename))\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 5-fold cross validation test harness\n",
    "k_fold = pd.read_csv(Path(data_path, 'clpsych19_public_crossvalidation_splits.csv'), header=None,\n",
    "                    names=['fold', 'train_text', 'user_id'])\n",
    "\n",
    "# keep non conrol user ids\n",
    "k_fold = k_fold[k_fold['user_id'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Feature Set   Description  \n",
    "A             static_derieved_features  \n",
    "B             post_coount_by_subreddit\n",
    "C             lexicon_count  \n",
    "D             sentiments_macro  \n",
    "E             sentiments_micro  \n",
    "F             empathy\n",
    "G             readability\n",
    "H             social_context\n",
    "I             srl\n",
    "J             ctakes\n",
    "K             sentiment statics\n",
    "L             sentiment statics filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_interest = ['SETAAA'] \n",
    "# features_interest = ['SETBBB'] # contains NaN\n",
    "# features_interest = ['SETCCC'] # contains NaN\n",
    "# features_interest = ['SETDDD']\n",
    "# features_interest = ['SETEEE']\n",
    "# features_interest = ['SETFFF'] # contains NaN\n",
    "# features_interest = ['SETGGG'] # contains NaN\n",
    "# features_interest = ['SETHHH'] # contains NaN\n",
    "# features_interest = ['SETIII'] # contains NaN\n",
    "# features_interest = ['SETJJJ'] # contains NaN\n",
    "# features_interest = ['SETKKK'] # contains NaN\n",
    "# features_interest = ['SETLLL'] # contains NaN\n",
    "# features_interest = ['SETAAA', 'SETDDD', 'SETEEE', 'SETFFF', 'SETGGG']\n",
    "features_interest = ['SETAAA', 'SETDDD', 'SETEEE']\n",
    "\n",
    "features = [x for x in dataset.columns if x.split('__')[0] in features_interest]\n",
    "# including label\n",
    "features.append('target')\n",
    "data = dataset[features].copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = 'nn2'\n",
    "\n",
    "ave_f1_scores = list()\n",
    "\n",
    "print('5 fold CV starting')\n",
    "for fold_ix in range(1,6):\n",
    "    \n",
    "    print('\\nFold = {0}'.format(fold_ix))\n",
    "    \n",
    "    train_ix = k_fold[(k_fold['fold'] == fold_ix) & (k_fold['train_text'] == 'training')]['user_id']\n",
    "    test_ix = k_fold[(k_fold['fold'] == fold_ix) & (k_fold['train_text'] == 'test')]['user_id']\n",
    "    \n",
    "    x_train = data[data.index.isin(train_ix)].copy()\n",
    "    x_test = data[data.index.isin(test_ix)].copy()\n",
    "    \n",
    "    x_train = x_train.fillna(0)\n",
    "    x_test = x_test.fillna(0)\n",
    "    \n",
    "    y_train = x_train['target']\n",
    "    x_train.drop(['target'], axis=1, inplace=True)\n",
    "    \n",
    "    y_test = x_test['target']\n",
    "    x_test.drop(['target'], axis=1, inplace=True)\n",
    "    \n",
    "    print(x_train.shape)\n",
    "    print(sorted(Counter(y_train).items()))\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "    print(sorted(Counter(y_train).items()))\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.fit_transform(x_test)\n",
    "    \n",
    "    # one-hot encoding\n",
    "    n_y_train = pd.get_dummies(y_train)\n",
    "    n_y_test = pd.get_dummies(y_test)\n",
    "    \n",
    "    if classifier == 'nn':\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(30, input_dim=np.shape(x_train)[1], activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(4, activation='softmax'))\n",
    "        \n",
    "        adam = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "        model = multi_gpu_model(model, gpus=available_gpus)\n",
    "        model.compile(optimizer=adam, loss=f1_loss,\n",
    "                      metrics=[f1])\n",
    "        model.fit(x_train, n_y_train, epochs=100, batch_size=100, verbose=1)\n",
    "        y_pred_train = np.argmax(model.predict(x_train), axis=1) + 1\n",
    "        y_pred_test = np.argmax(model.predict(x_test), axis=1) + 1\n",
    "        y_score = model.predict(x_test)\n",
    "        \n",
    "    elif classifier == 'nn2':\n",
    "        \n",
    "        # divide training dataset\n",
    "        ix_2_4 = (y_train == 2) | (y_train == 4)\n",
    "        ix_1_3 = (y_train == 1) | (y_train == 3)\n",
    "\n",
    "        # fetch the right data points\n",
    "        x_train_24 = x_train[ix_2_4]\n",
    "        y_train_24 = y_train[ix_2_4]\n",
    "        x_train_13 = x_train[ix_1_3]\n",
    "        y_train_13 = y_train[ix_1_3]\n",
    "        \n",
    "        # one-hot encoding\n",
    "        n_y_train = pd.get_dummies(y_train_13)\n",
    "        \n",
    "        model13 = Sequential()\n",
    "        model13.add(Dense(30, input_dim=np.shape(x_train)[1], activation='relu'))\n",
    "        model13.add(Dropout(0.2))\n",
    "        model13.add(Dense(2, activation='softmax'))\n",
    "        \n",
    "        adam = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "        model13 = multi_gpu_model(model13, gpus=available_gpus)\n",
    "        model13.compile(optimizer=adam, loss=f1_loss,\n",
    "                      metrics=[f1])\n",
    "        model13.fit(x_train_13, n_y_train, epochs=100, batch_size=100, verbose=1)\n",
    "        \n",
    "        # one-hot encoding\n",
    "        n_y_train = pd.get_dummies(y_train_24)\n",
    "        \n",
    "        model24 = Sequential()\n",
    "        model24.add(Dense(30, input_dim=np.shape(x_train)[1], activation='relu'))\n",
    "        model24.add(Dropout(0.2))\n",
    "        model24.add(Dense(2, activation='softmax'))\n",
    "        \n",
    "        adam = Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "        model24 = multi_gpu_model(model24, gpus=available_gpus)\n",
    "        model24.compile(optimizer=adam, loss=f1_loss,\n",
    "                      metrics=[f1])\n",
    "        model24.fit(x_train_24, n_y_train, epochs=100, batch_size=100, verbose=1)\n",
    "        \n",
    "        y_pred_test_13 = model13.predict(x_test)\n",
    "        y_pred_test_24 = model24.predict(x_test)\n",
    "\n",
    "        y_score = np.concatenate((y_pred_test_13, y_pred_test_24), axis=1)\n",
    "        y_pred_test = np.argmax(y_score, axis=1) + 1\n",
    "        \n",
    "    elif classifier == 'logit':\n",
    "        model = LogisticRegression(class_weight='balanced', n_jobs=-1,\n",
    "                          multi_class='auto', solver='lbfgs',\n",
    "                          tol=0.00001, C=10, max_iter=1, verbose=True,\n",
    "                          random_state=random_state)  \n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred_test = model.predict(x_test)\n",
    "        y_score=model.predict_proba(x_test)\n",
    "    \n",
    "    elif classifier == 'logit2':\n",
    "        \n",
    "        # 1-2,3-4 = 0.263955\n",
    "        # 1-3,2-4 = 0.332951\n",
    "        # 1-4,2-3 = 0.193993\n",
    "\n",
    "        # divide training dataset\n",
    "        ix_2_4 = (y_train == 2) | (y_train == 4)\n",
    "        ix_1_3 = (y_train == 1) | (y_train == 3)\n",
    "\n",
    "        # fetch the right data points\n",
    "        x_train_24 = x_train[ix_2_4]\n",
    "        y_train_24 = y_train[ix_2_4]\n",
    "        x_train_13 = x_train[ix_1_3]\n",
    "        y_train_13 = y_train[ix_1_3]\n",
    "\n",
    "        # first model\n",
    "        model24 = LogisticRegression(class_weight='balanced', n_jobs=-1,\n",
    "                          multi_class='auto', solver='lbfgs',\n",
    "                          tol=0.00001, C=10, max_iter=1, verbose=True,\n",
    "                          random_state=random_state)  \n",
    "        model24.fit(x_train_24, y_train_24)\n",
    "\n",
    "        # second model\n",
    "        model13 = LogisticRegression(class_weight='balanced', n_jobs=-1,\n",
    "                          multi_class='auto', solver='lbfgs',\n",
    "                          tol=0.00001, C=10, max_iter=1, verbose=True,\n",
    "                          random_state=random_state)  \n",
    "        model13.fit(x_train_13, y_train_13)\n",
    "\n",
    "        y_pred_test_13 = model13.predict(x_train_13)\n",
    "        y_score_13 = model13.predict_proba(x_train_13)\n",
    "        print(class_report( y_true=y_train_13, y_pred=y_pred_test_13, y_score=y_score_13, average='macro'))\n",
    "\n",
    "        y_pred_test_24 = model24.predict(x_train_24)\n",
    "        y_score_24 = model24.predict_proba(x_train_24)\n",
    "        print(class_report( y_true=y_train_24, y_pred=y_pred_test_24, y_score=y_score_24, average='macro'))\n",
    "\n",
    "        y_pred_test_13 = model13.predict_log_proba(x_test)\n",
    "        y_pred_test_24 = model24.predict_log_proba(x_test)\n",
    "\n",
    "        y_score = np.concatenate((y_pred_test_13, y_pred_test_24), axis=1)\n",
    "        y_pred_test = np.argmax(y_score, axis=1) + 1\n",
    "    \n",
    "    report_with_auc = class_report( y_true=y_test, \n",
    "                                   y_pred=y_pred_test, \n",
    "                                   y_score=y_score,\n",
    "                                   average='macro')\n",
    "        \n",
    "    ave_f1_scores.append(report_with_auc['f1-score'].values[-1])\n",
    "\n",
    "    cv_column = [fold_ix]\n",
    "    cv_column.extend( [''] * (report_with_auc.index.shape[0] - 1))\n",
    "    report_with_auc['Fold'] = cv_column\n",
    "    report_with_auc['Risk-Factor'] = report_with_auc.index\n",
    "    report_with_auc = report_with_auc.set_index(['Fold', 'Risk-Factor'])\n",
    "\n",
    "    if fold_ix == 1:\n",
    "        report_with_auc_df = report_with_auc.copy()\n",
    "    else:\n",
    "        report_with_auc_df = report_with_auc_df.append(report_with_auc.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_with_auc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('average f1-score (all folds) = {0}'.format(np.mean(ave_f1_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "'all'    \n",
    "'SETAAA' \n",
    "'SETBBB' \n",
    "'SETCCC'\n",
    "'SETDDD'\n",
    "'SETEEE'\n",
    "'SETFFF' \n",
    "'SETGGG' \n",
    "'SETHHH' \n",
    "'SETIII' \n",
    "'SETJJJ' \n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETFFF', 'SETGGG'\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETEEE'\n",
    "'SETAAA', 'SETKKK' 0.20175058667560125 (droptout)\n",
    "'SETAAA', 'SETLLL' 0.12430375888494909 (dropout)\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETLLL', 'SETHHH', 'SETGGG' 0.11088291908006576 (dropout)\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETLLL', 'SETHHH', 'SETGGG' 0.07753291443154947 (reg)\n",
    "'SETAAA', 'SETDDD', 'SETEEE'  0.17349602442078949 (drop)\n",
    "\n",
    "(nn) (dropout)\n",
    "'SETAAA', 'SETDDD', 'SETEEE' 0.19014727458966665\n",
    "\n",
    "(nn) (dropout) (1hl)\n",
    "'SETAAA', 'SETDDD', 'SETEEE' 0.3255926882010475\n",
    "\n",
    "(logit)\n",
    "'SETAAA', 'SETDDD', 'SETEEE'  0.32241407748653905 \n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETLLL' 0.24496511781531444\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETFFF', 'SETGGG' 0.11353938632270907\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETFFF' 0.12148581335816777\n",
    "'SETAAA', 'SETDDD', 'SETEEE', 'SETGGG' 0.25394562653592845"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
