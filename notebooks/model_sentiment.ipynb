{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project path = /home/guerramarj/github/clpsych\n",
      "data path = /home/guerramarj/github/clpsych/dataset\n",
      "model path = /home/guerramarj/github/clpsych/models\n",
      "sys.path = ['/cm/local/apps/cuda/libs/current/pynvml', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python36.zip', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/lib-dynload', '', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/Mako-1.0.7-py3.6.egg', '/home/guerramarj/packages/anaconda3/envs/deeplearning/lib/python3.6/site-packages/IPython/extensions', '/home/guerramarj/.ipython', '/home/guerramarj/github/clpsych', '/home/guerramarj/github/clpsych/utils', '/home/guerramarj/github/clpsych/src']\n"
     ]
    }
   ],
   "source": [
    "project_name = 'clpsych'\n",
    "project_path = Path(os.getcwd()).parent\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    data_path = 'D:\\Dataset\\{0}\\dataset'.format(project_name)\n",
    "    model_path = 'D:\\Dataset\\{0}\\models'.format(project_name)\n",
    "    src_path = '/Volumes/Dataset/{0}/src'.format(project_name)\n",
    "    \n",
    "elif sys.platform == 'darwin':\n",
    "    data_path = '/Volumes/Dataset/{0}/dataset'.format(project_name)\n",
    "    model_path = '/Volumes/Dataset/{0}/models'.format(project_name)\n",
    "    src_path = '/Volumes/Dataset/{0}/src'.format(project_name)\n",
    "    \n",
    "else:\n",
    "    data_path = Path(project_path, 'dataset')\n",
    "    model_path = Path(project_path, 'models')\n",
    "    src_path = Path(project_path, 'src')\n",
    "\n",
    "utils_path = str(Path(project_path, 'utils'))\n",
    "# including the project folder and the utils folder\n",
    "if utils_path not in ''.join(sys.path):\n",
    "    sys.path.extend([str(project_path), utils_path, str(src_path)])\n",
    "\n",
    "print('project path = {0}'.format(project_path))\n",
    "print('data path = {0}'.format(data_path))\n",
    "print('model path = {0}'.format(model_path))\n",
    "print('sys.path = {0}'.format(sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from utils.datapath import data_path_scripts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from numpy import interp \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam, Adam\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "import tensorflow as tf\n",
    "import _pickle as pickle \n",
    "import stanfordnlp\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.datasets import imdb \n",
    "from utils.data_helpers import load_data_and_labels\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import multi_gpu_model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 13]\n",
    "\n",
    "# seed for numpy and sklearn\n",
    "random_state = 7\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow recognizes GPUs\n",
      "number of available GPUs = 4\n",
      "list of GPUs = ['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confirm TensorFlow sees the GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "assert 'GPU' in str(device_lib.list_local_devices())\n",
    "print('Tensorflow recognizes GPUs')\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "available_gpu = backend.tensorflow_backend._get_available_gpus()\n",
    "assert len(available_gpu) > 0\n",
    "available_gpus = len(available_gpu)\n",
    "print('number of available GPUs = {0}'.format(available_gpus))\n",
    "print('list of GPUs = {0}\\n'.format(available_gpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_report(y_true, y_pred, y_score=None, average='macro'):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        print(\"Error! y_true {0} is not the same shape as y_pred {1}\".format(\n",
    "              y_true.shape,\n",
    "              y_pred.shape)\n",
    "        )\n",
    "        return\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "\n",
    "    if len(y_true.shape) == 1:\n",
    "        lb.fit(y_true)\n",
    "\n",
    "    #Value counts of predictions\n",
    "    labels, cnt = np.unique(\n",
    "        y_pred,\n",
    "        return_counts=True)\n",
    "    n_classes = len(labels)\n",
    "    pred_cnt = pd.Series(cnt, index=labels)\n",
    "\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            labels=labels)\n",
    "\n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average=average))\n",
    "\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list(metrics_summary),\n",
    "        index=metrics_sum_index,\n",
    "        columns=labels)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    class_report_df['avg / total'] = avg[:-1] + [total]\n",
    "\n",
    "    class_report_df = class_report_df.T\n",
    "    class_report_df['pred-cnt'] = pred_cnt\n",
    "    class_report_df['pred-cnt'].iloc[-1] = total\n",
    "\n",
    "    if not (y_score is None):\n",
    "        # false positive rate\n",
    "        fpr = dict()\n",
    "        # true positive rate\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for label_ix, label in enumerate(labels):\n",
    "            fpr[label], tpr[label], _ = roc_curve(\n",
    "                (y_true == label).astype(int), \n",
    "                y_score[:, label_ix])\n",
    "\n",
    "            roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        if average == 'micro':\n",
    "            if n_classes <= 2:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                    lb.transform(y_true).ravel(), \n",
    "                    y_score[:, 1].ravel())\n",
    "            else:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                        lb.transform(y_true).ravel(), \n",
    "                        y_score.ravel())\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(\n",
    "                fpr[\"avg / total\"], \n",
    "                tpr[\"avg / total\"])\n",
    "\n",
    "        elif average == 'macro':\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([\n",
    "                fpr[i] for i in labels]\n",
    "            ))\n",
    "\n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in labels:\n",
    "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "            # Finally average it and compute AUC\n",
    "            mean_tpr /= n_classes\n",
    "\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        class_report_df['AUC'] = pd.Series(roc_auc)\n",
    "\n",
    "    return class_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_metric(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    backend.get_session().run(tf.local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clpsych19_public_crowd_train.csv         risk_title_body.csv\r\n",
      "clpsych19_public_shared_task_posts.csv   sentiment_per_post.csv\r\n",
      "clpsych19_public_task_B_train_posts.csv  sentiment_per_user.csv\r\n",
      "database.sqlite                          sentiment_per_user_macro.csv\r\n",
      "dataset_sentiments.csv                   sentiment_per_user_micro.csv\r\n",
      "\u001b[0m\u001b[01;32mpost_risklabel.csv\u001b[0m*                      sentiment_per_user_micro_macro.csv\r\n",
      "post_sent_user_risk.csv                  sentiment_user.csv\r\n",
      "post_user_risk                           sent_user.csv\r\n",
      "post_user_risk.csv                       Tweets.csv\r\n",
      "preprocessed_corpus.pkl                  twitter-airline-sentiment.zip\r\n",
      "\u001b[01;32mrisk_tbs.csv\u001b[0m*                            users.csv\r\n",
      "\u001b[01;32mrisk_tbs_num.csv\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "%ls ../dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>sent_1_macro</th>\n",
       "      <th>sent_2_macro</th>\n",
       "      <th>sent_3_macro</th>\n",
       "      <th>sent_4_macro</th>\n",
       "      <th>sent_5_macro</th>\n",
       "      <th>risk_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40130</td>\n",
       "      <td>0.169393</td>\n",
       "      <td>0.406493</td>\n",
       "      <td>0.283100</td>\n",
       "      <td>0.103859</td>\n",
       "      <td>0.037155</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19368</td>\n",
       "      <td>0.063450</td>\n",
       "      <td>0.282020</td>\n",
       "      <td>0.427240</td>\n",
       "      <td>0.189121</td>\n",
       "      <td>0.038170</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20841</td>\n",
       "      <td>0.084885</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.426645</td>\n",
       "      <td>0.097172</td>\n",
       "      <td>0.020333</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8720</td>\n",
       "      <td>0.132132</td>\n",
       "      <td>0.447101</td>\n",
       "      <td>0.281858</td>\n",
       "      <td>0.108090</td>\n",
       "      <td>0.030819</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32730</td>\n",
       "      <td>0.113888</td>\n",
       "      <td>0.424265</td>\n",
       "      <td>0.250576</td>\n",
       "      <td>0.170719</td>\n",
       "      <td>0.040552</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  sent_1_macro  sent_2_macro  sent_3_macro  sent_4_macro  \\\n",
       "0    40130      0.169393      0.406493      0.283100      0.103859   \n",
       "1    19368      0.063450      0.282020      0.427240      0.189121   \n",
       "2    20841      0.084885      0.370965      0.426645      0.097172   \n",
       "3     8720      0.132132      0.447101      0.281858      0.108090   \n",
       "4    32730      0.113888      0.424265      0.250576      0.170719   \n",
       "\n",
       "   sent_5_macro risk_label  \n",
       "0      0.037155          a  \n",
       "1      0.038170          a  \n",
       "2      0.020333          a  \n",
       "3      0.030819          a  \n",
       "4      0.040552          a  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(Path(data_path, 'sentiment_per_user_macro.csv'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>sent_1_macro</th>\n",
       "      <th>sent_2_macro</th>\n",
       "      <th>sent_3_macro</th>\n",
       "      <th>sent_4_macro</th>\n",
       "      <th>sent_5_macro</th>\n",
       "      <th>risk_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40130</td>\n",
       "      <td>0.169393</td>\n",
       "      <td>0.406493</td>\n",
       "      <td>0.283100</td>\n",
       "      <td>0.103859</td>\n",
       "      <td>0.037155</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19368</td>\n",
       "      <td>0.063450</td>\n",
       "      <td>0.282020</td>\n",
       "      <td>0.427240</td>\n",
       "      <td>0.189121</td>\n",
       "      <td>0.038170</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20841</td>\n",
       "      <td>0.084885</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.426645</td>\n",
       "      <td>0.097172</td>\n",
       "      <td>0.020333</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8720</td>\n",
       "      <td>0.132132</td>\n",
       "      <td>0.447101</td>\n",
       "      <td>0.281858</td>\n",
       "      <td>0.108090</td>\n",
       "      <td>0.030819</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32730</td>\n",
       "      <td>0.113888</td>\n",
       "      <td>0.424265</td>\n",
       "      <td>0.250576</td>\n",
       "      <td>0.170719</td>\n",
       "      <td>0.040552</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  sent_1_macro  sent_2_macro  sent_3_macro  sent_4_macro  \\\n",
       "0    40130      0.169393      0.406493      0.283100      0.103859   \n",
       "1    19368      0.063450      0.282020      0.427240      0.189121   \n",
       "2    20841      0.084885      0.370965      0.426645      0.097172   \n",
       "3     8720      0.132132      0.447101      0.281858      0.108090   \n",
       "4    32730      0.113888      0.424265      0.250576      0.170719   \n",
       "\n",
       "   sent_5_macro risk_label  \n",
       "0      0.037155          a  \n",
       "1      0.038170          a  \n",
       "2      0.020333          a  \n",
       "3      0.030819          a  \n",
       "4      0.040552          a  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(subset=['risk_label'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb_make = LabelEncoder()\n",
    "\n",
    "y = data[['risk_label']].copy()\n",
    "y.loc[:, 'risk_label'] = lb_make.fit_transform(y['risk_label'])\n",
    "y_temp = pd.get_dummies(data['risk_label'].astype('category')).copy()\n",
    "y[y_temp.keys()] = y_temp\n",
    "x = data.drop('risk_label', axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 fold CV starting\n",
      "\n",
      "Fold = 0\n",
      "\n",
      "Epoch 1/1\n",
      "396/396 [==============================] - 1s 3ms/step - loss: 0.6817 - acc: 0.2121 - f1: 0.2839 - auc_metric: 0.4491\n",
      "\n",
      "Fold = 1\n",
      "\n",
      "Epoch 1/1\n",
      "397/397 [==============================] - 1s 3ms/step - loss: 0.6764 - acc: 0.4257 - f1: 0.3437 - auc_metric: 0.6748\n",
      "\n",
      "Fold = 2\n",
      "\n",
      "Epoch 1/1\n",
      "397/397 [==============================] - 1s 3ms/step - loss: 0.6843 - acc: 0.2267 - f1: 0.1936 - auc_metric: 0.4231\n",
      "\n",
      "Fold = 3\n",
      "\n",
      "Epoch 1/1\n",
      "397/397 [==============================] - 1s 3ms/step - loss: 0.6871 - acc: 0.1310 - f1: 0.0223 - auc_metric: 0.3393\n",
      "\n",
      "Fold = 4\n",
      "\n",
      "Epoch 1/1\n",
      "397/397 [==============================] - 1s 3ms/step - loss: 0.6821 - acc: 0.2594 - f1: 0.1294 - auc_metric: 0.5084\n"
     ]
    }
   ],
   "source": [
    "cvscores = list()\n",
    "auc_scores = list()\n",
    "report_with_auc_df = ''\n",
    "\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=7)\n",
    "\n",
    "print('5 fold CV starting')\n",
    "for cv_ix, x_y in enumerate(kfold.split(x, y)):\n",
    "    \n",
    "    print('\\nFold = {0}\\n'.format(cv_ix))\n",
    "    \n",
    "    X_train = x.loc[x_y[0]]\n",
    "    y_train = y.loc[x_y[0]]\n",
    "    \n",
    "    X_test = x.loc[x_y[1]]\n",
    "    y_test = y.loc[x_y[1]]\n",
    "\n",
    "#     x_train = X_train.drop(['post_id', 'user_id'], axis=1).copy()\n",
    "#     x_test = X_test.drop(['post_id', 'user_id'], axis=1).copy()\n",
    "    x_train = X_train.drop(['user_id'], axis=1).copy()\n",
    "    x_test = X_test.drop(['user_id'], axis=1).copy()\n",
    "    y_train_risk = y_train['risk_label'].copy()\n",
    "    y_test_risk = y_test['risk_label'].copy()\n",
    "    y_train.drop('risk_label', axis=1, inplace=True)\n",
    "    y_test.drop('risk_label', axis=1, inplace=True)\n",
    "\n",
    "    # basic neural network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, input_dim=np.shape(x_train)[1], activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "\n",
    "    model = multi_gpu_model(model, gpus=available_gpus)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss=f1_loss, metrics=['accuracy', f1, auc_metric])\n",
    "    model.fit(x_train, y_train, epochs=1, batch_size=100, verbose=1)\n",
    "\n",
    "    y_pred_train = np.argmax(model.predict(x_train), axis=1)\n",
    "    y_pred_test = np.argmax(model.predict(x_test), axis=1)\n",
    "\n",
    "    report_with_auc = class_report(\n",
    "        y_true=y_test_risk, \n",
    "        y_pred=y_pred_test, \n",
    "        y_score=model.predict(x_test),\n",
    "        average='macro')\n",
    "    \n",
    "    cv_column = [cv_ix]\n",
    "    cv_column.extend( [''] * (report_with_auc.index.shape[0] - 1))\n",
    "    report_with_auc['Fold'] = cv_column\n",
    "    report_with_auc['Risk-Factor'] = report_with_auc.index\n",
    "    report_with_auc = report_with_auc.set_index(['Fold', 'Risk-Factor'])\n",
    "\n",
    "    if cv_ix == 0:\n",
    "        report_with_auc_df = report_with_auc.copy()\n",
    "    else:\n",
    "        report_with_auc_df = report_with_auc_df.append(report_with_auc.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>pred-cnt</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fold</th>\n",
       "      <th>Risk-Factor</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.556738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th>3</th>\n",
       "      <td>0.459770</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>47.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.493376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.153404</td>\n",
       "      <td>0.296099</td>\n",
       "      <td>0.201885</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.539043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <td>0.373737</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>37.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.372276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.093434</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.136029</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.396469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>2</th>\n",
       "      <td>0.232323</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>23.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.461098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.058081</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.094262</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.484840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <td>0.183673</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.243243</td>\n",
       "      <td>25.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.419459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\"></th>\n",
       "      <th>1</th>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>9.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.590123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.469894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.146151</td>\n",
       "      <td>0.262980</td>\n",
       "      <td>0.158349</td>\n",
       "      <td>56.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.505327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>0</th>\n",
       "      <td>0.255102</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>25.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.471351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\"></th>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.550811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg / total</th>\n",
       "      <td>0.313776</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.120857</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.524324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  precision    recall  f1-score  support  pred-cnt       AUC\n",
       "Fold Risk-Factor                                                            \n",
       "0    1             0.153846  0.333333  0.210526      6.0      13.0  0.556738\n",
       "     3             0.459770  0.851064  0.597015     47.0      87.0  0.493376\n",
       "     avg / total   0.153404  0.296099  0.201885     53.0      53.0  0.539043\n",
       "1    3             0.373737  1.000000  0.544118     37.0      99.0  0.372276\n",
       "     avg / total   0.093434  0.250000  0.136029     37.0      37.0  0.396469\n",
       "2    2             0.232323  1.000000  0.377049     23.0      99.0  0.461098\n",
       "     avg / total   0.058081  0.250000  0.094262     23.0      23.0  0.484840\n",
       "3    0             0.183673  0.360000  0.243243     25.0      49.0  0.419459\n",
       "     1             0.128205  0.555556  0.208333      9.0      39.0  0.590123\n",
       "     2             0.272727  0.136364  0.181818     22.0      11.0  0.469894\n",
       "     avg / total   0.146151  0.262980  0.158349     56.0      56.0  0.505327\n",
       "4    0             0.255102  1.000000  0.406504     25.0      98.0  0.471351\n",
       "     2             1.000000  0.040000  0.076923     25.0       1.0  0.550811\n",
       "     avg / total   0.313776  0.260000  0.120857     50.0      50.0  0.524324"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_with_auc_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
